\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}

\setcounter{MaxMatrixCols}{10}


\textwidth=6.9in
\oddsidemargin=-0.3in
\topmargin=-0.5in
\textheight=9.0in
\linespread{1.8}

\input{preamble}
\usepackage{amsfonts}
\usepackage{amsmath}


\setcounter{MaxMatrixCols}{10}


\title{Algebra lineare e geometria\footnote{mariachiara.menicucci@mail.polimi.it per segnalare errori, richiedere il codice LaTeX ecc.}}


\begin{document}

\maketitle
Questi appunti sono stati presi durante le lezioni dell'insegnamento "Algebra lineare e geometria" tenuto dal prof. Lella durante l'A. A. 2021/22. Non sono stati revisionati da alcun docente (potrebbero contenere errori, in forma e in sostanza, di qualsiasi tipo) e non sono in alcun modo sostitutivi della frequentazione delle lezioni. L'ordine seguito nella presentazione degli argomenti Ã¨ quello del libro \emph{Algebra lineare e geometria} di E. Schlesinger.
\newpage
\tableofcontents

\newpage

\section{Algebra delle matrici}

Si chiama $\mathbf{K}$ un generico campo ($%
%TCIMACRO{\U{211a} }%
%BeginExpansion
\mathbb{Q}
%EndExpansion
$, $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ o $%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
$).

\textbf{Def} Una matrice di tipo $\left( m,n\right) $ su $\mathbf{K}$ \`{e}
una tabella di elementi di $\mathbf{K}$ disposti su $m$ righe e $n$ colonne.

$M_{\mathbf{K}}\left( m,n\right) $ indica l'insieme delle matrici su $%
\mathbf{K}$ di tipo $\left( m,n\right) $; $M$ sta per matrice. Poich\'{e} $%
%TCIMACRO{\U{211a} }%
%BeginExpansion
\mathbb{Q}
%EndExpansion
\subseteq 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\subseteq 
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
$, $M_{%
%TCIMACRO{\U{211a} }%
%BeginExpansion
\mathbb{Q}
%EndExpansion
}\left( m,n\right) \subseteq M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( m,n\right) \subseteq M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( m,n\right) $.

In generale, data la matrice $A\in M_{\mathbf{K}}\left( m,n\right) $, il
generico elemento di $A$ si indica con il nome della matrice in minuscolo e
un pedice il cui primo numero \`{e} l'indice di riga e il cui secondo numero 
\`{e} l'indica di colonna: $a_{ij}$ \`{e} l'elemento generico di $A$ e si
trova nella riga $i$ e nella colonna $j$.%
\begin{equation*}
A=\left[ 
\begin{array}{cccc}
a_{11} & a_{12} & ... & a_{1n} \\ 
a_{21} & a_{22} & ... & a_{2n} \\ 
... & ... & ... & ... \\ 
a_{m1} & a_{m2} & ... & a_{mn}%
\end{array}%
\right] =\left[ a_{ij}\right]
\end{equation*}

dove $\left[ a_{ij}\right] $ indica la matrice degli elementi $a_{ij}$.

Gli elementi $a_{1j}$ sono tutti gli elementi della prima riga, gli elementi 
$a_{i2}$ tutti gli elementi della seconda colonna ecc.

Ci sono alcuni casi particolari di $m$ e $n$:

\begin{description}
\item[-] $m=n=1$: la matrice $A=\left[ a_{11}\right] $ contiene un solo
elemento. $M_{\mathbf{K}}\left( 1,1\right) =\mathbf{K}$.

\item[-] $m=n>1$: la matrice $A$ \`{e} quadrata e $M_{\mathbf{K}}\left(
n,n\right) $ \`{e} l'insieme delle matrici quadrate di ordine $n$.

\item[-] $m=1$: $M_{\mathbf{K}}\left( 1,n\right) $ \`{e} l'insieme dei
vettori riga con $n$ componenti, e $A=\left[ a_{11}|a_{12}|...|a_{1n}\right] 
$. $M_{\mathbf{K}}\left( 1,n\right) =\mathbf{K}^{n}$ \`{e} l'insieme delle
n-uple di elementi di $\mathbf{K}$.

\item[-] $n=1$: $M_{\mathbf{K}}\left( m,1\right) $ \`{e} l'insieme dei
vettori colonna con $n$ componenti, e $A=\left[ 
\begin{array}{c}
a_{11} \\ 
a_{21} \\ 
... \\ 
a_{m1}%
\end{array}%
\right] $. $M_{\mathbf{K}}\left( m,1\right) =\mathbf{K}^{m}$ \`{e} l'insieme
delle m-uple di elementi di $\mathbf{K}$.
\end{description}

\subsection{Operazioni tra matrici}

\textbf{Uguaglianza tra matrici} Date $A\in M_{\mathbf{K}}\left( m,n\right) $%
, $B\in M_{\mathbf{K}}\left( p,q\right) $, se $m\neq p$ o $n\neq q$, $A$ e $%
B $ sono diverse. Se $m=n$ e $p=q$, cio\`{e} $A,B\in M_{\mathbf{K}}\left(
m,n\right) $, $A=B\Longleftrightarrow a_{ij}=b_{ij}$ $\forall $ $i=1,...,m$
e $\forall $ $j=1,...,n$.

\subsubsection{Somma tra matrici}

L'operazione $+:M_{\mathbf{K}}\left( m,n\right) \times M_{\mathbf{K}}\left(
m,n\right) \rightarrow M_{\mathbf{K}}\left( m,n\right) $ \`{e} tale che,
date $A=\left[ a_{ij}\right] $ e $B=\left[ b_{ij}\right] $, $A+B=\left[
a_{ij}+b_{ij}\right] $. Dunque $\left( A,B\right) \mapsto \left( A+B\right) $%
.

\begin{enumerate}
\item $A=\left[ 
\begin{array}{cc}
1 & 1 \\ 
-1 & 0%
\end{array}%
\right] $, $B=\left[ 
\begin{array}{cc}
0 & 1 \\ 
3 & 4%
\end{array}%
\right] $, $A+B=\left[ 
\begin{array}{cc}
1 & 2 \\ 
2 & 4%
\end{array}%
\right] $.
\end{enumerate}

\paragraph{Propriet\`{a}}

\begin{description}
\item[-] Associativa: $\forall $ $A,B,C\in M_{\mathbf{K}}\left( m,n\right) $%
, $A+\left( B+C\right) =\left( A+B\right) +C$.

\item[-] Commutativa: $\forall $ $A,B\in M_{\mathbf{K}}\left( m,n\right) $, $%
A+B=B+A$.

\item[-] Esistenza dell'elemento neutro: $\forall $ $A\in M_{\mathbf{K}%
}\left( m,n\right) $ $\exists $ $0_{M}\in M_{\mathbf{K}}\left( m,n\right) :$
vale $A+0_{M}=A$. $0_{M}\in M_{\mathbf{K}}\left( m,n\right) $ \`{e} la
matrice nulla $0_{M}=\left[ 
\begin{array}{cccc}
0 & 0 & ... & 0 \\ 
0 & 0 & ... & 0 \\ 
... & ... & ... & ... \\ 
0 & 0 & ... & 0%
\end{array}%
\right] $.

\item[-] Esistenza dell'elemento opposto: $\forall $ $A\in M_{\mathbf{K}%
}\left( m,n\right) $ $\exists $ $B\in M_{\mathbf{K}}\left( m,n\right) :$
vale $A+B=0_{M}$. Data $A=\left[ a_{ij}\right] $, $B=\left[ -a_{ij}\right] $.
\end{description}

$\left( M_{\mathbf{K}}\left( m,n\right) ,+\right) $ \`{e} un gruppo abeliano.

\subsubsection{Prodotto scalare per matrice}

L'operazione $\cdot :\mathbf{K}\times M_{\mathbf{K}}\left( m,n\right)
\rightarrow M_{\mathbf{K}}\left( m,n\right) $ \`{e} tale che, date $A=\left[
a_{ij}\right] $ e $t\in \mathbf{K}$, $tA=\left[ ta_{ij}\right] $. Dunque $%
\left( t,A\right) \mapsto tA$.

\begin{enumerate}
\item $A=\left[ 
\begin{array}{cc}
1 & -1 \\ 
2 & 3%
\end{array}%
\right] $, $t=2$, $tA=\left[ 
\begin{array}{cc}
2 & -2 \\ 
4 & 6%
\end{array}%
\right] $.
\end{enumerate}

\paragraph{Propriet\`{a}}

\begin{description}
\item[-] Distributiva a sinistra: $\forall $ $t,s\in \mathbf{K}$, $\forall $ 
$A\in M_{\mathbf{K}}\left( m,n\right) $ $\left( t+s\right) A=tA+sA$.

\item Al lato sinistro il $+$ indica una somma in $\mathbf{K}$, al lato
destro una somma in $M_{\mathbf{K}}\left( m,n\right) \times M_{\mathbf{K}%
}\left( m,n\right) $.

\item[-] Distributiva a destra: $\forall $ $t\in \mathbf{K}$, $\forall $ $%
A,B\in M_{\mathbf{K}}\left( m,n\right) $ $t\left( A+B\right) =tA+tB$.

\item Al lato destro e sinistro il $+$ indica una somma in $M_{\mathbf{K}%
}\left( m,n\right) \times M_{\mathbf{K}}\left( m,n\right) $.

\item[-] Associativa: $\forall $ $t,s\in \mathbf{K}$, $\forall $ $A\in M_{%
\mathbf{K}}\left( m,n\right) $ $\left( ts\right) A=t\left( sA\right) $.

\item Al lato sinistro c'\`{e} un prodotto in $\mathbf{K}$, al lato destro
un prodotto in $\mathbf{K}\times M_{\mathbf{K}}\left( m,n\right) $.

\item[-] Esistenza dell'elemento neutro: $\forall $ $A\in M_{\mathbf{K}%
}\left( m,n\right) $ $\exists $ $1\in \mathbf{K}:$ vale $1_{\mathbf{K}}A=A$.
\end{description}

Queste operazioni e propriet\`{a} si possono usare per scrivere pi\`{u}
sinteticamente un sistema lineare.

\begin{enumerate}
\item $\left\{ 
\begin{array}{c}
x-3y=2 \\ 
4x+5y=1%
\end{array}%
\right. \Longleftrightarrow \left[ 
\begin{array}{c}
x-3y \\ 
4x+5y%
\end{array}%
\right] =\left[ 
\begin{array}{c}
2 \\ 
1%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{c}
x \\ 
4x%
\end{array}%
\right] +\left[ 
\begin{array}{c}
-3y \\ 
5y%
\end{array}%
\right] =\left[ 
\begin{array}{c}
2 \\ 
1%
\end{array}%
\right] $ $\Longleftrightarrow x\left[ 
\begin{array}{c}
1 \\ 
4%
\end{array}%
\right] +y\left[ 
\begin{array}{c}
-3 \\ 
5%
\end{array}%
\right] =\left[ 
\begin{array}{c}
2 \\ 
1%
\end{array}%
\right] $.
\end{enumerate}

\subsubsection{Prodotto matriciale righe per colonne}

L'operazione $\cdot :M_{\mathbf{K}}\left( m,n\right) \times M_{\mathbf{K}%
}\left( n,p\right) \rightarrow M_{\mathbf{K}}\left( m,p\right) $ assegna a $%
A\in M_{\mathbf{K}}\left( m,n\right) $ e $B\in M_{\mathbf{K}}\left(
n,p\right) $ una matrice $C=AB\in M_{\mathbf{K}}\left( m,p\right) $, tale
che $c_{ij}=\sum_{k=1}^{n}a_{ik}b_{kj}$, cio\`{e} il generico elemento $%
c_{ij}$ \`{e} uguale al prodotto scalare tra il vettore riga $i$ di $A$ e il
vettore colonna $j$ di $B$.

\begin{enumerate}
\item $A=\left[ a_{11}|a_{12}...|a_{1n}\right] $ \`{e} $1\times n$, $B=\left[
\begin{array}{c}
b_{11} \\ 
b_{21} \\ 
... \\ 
b_{n1}%
\end{array}%
\right] $ \`{e} $n\times 1$, $C=AB\in M_{\mathbf{K}}\left( 1,1\right) $: $%
c_{11}=\sum_{k=1}^{n}a_{1k}b_{k1}=a_{11}b_{11}+a_{12}b_{21}+...+a_{1n}b_{n1}$%
.

\item $A=\left[ a_{11}a_{12}...a_{1n}\right] $ \`{e} $1\times n$, $B=\left[ 
\begin{array}{ccc}
b_{11} & ... & b_{1p} \\ 
... & ... & ... \\ 
b_{n1} & ... & b_{np}%
\end{array}%
\right] $ \`{e} $n\times p$, $C=AB\in M_{\mathbf{K}}\left( 1,p\right) $: $%
c_{1p}=\sum_{k=1}^{n}a_{1k}b_{kp}=a_{11}b_{1p}+a_{12}b_{2p}+...+a_{1n}b_{np}$%
.

\item $A=\left[ 
\begin{array}{cc}
1 & 2 \\ 
3 & 4%
\end{array}%
\right] \in M_{\mathbf{K}}\left( 2,2\right) $, $B=\left[ 
\begin{array}{ccc}
0 & 1 & -1 \\ 
0 & -1 & 2%
\end{array}%
\right] \in M_{\mathbf{K}}\left( 2,3\right) $, $AB=\left[ 
\begin{array}{ccc}
0 & -1 & 3 \\ 
0 & -1 & 5%
\end{array}%
\right] \in M_{\mathbf{K}}\left( 2,3\right) $.
\end{enumerate}

\paragraph{Propriet\`{a}}

Si suppone che le matrici coinvolte abbiano dimensioni conformabili. Il
prodotto matriciale ha le seguenti propriet\`{a}:

\begin{enumerate}
\item Propriet\`{a} distributiva a destra: $\forall $ $A\in M_{\mathbf{K}%
}\left( m,n\right) $, $\forall $ $B,C\in M_{\mathbf{K}}\left( n,p\right) $, $%
A\left( B+C\right) =AB+AC$.

A sinistra il prodotto \`{e} in $M_{\mathbf{K}}\left( n,p\right) $, a destra
in $M_{\mathbf{K}}\left( m,p\right) $.

\item Propriet\`{a} distributiva a sinistra: $\forall $ $A,B\in M_{\mathbf{K}%
}\left( m,n\right) $, $\forall $ $C\in M_{\mathbf{K}}\left( n,p\right) $, $%
\left( A+B\right) C=AC+BC$.

A sinistra il prodotto \`{e} in $M_{\mathbf{K}}\left( m,n\right) $, a destra
in $M_{\mathbf{K}}\left( m,p\right) $.

\item Propriet\`{a} associativa: $\forall $ $A\in M_{\mathbf{K}}\left(
m,n\right) $, $\forall $ $B\in M_{\mathbf{K}}\left( n,p\right) $, $\forall $ 
$C\in M_{\mathbf{K}}\left( p,q\right) $, $\left( AB\right) C=A\left(
BC\right) $.

A sinistra il primo prodotto \`{e} in $M_{\mathbf{K}}\left( m,p\right) $ e
il secondo in $M_{\mathbf{K}}\left( m,q\right) $, a destra il secondo
prodotto \`{e} in $M_{\mathbf{K}}\left( n,q\right) $ e il primo in $M_{%
\mathbf{K}}\left( m,q\right) $.

\item Omogeneit\`{a}: $\forall $ $t\in \mathbf{K}$, $\forall $ $A\in M_{%
\mathbf{K}}\left( n,m\right) $, $\forall $ $B\in M_{\mathbf{K}}\left(
m,p\right) $, $t\left( AB\right) =\left( tA\right) B=A\left( tB\right) $.

\item Esistenza dell'elemento neutro: se voglio che $A?=A$, la matrice
incognita deve avere dimensione $n\times n$, se voglio $?A=A$ deve avere
dimensione $m\times m$. La matrice incognita \`{e} la matrice identit\`{a},
quadrata, con a pedice la dimensione: $Id_{n}=\left[ 
\begin{array}{cccc}
1 & 0 & ... & 0 \\ 
0 & 1 & ... & 0 \\ 
... & ... & ... & ... \\ 
0 & 0 & ... & 1%
\end{array}%
\right] =\left\{ 
\begin{array}{c}
1\text{ se }i=j \\ 
0\text{ se }i\neq j%
\end{array}%
\right. $. Vale $AId_{n}=A=Id_{m}A$.
\end{enumerate}

\bigskip Per il prodotto matriciale non vale la legge di annullamento del
prodotto, cio\`{e} non \`{e} vero che se $AB$ \`{e} nulla allora $A$ \`{e}
nulla o $B$ \`{e} nulla: l'elemento neutro rispetto alla somma si pu\`{o}
ottenere senza che $A$ o $B$ siano nulle. Infatti:

\begin{enumerate}
\item $A=\left[ 
\begin{array}{cc}
1 & 1%
\end{array}%
\right] $, $B=\left[ 
\begin{array}{c}
1 \\ 
-1%
\end{array}%
\right] $, $AB=\left[ 0\right] $

\item $A=\left[ 
\begin{array}{cc}
1 & 0 \\ 
0 & 0%
\end{array}%
\right] $, $B=\left[ 
\begin{array}{cc}
0 & 0 \\ 
0 & 1%
\end{array}%
\right] $, $AB=\left[ 
\begin{array}{cc}
0 & 0 \\ 
0 & 0%
\end{array}%
\right] $.
\end{enumerate}

Inoltre il prodotto tra matrici in generale non ha la propriet\`{a}
commutativa. Infatti, se $A\in M_{\mathbf{K}}\left( m,n\right) $ e $B\in M_{%
\mathbf{K}}\left( n,p\right) $, si pu\`{o} calcolare $AB$, ma se $p\neq m$
non si pu\`{o} calcolare $BA$. Suppongo allora $m=p$: $AB\in M_{\mathbf{K}%
}\left( m,m\right) $, mentre $BA\in M_{\mathbf{K}}\left( n,n\right) $;
quindi, se $m\neq n$, $AB\neq BA$ e non vale la propriet\`{a} commutativa.
Suppongo allora $m=n=p$: ma la propriet\`{a} commutativa non vale neanche
per matrici quadrate. Infatti:

\begin{enumerate}
\item $A=\left[ 
\begin{array}{cc}
1 & 2 \\ 
3 & 4%
\end{array}%
\right] $, $B=\left[ 
\begin{array}{cc}
1 & 0 \\ 
2 & 3%
\end{array}%
\right] $, $AB=\left[ 
\begin{array}{cc}
5 & 6 \\ 
11 & 12%
\end{array}%
\right] \neq BA=\left[ 
\begin{array}{cc}
1 & 2 \\ 
11 & 16%
\end{array}%
\right] $.
\end{enumerate}

In alcuni casi il prodotto pu\`{o} essere commutativo: data $A\in M_{\mathbf{%
K}}\left( n,n\right) $, $0_{M}\in M_{\mathbf{K}}\left( n,n\right) $, $%
A0_{n,n}=0_{n,n}A=0_{M}\in M_{\mathbf{K}}\left( n,n\right) $; data $A\in M_{%
\mathbf{K}}\left( n,n\right) $, $AId_{n}=Id_{n}A=A\in M_{\mathbf{K}}\left(
n,n\right) $; $A=\left[ 
\begin{array}{cc}
1 & 0 \\ 
0 & 0%
\end{array}%
\right] $, $B=\left[ 
\begin{array}{cc}
0 & 0 \\ 
0 & 1%
\end{array}%
\right] $, $AB=\left[ 
\begin{array}{cc}
0 & 0 \\ 
0 & 0%
\end{array}%
\right] =BA=0_{M}\in M_{\mathbf{K}}\left( 2,2\right) $.

\begin{enumerate}
\item $A=B$ non \`{e} equivalente a $CA=CB$. $A=B\Longrightarrow CA=CB$, ma
non vale in generale l'implicazione opposta: e. g. $\left[ 
\begin{array}{cc}
1 & 1 \\ 
1 & 1%
\end{array}%
\right] \left[ 
\begin{array}{cc}
2 & 3 \\ 
1 & 4%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
1 & 1 \\ 
1 & 1%
\end{array}%
\right] \left[ 
\begin{array}{cc}
1 & 4 \\ 
2 & 3%
\end{array}%
\right] $, ma $\left[ 
\begin{array}{cc}
2 & 3 \\ 
1 & 4%
\end{array}%
\right] \neq \left[ 
\begin{array}{cc}
1 & 4 \\ 
2 & 3%
\end{array}%
\right] $. $A=B\Longleftrightarrow CA=CB$ quando $C$ rappresenta
un'applicazione lineare iniettiva (?).
\end{enumerate}

\subsection{Matrice inversa}

\textbf{Def} Una matrice $A\in M_{\mathbf{K}}\left( m,n\right) $ si dice
invertibile a destra se esiste $B\in M_{\mathbf{K}}\left( n,m\right)
:AB=Id_{m}$. Una matrice $A\in M_{\mathbf{K}}\left( m,n\right) $ si dice
invertibile a sinistra se esiste $C\in M_{\mathbf{K}}\left( n,m\right)
:CA=Id_{n}$. Una matrice $A\in M_{\mathbf{K}}\left( m,n\right) $ si dice
invertibile se \`{e} invertibile sia a destra che a sinistra.

Non tutte le matrici sono invertibili, quindi l'insieme delle matrici non 
\`{e} un campo con l'operazione di prodotto matriciale. Una matrice non
invertibile si dice anche singolare.

\textbf{Proposizione}%
\begin{gather*}
\text{Hp}\text{: }A\in M_{\mathbf{K}}\left( m,n\right) \text{ \`{e}
invertibile} \\
\text{Ts}\text{:}\text{ (i) L'inversa destra \`{e} uguale all'inversa
sinistra} \\
\text{(ii) L'inversa \`{e} unica}
\end{gather*}

\textbf{Dim} (1) Essendo $A$ invertibile, $\exists $ $B\in M_{\mathbf{K}%
}\left( n,m\right) :AB=Id_{m}$ e $\exists $ $C\in M_{\mathbf{K}}\left(
n,m\right) :CA=Id_{n}$. Considero $B\in M_{\mathbf{K}}\left( n,m\right) $:
poich\'{e} esiste l'elemento neutro, $B=Id_{n}B=\left( CA\right) B$ per
ipotesi. Poich\'{e} vale la propriet\`{a} associativa e $B$ \`{e} l'inversa
destra di $A$, $\left( CA\right) B=C\left( AB\right) =CId_{m}=C$, dunque $%
B=C $ e le inverse sono uguali.

(2) Dimostro che, date due generiche inverse di $A$, queste sono uguali.
Considero due generiche inverse $B_{1},B_{2}\in M_{\mathbf{K}}\left(
n,m\right) $: vale $AB_{1}=AB_{2}=Id_{m}$ e, poich\'{e} l'inversa destra 
\`{e} uguale alla sinistra, $B_{1}A=B_{2}A=Id_{n}$. Allora $%
B_{1}=Id_{n}B_{1}=\left( B_{2}A\right) B_{1}=B_{2}\left( AB_{1}\right)
=B_{2}Id_{m}=B_{2}$. $\blacksquare $

Si indica con $A^{-1}$ l'unica matrice inversa di una matrice invertibile $A$%
.

\begin{enumerate}
\item Provo a invertire una matrice $A=\left[ 
\begin{array}{ccc}
1 & 2 & 3 \\ 
4 & 5 & 6%
\end{array}%
\right] \in M_{\mathbf{K}}\left( 2,3\right) $. Esiste un'inversa destra, cio%
\`{e} una matrice $X\in M_{\mathbf{K}}\left( 3,2\right) :AX=Id_{2}$? Voglio $%
X=\left[ 
\begin{array}{cc}
x_{11} & x_{12} \\ 
x_{21} & x_{22} \\ 
x_{31} & x_{32}%
\end{array}%
\right] :AX=\left[ 
\begin{array}{cc}
1 & 0 \\ 
0 & 1%
\end{array}%
\right] $. Calcolo $AX=\left[ 
\begin{array}{ccc}
1 & 2 & 3 \\ 
4 & 5 & 6%
\end{array}%
\right] \left[ 
\begin{array}{cc}
x_{11} & x_{12} \\ 
x_{21} & x_{22} \\ 
x_{31} & x_{32}%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
x_{11}+2x_{21}+3x_{31} & x_{12}+2x_{22}+3x_{32} \\ 
4x_{11}+5x_{21}+6x_{31} & 4x_{12}+4x_{22}+6x_{32}%
\end{array}%
\right] $. Eguaglio questa matrice a $Id_{2}$, elemento per elemento: $%
\left\{ 
\begin{array}{c}
x_{11}+2x_{21}+3x_{31}=1 \\ 
x_{12}+2x_{22}+3x_{32}=0 \\ 
4x_{11}+5x_{21}+6x_{31}=0 \\ 
4x_{12}+4x_{22}+6x_{32}=1%
\end{array}%
\right. $. E' un sistema a sei incognite e quattro equazioni: l'intuizione
suggerisce che abbia infinite soluzioni, perch\'{e} i gradi di libert\`{a}
(cio\`{e} il numero di equazioni) sono pi\`{u} dei vincoli.

Esiste un'inversa sinistra, cio\`{e} una matrice $X\in M_{\mathbf{K}}\left(
3,2\right) :XA=Id_{3}$? Voglio $X=\left[ 
\begin{array}{cc}
x_{11} & x_{12} \\ 
x_{21} & x_{22} \\ 
x_{31} & x_{32}%
\end{array}%
\right] :XA=\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1%
\end{array}%
\right] $. Calcolo $XA=\left[ 
\begin{array}{cc}
x_{11} & x_{12} \\ 
x_{21} & x_{22} \\ 
x_{31} & x_{32}%
\end{array}%
\right] \left[ 
\begin{array}{ccc}
1 & 2 & 3 \\ 
4 & 5 & 6%
\end{array}%
\right] =\left[ 
\begin{array}{ccc}
x_{11}+4x_{12} & 2x_{11}+5x_{12} & 3x_{11}+6x_{12} \\ 
x_{21}+4x_{22} & 2x_{21}+5x_{22} & 3x_{21}+6x_{22} \\ 
x_{31}+4x_{32} & 2x_{31}+5x_{32} & 3x_{31}+6x_{32}%
\end{array}%
\right] $. Uguaglia questa matrice a $Id_{3}$, elemento per elemento, e
ottengo un sistema a sei incognite e nove equazioni: l'intuizione suggerisce
che non abbia soluzioni, perch\'{e} i gradi di libert\`{a} sono meno dei
vincoli. Il sistema \`{e} l'unione di tre sistemi, ciascuno dei quali con
due incognite: $\left\{ 
\begin{array}{c}
x_{11}+4x_{12}=1 \\ 
2x_{11}+5x_{12}=0 \\ 
3x_{11}+6x_{12}=0%
\end{array}%
\right. $, $\left\{ 
\begin{array}{c}
x_{21}+4x_{22}=0 \\ 
2x_{21}+5x_{22}=1 \\ 
3x_{21}+6x_{22}=0%
\end{array}%
\right. $, $\left\{ 
\begin{array}{c}
x_{31}+4x_{32}=0 \\ 
2x_{31}+5x_{32}=0 \\ 
3x_{31}+6x_{32}=1%
\end{array}%
\right. $. Dal primo, sostituendo, si ha $\left\{ 
\begin{array}{c}
x_{11}+4x_{12}=1 \\ 
2x_{11}+5x_{12}=0 \\ 
x_{11}=-2x_{12}%
\end{array}%
\right. \Longleftrightarrow \left\{ 
\begin{array}{c}
-2x_{12}+4x_{12}=1 \\ 
2x_{11}+5x_{12}=x_{12}=0 \\ 
x_{11}=-2x_{12}%
\end{array}%
\right. $. Poich\'{e} dalle prime due equazioni si ricava che $%
x_{12}=x_{11}=0$ e sostituendo nella prima si ha $0=1$, si ricava che il
sistema \`{e} impossibile. Dunque la matrice non \`{e} invertibile.

Quindi, se $m\neq n$, ricercando l'inversa destra si ottiene un sistema con
pi\`{u} incognite che equazioni, ricercando l'inversa sinistra un sistema
con pi\`{u} equazioni che incognite. Rimane la speranza di poter calcolare
la matrice inversa se $m=n$.
\end{enumerate}

\section{Sistemi lineari}

Dato il sistema $\left\{ 
\begin{array}{c}
x-3y=2 \\ 
4x+5y=1%
\end{array}%
\right. $, se ne pu\`{o} dare una rappresentazione matriciale raccogliendo
le incognite: $\left[ 
\begin{array}{c}
x-3y \\ 
4x+5y%
\end{array}%
\right] =\left[ 
\begin{array}{c}
2 \\ 
1%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{c}
x \\ 
4x%
\end{array}%
\right] +\left[ 
\begin{array}{c}
-3y \\ 
5y%
\end{array}%
\right] =\left[ 
\begin{array}{c}
2 \\ 
1%
\end{array}%
\right] \Longleftrightarrow x\left[ 
\begin{array}{c}
1 \\ 
4%
\end{array}%
\right] +y\left[ 
\begin{array}{c}
-3 \\ 
5%
\end{array}%
\right] =\left[ 
\begin{array}{c}
2 \\ 
1%
\end{array}%
\right] $ (primo tipo di rappresentazione matriciale del sistema).
Utilizzando il prodotto tra matrici, vale anche $x\left[ 
\begin{array}{c}
1 \\ 
4%
\end{array}%
\right] +y\left[ 
\begin{array}{c}
-3 \\ 
5%
\end{array}%
\right] =\left[ 
\begin{array}{c}
2 \\ 
1%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{cc}
1 & -3 \\ 
4 & 5%
\end{array}%
\right] \left[ 
\begin{array}{c}
x \\ 
y%
\end{array}%
\right] =\left[ 
\begin{array}{c}
2 \\ 
1%
\end{array}%
\right] $: quest'ultimo \`{e} il secondo tipo di rappresentazione matriciale
del sistema. Ma $x$ e $y$ sono variabili mute, quindi \`{e} inutile scrivere
il vettore delle incognite. Si pu\`{o} quindi unire tutto in una matrice $%
\left[ 
\begin{array}{ccc}
1 & -3 & 2 \\ 
4 & 5 & 1%
\end{array}%
\right] $, giustapponendo a destra il vettore dei termini noti: questo \`{e}
il terzo tipo di rappresentazione matriciale di un sistema.

In generale, un sistema lineare \`{e} un insieme di $m$ equazioni con $n$
incognite, del tipo%
\begin{equation*}
\left\{ 
\begin{array}{c}
a_{11}x_{1}+a_{12}x_{2}+...+a_{1n}x_{n}=b_{1} \\ 
a_{21}x_{1}+a_{22}x_{2}+...+a_{2n}x_{n}=b_{2} \\ 
... \\ 
a_{m1}x_{1}+a_{m2}x_{2}+...+a_{mn}x_{n}=b_{m}%
\end{array}%
\right.
\end{equation*}

che \`{e} una matrice $m\times 1$ dove $x_{1},x_{2},...,x_{n}$ sono le $n$
incognite. Tale sistema si pu\`{o} rappresentare usando le matrici in tre
modi:

\begin{enumerate}
\item $x_{1}\left[ 
\begin{array}{c}
a_{11} \\ 
a_{21} \\ 
... \\ 
a_{m1}%
\end{array}%
\right] +x_{2}\left[ 
\begin{array}{c}
a_{12} \\ 
a_{22} \\ 
... \\ 
a_{m2}%
\end{array}%
\right] +...+x_{n}\left[ 
\begin{array}{c}
a_{1n} \\ 
a_{2n} \\ 
... \\ 
a_{mn}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
b_{1} \\ 
b_{2} \\ 
... \\ 
b_{m}%
\end{array}%
\right] $

\item $\left[ 
\begin{array}{ccc}
a_{11} & ... & a_{1n} \\ 
... & ... & ... \\ 
a_{m1} & ... & a_{mn}%
\end{array}%
\right] \left[ 
\begin{array}{c}
x_{1} \\ 
x_{2} \\ 
... \\ 
x_{n}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
b_{1} \\ 
... \\ 
b_{m}%
\end{array}%
\right] $ dove $\left[ 
\begin{array}{c}
x_{1} \\ 
x_{2} \\ 
... \\ 
x_{n}%
\end{array}%
\right] $ \`{e} il vettore delle incognite. Il generico sistema lineare, in
forma estremamente sintetica, si scrive $A\mathbf{x}=\mathbf{b}$, dove $A$ 
\`{e} la matrice dei coefficienti, $\mathbf{x}$ il vettore delle incognite, $%
\mathbf{b}$ il vettore dei termini noti.

\item $\left[ 
\begin{array}{cccc}
a_{11} & ... & a_{1n} & b_{1} \\ 
... & ... & ... & ... \\ 
a_{m1} & ... & a_{mn} & b_{m}%
\end{array}%
\right] \in M_{\mathbf{K}}\left( m,n+1\right) $ \`{e} la matrice completa o
estesa del sistema, che non riporta le incognite.
\end{enumerate}

\subsection{Metodi di risoluzione e matrici a scala}

Tra i sistemi $\left\{ 
\begin{array}{c}
x+y-2z=0 \\ 
x-y+3z=0 \\ 
2x-2y+z=-7%
\end{array}%
\right. $ e $\left\{ 
\begin{array}{c}
x+y-3z=4 \\ 
y+2z=1 \\ 
z=-4%
\end{array}%
\right. $, il secondo \`{e} pi\`{u} facile da risolvere perch\'{e} la
seconda equazione \`{e} in due incognite e la terza in una sola incognita.
Questo \`{e} molto evidente scrivendo la rappresentazione matriciale del
terzo tipo dei due sistemi: $\left[ 
\begin{array}{cccc}
1 & 1 & -2 & 0 \\ 
1 & -1 & 3 & 1 \\ 
2 & -2 & 1 & -7%
\end{array}%
\right] $ e $\left[ 
\begin{array}{cccc}
1 & 1 & -3 & 4 \\ 
0 & 1 & 2 & 1 \\ 
0 & 0 & 1 & -4%
\end{array}%
\right] $. La matrice dei coefficienti del secondo sistema \`{e} tale che le
ultime due righe costituiscono un sistema di due equazioni in due incognite
e l'ultima riga \`{e} un'equazione in un'incognita: questo tipo di matrice
prende il nome di matrice a scala.

\textbf{Def} Sia $U$ una matrice in $M_{\mathbf{K}}\left( m,n\right) $,
siano $U_{1}$, $U_{2}$,..., $U_{m}$ le sue righe, con $U_{i}\in M_{\mathbf{K}%
}\left( 1,n\right) $. Si chiama pivot della riga $U_{i}$ il (eventuale?)
primo elemento non nullo, a partire da sinistra, di $U_{i}$. La matrice $U$
si dice a scala se

\begin{description}
\item[-] date due righe consecutive $U_{i}$, $U_{i+1}$, entrambe non nulle,
l'indice di colonna del pivot di $U_{i}$ \`{e} minore dell'indice di colonna
del pivot di $U_{i+1}$ (il pivot di $U_{i}$ compare a sinistra rispetto al
pivot di $U_{i+1}$)

\item[-] se $U_{i}$ \`{e} nulla, allora sono nulle tutte le righe successive.
\end{description}

Tutte le matrici con una riga sono a scala.

\begin{enumerate}
\item $\left[ 
\begin{array}{ccccccc}
0 & p_{1} & \ast & \ast & \ast & \ast & \ast \\ 
0 & 0 & p_{2} & \ast & \ast & \ast & \ast \\ 
0 & 0 & 0 & 0 & 0 & p_{3} & \ast \\ 
0 & 0 & 0 & 0 & 0 & 0 & 0%
\end{array}%
\right] $ \`{e} una matrice a scala i cui pivot sono $p_{1}$, $p_{2}$ e $%
p_{3}$; i $\ast $ indicano elementi irrilevanti; e. g. $p_{2}$ non pu\`{o}
essere nella prima colonna.
\end{enumerate}

Risolvo il primo sistema col metodo di riduzione di Gauss: l'obiettivo \`{e}
rendere la matrice dei coefficienti una matrice a scala. $\left\{ 
\begin{array}{c}
x+y-2z=0 \\ 
x-y+3z=0 \\ 
2x-2y+z=-7%
\end{array}%
\right. \Longleftrightarrow \left( A|\mathbf{b}\right) ^{\left( 1\right) }=%
\left[ 
\begin{array}{cccc}
1 & 1 & -2 & 0 \\ 
1 & -1 & 3 & 1 \\ 
2 & -2 & 1 & -7%
\end{array}%
\right] $. Considero il pivot della prima riga: il pivot d\qquad ella
seconda riga deve essere a destra, quindi il primo elemento della seconda
riga dev'essere nullo. Analogamente per la terza riga: faccio $\left(
3\right) =\left( 3\right) -2\left( 2\right) $ e $\left( 2\right) =\left(
2\right) -\left( 1\right) $, e ottengo $\left( A|\mathbf{b}\right) ^{\left(
2\right) }=\left[ 
\begin{array}{cccc}
1 & 1 & -2 & 0 \\ 
0 & -2 & 5 & 1 \\ 
0 & 0 & -5 & -9%
\end{array}%
\right] $. La matrice dei coefficienti \`{e} una matrice a scala: si \`{e}
ottenuta con il \textbf{metodo di riduzione di Gauss}. Adesso cerco di avere
tutti i pivot uguali a $1$: $\left( 2\right) =\frac{-1}{2}\left( 2\right) $, 
$\left( 3\right) =\frac{-1}{5}\left( 3\right) $ e ottengo $\left[ 
\begin{array}{cccc}
1 & 1 & -2 & 0 \\ 
0 & 1 & -\frac{5}{2} & -\frac{1}{2} \\ 
0 & 0 & 1 & \frac{9}{5}%
\end{array}%
\right] $. Adesso, seguendo il metodo di riduzione di Gauss-Jordan, cerco di
rendere la matrice dei coefficienti una matrice identit\`{a}: modifico le
righe dal basso verso l'alto, perch\'{e} gli elementi nulli prima del pivot
permettono di conservare la parte di matrice che non voglio modificare.
Faccio $\left( 2\right) =\left( 2\right) +\frac{5}{2}\left( 3\right) $ e $%
\left( 1\right) =\left( 1\right) +2\left( 3\right) $ e ottengo $\left[ 
\begin{array}{cccc}
1 & 1 & 0 & \frac{18}{5} \\ 
0 & 1 & 0 & 4 \\ 
0 & 0 & 1 & \frac{9}{5}%
\end{array}%
\right] $. Manca un solo elemento da modificare: faccio $\left( 1\right)
=\left( 1\right) -\left( 2\right) $ e ottengo $\left[ 
\begin{array}{cccc}
1 & 0 & 0 & -\frac{2}{5} \\ 
0 & 1 & 0 & 4 \\ 
0 & 0 & 1 & \frac{9}{5}%
\end{array}%
\right] $. Avere i pivot uguali a $1$ e la matrice dei coefficienti identit%
\`{a} permette di ottenere il valore esatto delle incognite nel vettore dei
termini noti: infatti quest'ultima matrice \`{e} la rappresentazione del
sistema $\left\{ 
\begin{array}{c}
x=-\frac{2}{5} \\ 
y=4 \\ 
z=\frac{9}{5}%
\end{array}%
\right. $

Quindi \`{e} possibile usare il metodo di Gauss e risolvere il sistema
partendo dalla sua rappresentazione con matrice a scala e sostituendo a
ritroso, oppure si pu\`{o} seguire il metodo di Gauss-Jordan e risolvere il
sistema lavorando direttamente sulla matrice. Il primo richiede meno
operazioni e permette di capire pi\`{u} in fretta se il sistema ha
soluzione, mentre il secondo permette di ottenere direttamente la soluzione
e porta inoltre a un'unica matrice (SE c'\`{e} il risultato), a differenza
del metodo di Gauss.

In generale, dato un sistema lineare, il metodo di riduzione permette di:

\begin{enumerate}
\item moltiplicare un'equazione (una riga) per una costante non nulla.

\item scambiare la posizione di due righe, e. g. se si trova una riga nulla;

\item sostituire a una riga $U_{i}$ la somma di $U_{i}$ e un multiplo di
un'altra riga;
\end{enumerate}

\textbf{Proposizione}%
\begin{gather*}
\text{Hp}\text{: }A\mathbf{x}=\mathbf{b}\text{ \`{e} un sistema lineare; }%
A^{\prime }\mathbf{x}=\mathbf{b}^{\prime }\text{ \`{e} un sistema lineare
ottenuto dal} \\
\text{sistema di partenza }\text{mediante una sequenza di operazioni sulle
righe} \\
\text{Ts}\text{: }A\mathbf{x}=\mathbf{b}\text{ e }A^{\prime }\mathbf{x}=%
\mathbf{b}^{\prime }\text{ hanno le stesse soluzioni}
\end{gather*}

Questo significa che le operazioni sulle righe che si fanno col metodo di
Gauss non modificano le soluzioni del sistema (lo spazio delle righe si
conserva). L'idea della dimostrazione \`{e} mostrare che le operazioni sulle
righe sono reversibili e in un singolo passaggio le soluzioni non cambiano.

\textbf{Teorema}%
\begin{gather*}
\text{Hp}\text{: }A\in M_{\mathbf{K}}\left( m,n\right) \\
\text{Ts}\text{: }A\text{ pu\`{o} essere trasformata in una matrice a scala}
\\
\text{ con una sequenza finita di operazioni sulle righe}
\end{gather*}

\textbf{Dim} La dimostrazione \`{e} costruttiva e si basa su un procedimento
ricorsivo. Se $A$ \`{e} a scala, non devo fare niente. Suppongo che $A$ non
sia a scala: questo garantisce che ci sia almeno un elemento non nullo (una
matrice nulla \`{e} a scala). Fisso l'indice $j$ della prima colonna da
sinistra contenente un elemento non nullo; fisso l'indice $i$ di una
qualsiasi riga tale che $a_{ij}\neq 0$ e scambio la prima riga con la riga $%
i $-esima (voglio "schiacciare" l'informazione in alto). Ora l'elemento non
nullo nella prima riga \`{e} un pivot, che chiamo $p_{1}$; cancello tutti
gli elementi non nulli che si trovano sotto al primo pivot (nella colonna $j$%
), facendo (per ogni riga $i$ sotto la prima tale che l'elemento sulla
j-esima colonna \`{e} non nullo) l'operazione $R_{i}-\frac{a_{ij}}{p_{1}}%
R_{1}\rightarrow R_{i}$. Dopo queste operazioni, tutti gli elementi sotto $%
p_{1}$ sono nulli; ripeto il procedimento per la sottomatrice formata dalle
righe $\left( 2,...,m\right) $ e dalle colonne $\left( j+1,...,n\right) $.
L'algoritmo termina perch\'{e} a ogni iterazione del procedimento la nuova
matrice da ridurre ha meno righe: si arriver\`{a} ad avere quindi una riga. $%
\blacksquare $

L'elemento $\frac{a_{ij}}{p_{1}}$ \`{e} detto moltiplicatore.

\subsection{Soluzioni di un sistema}

\textbf{Def} Un sistema lineare si dice omogeneo se tutti i termini noti
sono nulli: $A\mathbf{x}=\mathbf{0}$. Dato un sistema lineare qualsiasi $A%
\mathbf{x}=\mathbf{b}$, si dice sistema omogeneo associato ad esso il
sistema $A\mathbf{x}=\mathbf{0}$.

Un sistema omogeneo ammette sempre almeno una soluzione: c'\`{e} sempre la
soluzione nulla $\mathbf{x=0}$. In generale, ammette una soluzione o
infinite soluzioni: se $\mathbf{v}$ \`{e} una soluzione, anche $\lambda 
\mathbf{v}$ lo \`{e}; se $\mathbf{v,w}$ sono soluzioni, anche $\mathbf{v+w}$
lo \`{e}.

\textbf{Teorema (struttura generale della soluzione di un sistema lineare)}%
\begin{gather*}
\text{Hp}\text{:}A\mathbf{x}=\mathbf{b}\text{ \`{e} un sistema lineare, }A%
\mathbf{x}=\mathbf{0}\text{ \`{e} il sistema omogeneo } \\
\text{ad esso associato, }\mathbf{v}_{0}\text{ \`{e} una soluzione di }A%
\mathbf{x}=\mathbf{b} \\
\text{Ts}\text{:}\text{ tutte e sole le soluzioni di }A\mathbf{x}=\mathbf{b}%
\text{ si ottengono sommando } \\
\text{a }\mathbf{v}_{0}\text{ una soluzione }\mathbf{v}_{h}\text{ di }A%
\mathbf{x}=\mathbf{0}\text{, cio\`{e} }\left\{ \mathbf{x}\in \mathbf{K}^{n}:A%
\mathbf{x}=\mathbf{b}\right\} = \\
\left\{ \mathbf{x}\in \mathbf{K}^{n}:\mathbf{x=v}_{0}\mathbf{+v}_{h}\text{,
al variare di }\mathbf{v}_{h}\text{ nell'insieme delle soluzioni del sistema
omogeneo}\right\} 
\end{gather*}

\textbf{Dim} Devo dimostrare che due insiemi coincidono, quindi ne mostro la
mutua inclusione. (1) Mostro che se ho una soluzione particolare e considero
una soluzione del sistema omogeneo associato, posso determinare una
soluzione del sistema lineare $A\mathbf{x}=\mathbf{b}$. Sia $\mathbf{v}_{h}$
una soluzione di $A\mathbf{x}=\mathbf{0}$: allora $\mathbf{v}_{0}+\mathbf{v}%
_{h}$ \`{e} una soluzione di $A\mathbf{x}=\mathbf{b}$. Infatti $A\left( 
\mathbf{v}_{0}+\mathbf{v}_{h}\right) =A\mathbf{v}_{0}+A\mathbf{v}_{h}$ per
la propriet\`{a} distributiva a destra del prodotto matriciale, e $A\mathbf{v%
}_{0}+A\mathbf{v}_{h}=\mathbf{b+0=b}$ per ipotesi e per l'esistenza
dell'elemento neutro della somma.

(2) Mostro che se considero una seconda soluzione $\mathbf{v}_{1}$ di $A%
\mathbf{x=b}$, questa pu\`{o} essere scritta come somma di $\mathbf{v}_{0}$
e una soluzione del sistema omogeneo, cio\`{e} $\exists $ $\mathbf{v}_{h}:A%
\mathbf{v}_{h}=\mathbf{0}$ tale che $\mathbf{v}_{1}=\mathbf{v}_{0}+\mathbf{v}%
_{h}$. Questo \`{e} equivalente a mostrare che $\mathbf{v}_{h}=\mathbf{v}_{1}%
\mathbf{-v}_{0}$ \`{e} soluzione del sistema omogeneo. Infatti, $A\left( 
\mathbf{v}_{1}\mathbf{-v}_{0}\right) =A\mathbf{v}_{1}\mathbf{-}A\mathbf{v}%
_{0}=\mathbf{b-b=0}$, per la propriet\`{a} distributiva a destra del
prodotto matriciale, per ipotesi e per esistenza dell'elemento opposto. $\blacksquare $

Questo teorema \`{e} estremamente utile per risolvere sistemi lineari che
condividono la matrice dei coefficienti.

\begin{enumerate}
\item Dati $a,b\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, il sistema $\left\{ 
\begin{array}{c}
3x-2y=1 \\ 
6x+ay=b%
\end{array}%
\right. $ \`{e} risolubile? Se s\`{\i}, che soluzioni ha?

La matrice completa del sistema \`{e} $\left[ 
\begin{array}{ccc}
3 & -2 & 1 \\ 
6 & a & b%
\end{array}%
\right] $: la riduco a scala facendo $\left( 2\right) =\left( 2\right)
-2\left( 1\right) $ e ottengo $\left[ 
\begin{array}{ccc}
3 & -2 & 1 \\ 
0 & a+4 & b-2%
\end{array}%
\right] $. Ci sono tre casi di risolubilit\`{a} del sistema:

\begin{enumerate}
\item Se $a+4\neq 0$, posso procedere con il MEGJ e determinare le soluzioni
del sistema: $\left[ 
\begin{array}{ccc}
3 & -2 & 1 \\ 
0 & a+4 & b-2%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{ccc}
3 & -2 & 1 \\ 
0 & 1 & \frac{b-2}{a+4}%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{ccc}
3 & 0 & 1+2\frac{b-2}{a+4} \\ 
0 & 1 & \frac{b-2}{a+4}%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{ccc}
1 & 0 & \frac{1}{3}+\frac{2}{3}\frac{b-2}{a+4} \\ 
0 & 1 & \frac{b-2}{a+4}%
\end{array}%
\right] $. La soluzione \`{e} quindi unica. (Caso in cui nel nucleo c'\`{e}
solo la soluzione nulla, vettori linearmente indipendenti, $r\left(
A|b\right) =n$)

\item Se $a+4=0$ e $b-2\neq 0$, il sistema $\left[ 
\begin{array}{ccc}
3 & -2 & 1 \\ 
0 & 0 & b-2%
\end{array}%
\right] $ \`{e} impossibile.

\item Se $a+4=0$ e $b-2=0$, il sistema $\left[ 
\begin{array}{ccc}
3 & -2 & 1 \\ 
0 & 0 & 0%
\end{array}%
\right] $ presenta una riga non informativa: la seconda, che si pu\`{o}
ignorare. Si deve allora risolvere il sistema $\left\{ 3x-2y=1\right. $, che 
\`{e} risolubile con infinite soluzioni, dipendenti da un parametro. Scrivo
le soluzioni in dipendenza di un parametro $t\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$: pongo $y=t$ e risolvo $\left\{ 
\begin{array}{c}
3x-2y=1 \\ 
y=t%
\end{array}%
\right. $: $\left[ 
\begin{array}{ccc}
3 & -2 & 1 \\ 
0 & 1 & t%
\end{array}%
\right] \Longleftrightarrow $ $\left[ 
\begin{array}{ccc}
3 & 0 & 1+2t \\ 
0 & 1 & t%
\end{array}%
\right] \Longleftrightarrow $ $\left[ 
\begin{array}{ccc}
1 & 0 & \frac{1}{3}+\frac{2}{3}t \\ 
0 & 1 & t%
\end{array}%
\right] $. Dunque $x=\frac{1}{3}+\frac{2}{3}t$, $y=t$, al variare di $t\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$. Rappresento le soluzioni in forma matriciale: $\left[ 
\begin{array}{c}
x \\ 
y%
\end{array}%
\right] =\left[ 
\begin{array}{c}
\frac{1}{3}+\frac{2}{3}t \\ 
t%
\end{array}%
\right] =\left[ 
\begin{array}{c}
\frac{1}{3} \\ 
0%
\end{array}%
\right] +t\left[ 
\begin{array}{c}
\frac{2}{3} \\ 
1%
\end{array}%
\right] $. Questo modo di scrivere la soluzione ricorda la struttura delle
soluzioni enunciata nel teorema appena visto. Questo fa pensare che il
contributo del parametro $t$ non dipenda dal vettore dei termini noti, ma
costituisca l'inseme delle soluzioni del sistema omogeneo associato (infatti 
$\left[ 
\begin{array}{c}
\frac{2}{3} \\ 
1%
\end{array}%
\right] $ \`{e} soluzione del sistema omogeneo: $\left[ 
\begin{array}{cc}
3 & -2 \\ 
0 & 0%
\end{array}%
\right] \left[ 
\begin{array}{c}
\frac{2}{3} \\ 
1%
\end{array}%
\right] =\left[ 
\begin{array}{c}
2-2 \\ 
0%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
0%
\end{array}%
\right] $, e ogni vettore $t\left[ 
\begin{array}{c}
\frac{2}{3} \\ 
1%
\end{array}%
\right] $ \`{e} soluzione del sistema omogeneo). $\left[ 
\begin{array}{c}
\frac{1}{3} \\ 
0%
\end{array}%
\right] $ sar\`{a} invece una soluzione del sistema $A\mathbf{x}=\mathbf{b}$.

Dal punto di vista geometrico $3x-2y=1$ \`{e} l'equazione cartesiana di una
retta nel piano $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$: tale soluzione \`{e} rappresentata come $\left[ 
\begin{array}{c}
\frac{1}{3} \\ 
0%
\end{array}%
\right] +t\left[ 
\begin{array}{c}
\frac{2}{3} \\ 
1%
\end{array}%
\right] $. Se scrivessi analogamente $3x-2y=-2$ (cambiando il termine noto),
otterrei $\left[ 
\begin{array}{c}
-\frac{2}{3} \\ 
0%
\end{array}%
\right] +t\left[ 
\begin{array}{c}
\frac{2}{3} \\ 
1%
\end{array}%
\right] $: modificare il termine noto equivale ad avere come soluzione una
retta traslata verticalmente rispetto alla prima trovata. Per tale sistema
quindi la soluzione \`{e} sempre una retta di coefficiente angolare fisso
(determinato dal sistema omogeneo associato) e intercetta variabile a
seconda del vettore dei termini noti $\mathbf{v}_{0}$. Infatti $%
3x-2y=1\Longleftrightarrow y=-\frac{1}{2}+\frac{3}{2}x$: mettere un altro
numero al posto dell'$1$ inficia solo l'intercetta.
\end{enumerate}

Nel primo caso il numero di pivot di $A$ \`{e} uguale al numero di pivot di $%
\left[ A|\mathbf{b}\right] $, e c'\`{e} un'unica soluzione. Nel secondo il
numero di pivot di $A$ \`{e} diverso da quello di $\left[ A|\mathbf{b}\right]
$, e non c'\`{e} alcuna soluzione. Nel terzo il numero di pivot \`{e} uguale
e c'\`{e} una riga nulla: ci sono infinite soluzioni. Il numero di pivot di $%
A$ \`{e} in relazione con la risolubilit\`{a} del sistema.
\end{enumerate}

\subsection{Rango e traccia}

\textbf{Def} Data una matrice a scala $U$, si definisce rango di $U$ il suo
numero di pivot. Data una matrice qualsiasi $A$, si definisce rango di $A$
il numero di pivot di una qualsiasi matrice a scala ottenuta a partire da $A$
con il metodo di eliminazione di Gauss.

Il rango della matrice $A$ si indica con $r\left( A\right) $, $rk\left(
A\right) $, $rg\left( A\right) $, $\rho \left( A\right) $.

La definizione \`{e} ben posta perch\'{e}, nonostante si possano ottenere
diverse matrici a scala da $A$ con il MEG, tutte hanno lo stesso numero di
pivot (si vedr\`{a} pi\`{u} avanti che $\dim \left( \func{row}A\right) =\dim
\left( \func{row}U\right) $).

Dalla definizione segue che, data $A\in M_{\mathbf{K}}\left( m,n\right) $, $%
r\left( A\right) \geq 0$, $r\left( A\right) \in 
%TCIMACRO{\U{2115} }%
%BeginExpansion
\mathbb{N}
%EndExpansion
$; dal momento che nella matrice a scala c'\`{e} al pi\`{u} un pivot per
riga, $r\left( A\right) \leq m$; poich\'{e} c'\`{e} al pi\`{u} un pivot per
colonna, $r\left( A\right) \leq n$. Dunque $0\leq r\left( A\right) \leq \min
\left\{ m,n\right\} $.

Il rango di $A$ \`{e} il numero di righe non nulle di $A$ ridotta a scala:
se $A$ \`{e} la matrice incompleta di un sistema, $r\left( A\right) $
rappresenta il numero di equazioni significative.

\textbf{Def} Data una matrice $A\in M_{\mathbf{K}}\left( n,n\right) $, si
definisce traccia $tr\left( A\right) $ la somma degli elementi sulla sua
diagonale: $tr\left( A\right) =\sum_{i=1}^{n}a_{ii}$.

\subsection{Teorema di Rouch\'{e}-Capelli}

\begin{gather*}
\text{(1) Hp}\text{: }A\mathbf{x}=\mathbf{b}\text{ \`{e} un sistema lineare
con }n\text{ incognite} \\
\text{Ts}\text{:}\text{ }A\mathbf{x}=\mathbf{b}\text{ ammette almeno una
soluzione }\Longleftrightarrow \text{ }r\left( A|\mathbf{b}\right) =r\left(
A\right)  \\
\text{(2) Hp}\text{: }A\mathbf{x}=\mathbf{b}\text{ \`{e} un sistema lineare
con }n\text{ incognite; }r\left( A|\mathbf{b}\right) =r\left( A\right)  \\
\text{Ts}\text{:}\text{ esistono }n-r+1\text{ vettori colonna }\mathbf{v}%
_{0},\mathbf{v}_{1},...,\mathbf{v}_{n-r}\in \mathbf{K}^{n}\text{ tali che la
generica soluzione } \\
\text{del sistema \`{e} descritta da }\mathbf{x}=\mathbf{v}_{0}+t_{1}\mathbf{%
v}_{1}+...+t_{n-r}\mathbf{v}_{n-r}\text{, }t_{i}\in \mathbf{K}
\end{gather*}

$\mathbf{v}_{0}$ \`{e} usato con lo stesso significato del teorema di
struttura.

\textbf{Dim} Dimostro l'implicazione da sinistra a destra della prima tesi
in forma negata. Riduco a scala la matrice completa del sistema $\left[ A|%
\mathbf{b}\right] $ e ottengo, con il MEG, $\left[ U|\mathbf{b}^{\prime }%
\right] $. Pongo $r\left( A\right) =r\left( U\right) =r$, che \`{e} il
numero di pivot di $U$. A questo punto, $\left[ A|\mathbf{b}\right] $ ha due
possibili configurazioni: o la colonna dei termini noti non contiene alcun
pivot, oppure ne contiene uno (non pu\`{o} contenerne pi\`{u} d'uno perch%
\'{e} due pivot non possono stare sulla stessa colonna).%
\begin{equation*}
\left[ 
\begin{array}{cccccc}
0 & p_{1} & \ast & \ast & \ast &  \\ 
0 & 0 & p_{2} & \ast & \ast &  \\ 
... & ... & ... & ... & ... &  \\ 
0 & 0 & 0 & 0 & p_{r} &  \\ 
0 & 0 & 0 & 0 & 0 & 0%
\end{array}%
\right]
\end{equation*}

cio\`{e} tutti i pivot sono contenuti di $U$ e nessuno in $\mathbf{b}%
^{\prime }$: in questo caso $r\left( A\right) =r\left( A|\mathbf{b}\right) $%
; oppure

\begin{equation*}
\left[ 
\begin{array}{cccccc}
0 & p_{1} & \ast & \ast & \ast &  \\ 
0 & 0 & p_{2} & \ast & \ast &  \\ 
... & ... & ... & ... & ... &  \\ 
0 & 0 & 0 & 0 & p_{r} &  \\ 
0 & 0 & 0 & 0 & 0 & b_{r+1}^{\prime }%
\end{array}%
\right]
\end{equation*}

con $b_{r+1}^{\prime }\neq 0$, cio\`{e} la colonna dei termini noti contiene
un pivot e $r\left( A|\mathbf{b}\right) =r\left( A\right) +1$. Per
definizione di pivot, tutti gli elementi precedenti a $b_{r+1}^{\prime }$
devono essere nulli, quindi la riga $r+1$ descrive l'equazione $%
0=b_{r+1}^{\prime }$, che \`{e} impossibile. Dunque, se $r\left( A|\mathbf{b}%
\right) =r\left( A\right) +1$, il sistema non ha soluzione. Cos\`{\i} ho
dimostrato l'implicazione da sinistra a destra della prima tesi.

Per dimostrare la seconda tesi e l'implicazione da destra a sinistra della
prima tesi, suppongo ora $r\left( A\right) =r\left( A|\mathbf{b}\right) $:
cancello le eventuali righe nulle e riparto da un sistema con $r$ equazioni
in $n$ incognite. Chiamo variabili dipendenti le variabili corrispondenti
alle $r$ colonne contenenti i pivot; chiamo le altre variabili indipendenti.
Suppongo senza perdita di generalit\`{a} che le variabili dipendenti siano
le prime: $x_{1},...,x_{r}$ (se non lo sono, le riordino: sono variabili
mute). A questo punto, nella parte sinistra della matrice (le prime $r$
colonne) ho una scala perfetta. Le altre $n-r$ colonne sono quelle
corrispondenti a variabili indipendenti.%
\begin{equation*}
\left[ 
\begin{array}{cccccc}
p_{1} & \ast & \ast & \ast & \ast & b_{1}^{\prime } \\ 
0 & p_{2} & \ast & \ast & \ast & b_{2}^{\prime } \\ 
0 & 0 & p_{3} & \ast & \ast & b_{3}^{\prime } \\ 
... & ... & ... & ... & ... & ... \\ 
0 & 0 & 0 & p_{3} & \ast & b_{r+1}^{\prime }%
\end{array}%
\right]
\end{equation*}

Considero le seguenti equazioni, una per ogni variabile indipendente: $%
x_{r+1}=t_{1},...,x_{n}=t_{n-r}$, con $t_{i}\in \mathbf{K}$, e le aggiungo
al sistema. Ottengo una matrice a scala con $n$ righe e $n+1$ colonne.

\begin{equation*}
\left[ 
\begin{array}{ccccccccc}
p_{1} & \ast & \ast & \ast & \ast & \ast & \ast & \ast & b_{1}^{\prime } \\ 
0 & p_{2} & \ast & \ast & \ast & \ast & \ast & \ast & b_{2}^{\prime } \\ 
0 & 0 & ... & \ast & \ast & \ast & \ast & \ast & ... \\ 
0 & 0 & 0 & p_{r} & \ast & \ast & \ast & \ast & b_{r}^{\prime } \\ 
0 & 0 & 0 & 0 & 1 & \ast & \ast & \ast & t_{1} \\ 
0 & 0 & 0 & 0 & 0 & 1 & \ast & \ast & t_{2} \\ 
... & ... & ... & ... & ... & ... & ... & ... & ... \\ 
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & t_{n-r}%
\end{array}%
\right]
\end{equation*}

Le equazioni dalla $r+1$ alla $n$ sono risolte in partenza. Quindi uso il
metodo di Gauss-Jordan dal basso verso l'alto per trovare il valore
(parametrico) di ogni variabile $x_{1},...,x_{n}$, e ottengo%
\begin{equation*}
\left\{ 
\begin{array}{c}
x_{1}=v_{10}+v_{11}t_{1}+...+v_{1,n-r}t_{n-r} \\ 
... \\ 
x_{r}=v_{r0}+v_{r1}t_{1}+...+v_{r,n-r}t_{n-r} \\ 
x_{r+1}=t_{1} \\ 
... \\ 
x_{n}=t_{n-r}%
\end{array}%
\right.
\end{equation*}

dove $v_{ij}\in \mathbf{K}$ per ogni $i=1,...,r$, $j=0,...,n-r$. Quindi ogni
componente del vettore soluzione ha una parte costante: la componente
i-esima ha per parte costante $v_{i0}$. Essendo $r\left( A|\mathbf{b}\right)
=r\left( A\right) $, esiste almeno una soluzione, e ne ho scritta almeno
una; inoltre ho mostrato che esistono gli $n-r+1$ vettori della seconda tesi
tali che $\mathbf{x}=\mathbf{v}_{0}+t_{1}\mathbf{v}_{1}+...+t_{n-r}\mathbf{v}%
_{n-r}$ \`{e} soluzione $\forall $ $\left( t_{1},...,t_{n-r}\right) \in 
\mathbf{K}$. 
\begin{equation*}
\mathbf{x=}\left[ 
\begin{array}{c}
x_{1} \\ 
... \\ 
x_{r} \\ 
x_{r+1} \\ 
... \\ 
x_{n}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
v_{10}+\sum_{k=1}^{n-r}v_{1k}t_{k} \\ 
... \\ 
v_{r0}+\sum_{k=1}^{n-r}v_{rk}t_{k} \\ 
t_{1} \\ 
... \\ 
t_{n-r}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
v_{10} \\ 
... \\ 
v_{r0} \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right] +t_{1}\left[ 
\begin{array}{c}
v_{11} \\ 
... \\ 
v_{r1} \\ 
1 \\ 
... \\ 
0%
\end{array}%
\right] +...+t_{n-r}\left[ 
\begin{array}{c}
v_{1,n-r} \\ 
... \\ 
0 \\ 
... \\ 
1%
\end{array}%
\right] =\mathbf{v}_{0}+t_{1}\mathbf{v}_{1}+...+t_{n-r}\mathbf{v}_{n-r}
\end{equation*}

$\blacksquare $

Il numero di equazioni $m$ non ha alcuna influenza sull'enunciato. Se $%
r\left( A|\mathbf{b}\right) =r\left( A\right) =n$, ci sono $n$ pivot e,
essendoci $n$ colonne, non esistono variabili indipendenti: dunque la
matrice incompleta a scala \`{e} quadrata ed esiste un'unica soluzione,
priva di parametri. $\mathbf{v}_{0}$ (che viene detto soluzione speciale o
particolare) \`{e} lo stesso vettore del teorema di struttura: questo
significa che $t_{1}\mathbf{v}_{1}+...+t_{n-r}\mathbf{v}_{n-r}$ \`{e}
l'insieme delle soluzioni del sistema omogeneo associato. Infatti, essendo $%
\mathbf{v}_{0}$ ($t_{1}=...=t_{n-r}=0$) e $\mathbf{x}=\mathbf{v}_{0}+t_{1}%
\mathbf{v}_{1}+...+t_{n-r}\mathbf{v}_{n-r}$ soluzioni di $A\mathbf{x=b}$, si
dimostra che $\mathbf{x-v}_{0}$ \`{e} soluzione del sistema omogeneo: $%
A\left( \mathbf{x}-\mathbf{v}_{0}\right) =A\mathbf{x-}A\mathbf{v}_{0}=%
\mathbf{b-b}=0$, per la propriet\`{a} distributiva a destra del prodotto
matriciale.

\begin{enumerate}
\item \textit{Studiare le soluzioni del sistema }$\left[ 
\begin{array}{cccc}
5 & 2 & 1 & 1+k \\ 
2 & 2 & -2 & k \\ 
1 & -2 & 5 & 1+2k%
\end{array}%
\right] $\textit{\ al variare di }$k\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$\textit{.}

Dovendo le soluzioni dipendere da un parametro, mi aspetto che non ce ne sia
una sola (con $r\left( A\right) =r\left( A|b\right) $ avrei $3-3=0$
parametri liberi), e quindi che $r\left( A\right) =r\left( A|b\right) <3$.
Riduco a scala la matrice completa: scambio $\left( 1\right) $ e $\left(
3\right) $, faccio $\left( 2\right) =\left( 2\right) -2\left( 1\right) $ e $%
\left( 3\right) =\left( 3\right) -5\left( 1\right) $, faccio $\left(
3\right) =\left( 3\right) -2\left( 2\right) $.%
\begin{equation*}
\left[ 
\begin{array}{cccc}
5 & 2 & 1 & 1+k \\ 
2 & 2 & -2 & k \\ 
1 & -2 & 5 & 1+2k%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{cccc}
1 & -2 & 5 & 1+2k \\ 
2 & 2 & -2 & k \\ 
5 & 2 & 1 & 1+k%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{cccc}
1 & -2 & 5 & 1+2k \\ 
0 & 6 & -12 & -3k-2 \\ 
0 & 12 & -24 & -4-9k%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{cccc}
1 & -2 & 5 & 1+2k \\ 
0 & 6 & -12 & -3k-2 \\ 
0 & 0 & 0 & -3k%
\end{array}%
\right]
\end{equation*}

Se $-3k\neq 0$, $r\left( A\right) =2\neq r\left( A|b\right) =3$, dunque il
sistema non ha soluzione, per il teorema di Rouch\'{e}-Capelli.

Se $-3k=0$, $r\left( A\right) =r\left( A|b\right) =2$ ed esistono infinite
soluzioni dipendenti da un parametro. Il sistema diventa%
\begin{equation*}
\left[ 
\begin{array}{cccc}
1 & -2 & 5 & 1 \\ 
0 & 6 & -12 & -2 \\ 
0 & 0 & 0 & 0%
\end{array}%
\right]
\end{equation*}

dove $x$ e $y$ sono le variabili dipendenti, $z$ \`{e} la variabile
indipendente. Aggiungo al sistema l'equazione $z=t$ e uso il MEGJ: faccio $%
\left( 2\right) =\left( 2\right) +12\left( 3\right) $ e $\left( 1\right)
=\left( 1\right) -5\left( 3\right) $, $\left( 2\right) =\frac{1}{6}\left(
2\right) $, $\left( 1\right) =\left( 1\right) +2\left( 2\right) $%
\begin{equation*}
\left[ 
\begin{array}{cccc}
1 & -2 & 5 & 1 \\ 
0 & 6 & -12 & -2 \\ 
0 & 0 & 1 & t%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{cccc}
1 & -2 & 0 & 1-5t \\ 
0 & 6 & 0 & -2+12t \\ 
0 & 0 & 1 & t%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{cccc}
1 & -2 & 0 & 1-5t \\ 
0 & 1 & 0 & -\frac{1}{3}+2t \\ 
0 & 0 & 1 & t%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{cccc}
1 & 0 & 0 & \frac{1}{3}-t \\ 
0 & 1 & 0 & -\frac{1}{3}+2t \\ 
0 & 0 & 1 & t%
\end{array}%
\right]
\end{equation*}

Quindi $\left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] =\left[ 
\begin{array}{c}
\frac{1}{3}-t \\ 
-\frac{1}{3}+2t \\ 
t%
\end{array}%
\right] =\left[ 
\begin{array}{c}
\frac{1}{3} \\ 
-\frac{1}{3} \\ 
0%
\end{array}%
\right] +t\left[ 
\begin{array}{c}
-1 \\ 
2 \\ 
1%
\end{array}%
\right] =\mathbf{v}_{0}+t_{1}\mathbf{v}_{1}$.

Se avessi un sistema con due parametri, ad esempio $\left[ 
\begin{array}{cccc}
0 & 1 & -1 & 3%
\end{array}%
\right] $, dovrei agire similmente aggiungendo due equazioni: $\left[ 
\begin{array}{cccc}
0 & 1 & -1 & 3 \\ 
1 & 0 & 0 & t_{1} \\ 
0 & 0 & 1 & t_{2}%
\end{array}%
\right] $ con $x,z$ variabili indipendenti. Poi risolverei normalmente
scambiando $\left( 1\right) $ e $\left( 2\right) $, ecc.: 
\begin{equation*}
\left[ 
\begin{array}{cccc}
1 & 0 & 0 & t_{1} \\ 
0 & 1 & -1 & 3 \\ 
0 & 0 & 1 & t_{2}%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{cccc}
1 & 0 & 0 & t_{1} \\ 
0 & 1 & 0 & 3+t_{2} \\ 
0 & 0 & 1 & t_{2}%
\end{array}%
\right]
\end{equation*}

$\left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
3 \\ 
0%
\end{array}%
\right] +t_{1}\left[ 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right] +t_{2}\left[ 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right] $.

\item Se $A\in M_{\mathbf{K}}\left( m,n\right) $, $\mathbf{b\in K}^{m}$, $%
r\left( A\right) =r$:

Il sistema lineare $A\mathbf{x=b}$ ammette almeno una soluzione per ogni $%
\mathbf{b}$ se e solo se $r=m$ (perch\'{e} questo accada \`{e} necessario $%
m\leq n$, essendo $r\left( A\right) \leq \min \left\{ m,n\right\} $), cio%
\`{e} $\func{col}\left( A\right) =\mathbf{K}^{m}$ e $\dim \left( \func{Im}%
\left( \tciLaplace _{A}\right) \right) =m$ ($\tciLaplace _{A}:\mathbf{K}%
^{n}\rightarrow \mathbf{K}^{m}$ \`{e} suriettiva, cio\`{e} $m\leq n$, cio%
\`{e} $A$ \`{e} invertibile a destra).

Il sistema lineare $A\mathbf{x=b}$ ammette al pi\`{u} una soluzione per ogni 
$\mathbf{b}$ se e solo se la soluzione non esiste o dipende da zero
parametri, quindi $r=n$, cio\`{e} le colonne di $A$ sono linearmente
indipendenti e $\ker \left( A\right) =\left\{ \mathbf{0}\right\} $ ($%
\tciLaplace _{A}$ \`{e} iniettiva, cio\`{e} $n\leq m$, cio\`{e} $A$ \`{e}
invertibile a sinistra).

Il sistema lineare $A\mathbf{x=b}$ ammette esattamente una soluzione per
ogni $\mathbf{b}$ se e solo se $r=m$ e $r=n$, cio\`{e} $A$ \`{e} quadrata e
ha rango massimo ($\tciLaplace _{A}$ \`{e} biunivoca; se $\dim V\neq \dim W$
l'applicazione lineare non pu\`{o} essere invertibile).

Dunque, se $m=n$ e $\exists $ $\mathbf{b}_{1}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}:A\mathbf{x=b}_{1}$ ha pi\`{u} di una soluzione, significa che $r<n$,
dunque esiste $\mathbf{b}_{2}:A\mathbf{x=b}_{2}$ non ha alcuna soluzione; se 
$m=n$ e $\exists $ $\mathbf{b}_{1}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}:A\mathbf{x=b}_{1}$ non ha soluzione, significa che $r<n$, dunque esiste 
$\mathbf{b}_{2}:A\mathbf{x=b}_{2}$ ha infinite soluzioni (basta prendere $%
\mathbf{b}_{2}=\mathbf{0}$). Se $\exists $ $\mathbf{b}_{1}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{m}:A\mathbf{x=b}_{1}$ ha esattamente una soluzione, significa che $r=n$,
ma pu\`{o} esistere $\mathbf{b}_{2}:A\mathbf{x=b}_{2}$ non ha alcuna
soluzione (basta prendere una matrice non quadrata e un termine noto con
l'ultima componente non nulla).
\end{enumerate}

\textbf{Corollario (teorema di Cramer)}%
\begin{gather*}
\text{Hp}\text{: }A\mathbf{x=b}\text{ \`{e} un sistema lineare con }n\text{
equazioni e }n\text{ incognite; }r\left( A\right) =n \\
\text{Ts}\text{: }\forall \text{ }\mathbf{b\in K}^{n}\text{ esiste un'unica
soluzione di }A\mathbf{x=b}
\end{gather*}

\textbf{Dim} Riducendo a scala $A|\mathbf{b}$, si ottiene $U|\mathbf{b}%
^{\prime }$, con $U$ che \`{e} necessariamente una "scala perfetta"%
\begin{equation*}
\left[ 
\begin{array}{ccccc}
p_{1} & \ast & ... & \ast & b_{1}^{\prime } \\ 
0 & p_{2} & ... & \ast & b_{2}^{\prime } \\ 
... & ... & ... & ... & ... \\ 
0 & 0 & ... & p_{n} & b_{n}^{\prime }%
\end{array}%
\right]
\end{equation*}

dato che ci sono $n$ pivot, uno su ciascuna riga, uno su ciascuna colonna,
perch\'{e} $r\left( A\right) =n$. Non \`{e} possibile $r\left( A\right) \neq
r\left( A|\mathbf{b}\right) $ perch\'{e} non ci possono essere pivot sulla
stessa riga o sulla stessa colonna. Dunque, se $A\in M_{\mathbf{K}}\left(
n,n\right) $ e $r\left( A\right) =n$, ne segue $r\left( A\right) =r\left( A|%
\mathbf{b}\right) $: per il teorema di Rouch\'{e}-Capelli esiste almeno una
soluzione. Ma poich\'{e} $n-r=0$, non c'\`{e} alcun parametro libero e
dunque esiste un'unica soluzione. $\blacksquare $

Il sistema omogeneo associato ha dunque come unica soluzione $\mathbf{x=0}$.

\subsection{Fattorizzazione LU}

E' un modo per "memorizzare" il processo di una riduzione a scala di una
matrice $A$, cos\`{\i} da non doverlo ripetere per ogni $\mathbf{b}$.

\textbf{Def} Una matrice $A\in M_{\mathbf{K}}\left( n,n\right) $ si dice
triangolare superiore se $a_{ij}=0$ $\forall $ $i>j$. Una matrice $A\in M_{%
\mathbf{K}}\left( n,n\right) $ si dice triangolare inferiore se $a_{ij}=0$ $%
\forall $ $i<j$.%
\begin{equation*}
\text{MT superiore: }\left[ 
\begin{array}{ccc}
\ast & \ast & \ast \\ 
0 & \ast & \ast \\ 
0 & 0 & \ast%
\end{array}%
\right] \text{; MT inferiore: }\left[ 
\begin{array}{ccc}
\ast & 0 & 0 \\ 
\ast & \ast & 0 \\ 
\ast & \ast & \ast%
\end{array}%
\right]
\end{equation*}

Una matrice quadrata a scala \`{e} una matrice triangolare superiore. Non
vale l'implicazione inversa: una matrice pu\`{o} essere triangolare
superiore e avere due pivot sulla stessa colonna.

\begin{enumerate}
\item $\left[ 
\begin{array}{ccc}
0 & 1 & 2 \\ 
0 & 1 & 2 \\ 
0 & 0 & 1%
\end{array}%
\right] $ \`{e} triangolare superiore ma non a scala.
\end{enumerate}

\textbf{Teo}%
\begin{gather*}
\text{Hp}\text{: }A\in M_{\mathbf{K}}\left( m,n\right) \text{ si pu\`{o}
ridurre a scala senza l'utilizzo dello scambio di due righe} \\
\text{Ts}\text{: }\exists \text{ }L\in M_{\mathbf{K}}\left( m,m\right) \text{
triangolare inferiore e }U\in M_{\mathbf{K}}\left( m,n\right) \text{ a scala
tali che }A=LU
\end{gather*}

Se $m=n$, $U$ \`{e} triangolare superiore: in tal caso $L$ sta per "lower" e 
$U$ per "upper".

L'idea \`{e} codificare in $L$ le operazioni del metodo di eliminazione di
Gauss.

\begin{enumerate}
\item \textbf{Scambio di due righe}

Sia $A\in M_{\mathbf{K}}\left( m,n\right) $ una matrice di cui si vogliono
scambiare le righe $R_{i}$ e $R_{j}$; sia $B$ la matrice $A$ dopo lo
scambio. Si cerca una matrice $S:SA=B$.

$S\in M_{\mathbf{K}}\left( m,m\right) $. Si consideri l'esempio di $A=\left[ 
\begin{array}{ccc}
a & b & c \\ 
d & e & f%
\end{array}%
\right] $: per ottenere $B=\left[ 
\begin{array}{ccc}
d & e & f \\ 
a & b & c%
\end{array}%
\right] $ calcolando $SA$ dev'essere $S=\left[ 
\begin{array}{cc}
0 & 1 \\ 
1 & 0%
\end{array}%
\right] $. Se $A=\left[ 
\begin{array}{cc}
a & b \\ 
c & d \\ 
e & f%
\end{array}%
\right] $, per ottenere $B=\left[ 
\begin{array}{cc}
e & f \\ 
c & d \\ 
a & b%
\end{array}%
\right] $ dev'essere $S=\left[ 
\begin{array}{ccc}
0 & 0 & 1 \\ 
0 & 1 & 0 \\ 
1 & 0 & 0%
\end{array}%
\right] $.

In generale, se $S$ \`{e} la matrice incognita, $s_{lk}=\left\{ 
\begin{array}{c}
1\text{ se }\left( l=i,k=j\right) \text{ o }\left( l=j,k=i\right) \text{, se 
}l=k\neq i,j \\ 
0\text{ altrimenti}%
\end{array}%
\right. $: $S$ \`{e} la matrice identit\`{a} con le righe $i$ e $j$
scambiate.

Lo scambio \`{e} reversibile: se $SA=B$, $SB=A$. Inoltre, dato che $S\left(
SA\right) =A$, $S^{2}A=A$, quindi $S^{2}=Id_{m}$ e $S=S^{-1}$.

\item \textbf{Moltiplicazione di una riga per un coefficiente non nullo}

Sia $A\in M_{\mathbf{K}}\left( m,n\right) $ una matrice di cui si vuole
sostituire la riga $R_{i}$ con $\lambda R_{i}$, con $\lambda \in \mathbf{K}%
-\left\{ 0\right\} $; sia $B$ la matrice $A$ dopo la moltiplicazione. Si
cerca una matrice $S:SA=B$.

$S\in M_{\mathbf{K}}\left( m,m\right) $. Si consideri l'esempio di $A=\left[ 
\begin{array}{ccc}
a & b & c \\ 
d & e & f%
\end{array}%
\right] $: per ottenere $B=\left[ 
\begin{array}{ccc}
a & b & c \\ 
\lambda d & \lambda e & \lambda f%
\end{array}%
\right] $ dev'essere $S=\left[ 
\begin{array}{cc}
1 & 0 \\ 
0 & \lambda%
\end{array}%
\right] $.

In generale, se $S$ \`{e} la matrice incognita, $s_{lk}=\left\{ 
\begin{array}{c}
\lambda \text{ se }l=k=i \\ 
1\text{ se }l=k\neq i \\ 
0\text{ altrimenti}%
\end{array}%
\right. $, cio\`{e} $S$ \`{e} la matrice identit\`{a} con $\lambda $ invece
che $1$ nella posizione $\left( i,i\right) $.

L'operazione \`{e} reversibile: se $SA=B$ e si \`{e} fatto $\lambda R_{i}$, $%
S^{\prime }B=A$, con $s_{lk}^{\prime }=\left\{ 
\begin{array}{c}
\frac{1}{\lambda }\text{ se }l=k=i \\ 
1\text{ se }l=k\neq i \\ 
0\text{ altrimenti}%
\end{array}%
\right. $. $S^{\prime }$ \`{e} la matrice inversa di $S$.

\item \textbf{Sostituzione di una riga con la sua somma con un multiplo di
un'altra}

Sia $A\in M_{\mathbf{K}}\left( m,n\right) $ una matrice di cui si vuole
sostituire la riga $R_{i}$ con $R_{i}+\lambda R_{j}$, con $\lambda \in 
\mathbf{K}-\left\{ 0\right\} $; sia $B$ la matrice $A$ dopo l'operazione. Si
cerca una matrice $E:EA=B$.

$E\in M_{\mathbf{K}}\left( m,m\right) $. Si consideri l'esempio di $A=\left[ 
\begin{array}{cc}
a & b \\ 
c & d \\ 
e & f%
\end{array}%
\right] $, per ottenere $A=\left[ 
\begin{array}{cc}
a & b \\ 
c+\lambda a & d+\lambda b \\ 
e & f%
\end{array}%
\right] $ dev'essere $E=\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
\lambda & 1 & 0 \\ 
0 & 0 & 1%
\end{array}%
\right] $.

In generale, se $E$ \`{e} la matrice incognita, $e_{lk}=\left\{ 
\begin{array}{c}
\lambda \text{ se }l=i\text{ e }k=j \\ 
1\text{ se }l=k \\ 
0\text{ altrimenti}%
\end{array}%
\right. $.

L'operazione \`{e} reversibile: se $EA=B$ e si \`{e} fatto $%
R_{i}=R_{i}+\lambda R_{j}$, $E^{-1}B=A$, con $e_{lk}^{-1}=\left\{ 
\begin{array}{c}
-\lambda \text{ se }l=i\text{ e }k=j \\ 
1\text{ se }l=k \\ 
0\text{ altrimenti}%
\end{array}%
\right. $. $E^{-1}$ \`{e} quindi la matrice inversa di $E$: $%
E^{-1}B=A\Longleftrightarrow EE^{-1}B=EA\Longleftrightarrow
EE^{-1}B=B\Longleftrightarrow EE^{-1}=Id_{m}$. Ogni matrice $E^{-1}$ (o $E$)
siffatta \`{e} triangolare: inferiore se $i>j$, superiore se $i<j$.
\end{enumerate}

\textbf{Dim} Nell'ipotesi del teorema, per la riduzione si possono usare
solo operazioni di tipo 3: cio\`{e}, eseguendo $t$ operazioni, si fa $%
E_{t}\left( ...\left( E_{3}\left( E_{2}\left( E_{1}A\right) \right) \right)
\right) =U$, con $U$ a scala. In ciascuna operazione, si somma a una riga $%
R_{i}$ della matrice a destra $\lambda R_{j}$, con $j<i$ (questo perch\'{e}
nel MEG si procede dall'alto verso il basso, azzerando gli elementi sotto
ciascun pivot: quindi si avr\`{a} $l>k$). Per la propriet\`{a} associativa
del prodotto matriciale vale $E_{t}...E_{3}E_{2}E_{1}A=U$. Voglio spostare a
destra le $E$: poich\'{e} $E_{t}$ ha per inversa $E_{t}^{-1}$, premoltiplico
lato sinistro e destro per $E_{t}^{-1}$:%
\begin{equation*}
E_{t}^{-1}\left( E_{t}...E_{3}E_{2}E_{1}A\right)
=E_{t}^{-1}U\Longleftrightarrow \left( E_{t}^{-1}E_{t}\right)
...E_{3}E_{2}E_{1}A=E_{t}^{-1}U\Longleftrightarrow
Id_{n}E_{t-1}...E_{3}E_{2}E_{1}A=E_{t}^{-1}U\Longleftrightarrow
E_{t-1}...E_{3}E_{2}E_{1}A=E_{t}^{-1}U
\end{equation*}

Procedo cos\`{\i} anche per $E_{t-1},...,E_{1}$ e ottengo%
\begin{equation*}
A=E_{1}^{-1}E_{2}^{-1}E_{3}^{-1}...E_{t-1}^{-1}E_{t}^{-1}U
\end{equation*}

Poich\'{e} ciascuna $E_{i}^{-1}$ \`{e} triangolare inferiore e il prodotto
di matrici triangolari inferiori \`{e} una matrice triangolare inferiore*,
chiamo $L$ la matrice triangolare inferiore $%
E_{1}^{-1}E_{2}^{-1}E_{3}^{-1}...E_{t-1}^{-1}E_{t}^{-1}$ e vale $A=LU$. $%
\blacksquare $

*\textbf{Proposizione}%
\begin{eqnarray*}
\text{Hp} &\text{: }&L_{1},L_{2}\text{ sono matrici triangolari inferiori} \\
\text{Ts} &\text{: }&L_{1}L_{2}\text{ \`{e} triangolare inferiore, cio\`{e} }%
i<j\Longrightarrow \left( L_{1}L_{2}\right) _{ij}=0
\end{eqnarray*}

\textbf{Dim} Cerco un'espressione per scrivere il generico elemento $\left(
L_{1}L_{2}\right) _{ij}$, con $i<j$. $\left( L_{1}L_{2}\right)
_{ij}=\sum_{k=1}^{m}\left( L_{1}\right) _{ik}\left( L_{2}\right)
_{kj}=\sum_{k=1}^{i}\left( L_{1}\right) _{ik}\left( L_{2}\right)
_{kj}+\sum_{k=i+1}^{m}\left( L_{1}\right) _{ik}\left( L_{2}\right) _{kj}=0+0$%
: il secondo fattore del primo addendo si annulla perch\'{e} $k\leq i<j$ e $%
L_{2}$ \`{e} triangolare inferiore, il primo fattore del secondo addendo si
annulla perch\'{e} $i<k$ e $L_{1}$ \`{e} triangolare inferiore. $%
\blacksquare $

\textbf{Corollario} 
\begin{equation*}
E_{1}^{-1}E_{2}^{-1}E_{3}^{-1}...E_{t-1}^{-1}E_{t}^{-1}\text{ \`{e} una
matrice triangolare inferiore}
\end{equation*}

Si nota che la generica matrice $E_{i}$, che incorpora l'operazione di
sostituzione di una riga con la sua somma con un multiplo di un'altra, ha
tutti gli elementi sulla diagonale uguali a $1$. Questa propriet\`{a} si
estende al prodotto di due matrici siffatte.%
\begin{eqnarray*}
\text{Hp}\text{: } &&L_{1},L_{2}\text{ sono matrici triangolari inferiori
tali che }\left( L_{1}\right) _{ii}=\left( L_{2}\right) _{ii}=1 \\
\text{Ts}\text{: } &&\left( L_{1}L_{2}\right) _{ii}=1
\end{eqnarray*}

\textbf{Dim} Cerco un'espressione per scrivere il generico elemento $\left(
L_{1}L_{2}\right) _{ii}$. $\left( L_{1}L_{2}\right)
_{ii}=\sum_{k=1}^{m}\left( L_{1}\right) _{ik}\left( L_{2}\right)
_{ki}=\sum_{k=1}^{i-1}\left( L_{1}\right) _{ik}\left( L_{2}\right)
_{ki}+\left( L_{1}\right) _{ii}\left( L_{2}\right)
_{ii}+\sum_{k=i+1}^{m}\left( L_{1}\right) _{ik}\left( L_{2}\right)
_{ki}=0+1\cdot 1+0$: il secondo fattore del primo addendo si annulla perch%
\'{e} $k<i$ e $L_{2}$ \`{e} triangolare inferiore, il primo fattore del
terzo addendo si annulla perch\'{e} $i<k$ e $L_{1}$ \`{e} triangolare
inferiore, il secondo addendo \`{e} $1$. $\blacksquare $

Si pu\`{o} quindi riesprimere il teorema sulla fattorizzazione LU
aggiungendo un dettaglio:

\textbf{Teo}%
\begin{gather*}
\text{Hp}\text{: }A\in M_{\mathbf{K}}\left( m,n\right) \text{ si pu\`{o}
ridurre a scala senza l'utilizzo dello scambio di due righe} \\
\text{Ts}\text{: }\exists \text{ }L\in M_{\mathbf{K}}\left( m,m\right) \text{%
, triangolare inferiore con }1\text{ sulla diagonale } \\
\text{principale, e }U\in M_{\mathbf{K}}\left( m,n\right) \text{, a scala,
tali che }A=LU
\end{gather*}

\begin{enumerate}
\item Voglio scrivere $A=\left[ 
\begin{array}{ccc}
1 & -4 & 1 \\ 
2 & -6 & 5 \\ 
1 & -2 & 5%
\end{array}%
\right] $ come $LU$, dove $U$ \`{e} $A$ ridotta a scala e $L$ \`{e} il
prodotto delle matrici triangolari necessarie per la riduzione. Voglio
mettere a zero $a_{21}$ e $a_{31}$: devo fare quindi $\left( 2\right)
=\left( 2\right) -2\left( 1\right) $ e $\left( 3\right) =\left( 3\right)
-\left( 1\right) $, dunque la prima matrice necessaria \`{e} $E_{1}=\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
-2 & 1 & 0 \\ 
0 & 0 & 1%
\end{array}%
\right] $, la seconda \`{e} $E_{2}$ $=\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
-1 & 0 & 1%
\end{array}%
\right] $, il loro prodotto \`{e} $E_{1}E_{2}=\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
-2 & 1 & 0 \\ 
-1 & 0 & 1%
\end{array}%
\right] $. Il risultato \`{e} $E_{2}E_{1}A=\left[ 
\begin{array}{ccc}
1 & -4 & 1 \\ 
0 & 2 & 3 \\ 
0 & 2 & 4%
\end{array}%
\right] $. Ora devo fare $\left( 3\right) =\left( 3\right) -\left( 2\right) $%
: la matrice necessaria \`{e} $E_{3}=\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & -1 & 1%
\end{array}%
\right] $ e ottengo $E_{3}E_{2}E_{1}A=\left[ 
\begin{array}{ccc}
1 & -4 & 1 \\ 
0 & 2 & 3 \\ 
0 & 0 & 1%
\end{array}%
\right] $, che \`{e} la matrice $U$ ridotta a scala. Ora calcolo il prodotto 
$E_{1}^{-1}E_{2}^{-1}E_{3}^{-1}U$, sapendo quali sono le matrici inverse: $%
\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
-2 & 1 & 0 \\ 
0 & 0 & 1%
\end{array}%
\right] ^{-1}\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
-1 & 0 & 1%
\end{array}%
\right] ^{-1}\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & -1 & 1%
\end{array}%
\right] ^{-1}\left[ 
\begin{array}{ccc}
1 & -4 & 1 \\ 
0 & 2 & 3 \\ 
0 & 0 & 1%
\end{array}%
\right] =$ $\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
2 & 1 & 0 \\ 
0 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
1 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 1 & 1%
\end{array}%
\right] \left[ 
\begin{array}{ccc}
1 & -4 & 1 \\ 
0 & 2 & 3 \\ 
0 & 0 & 1%
\end{array}%
\right] $ e $L=\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
2 & 1 & 0 \\ 
0 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
1 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 1 & 1%
\end{array}%
\right] =\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
2 & 1 & 0 \\ 
1 & 0 & 1%
\end{array}%
\right] \left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 1 & 1%
\end{array}%
\right] =\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
2 & 1 & 0 \\ 
1 & 1 & 1%
\end{array}%
\right] $.
\end{enumerate}

\subsubsection{Applicazione ai sistemi lineari}

Sia $A\in M_{\mathbf{K}}\left( n,n\right) $: in tal caso $A$ ridotta a scala 
\`{e} $U$ triangolare superiore (una matrice ridotta a scala non \`{e}
triangolare superiore in generale perch\'{e} la definizione di triangolare
si applica a matrici quadrate). Voglio risolvere $A\mathbf{x=b}$,
equivalente a $\left( LU\right) \mathbf{x=b}\Longleftrightarrow L\left( U%
\mathbf{x}\right) =\mathbf{b}$, che \`{e} equivalente a risolvere prima $L%
\mathbf{y=b}$, con $\mathbf{y}$ nuovo vettore incognita, e poi $U\mathbf{x=y}
$. Risolvere questi due sistemi \`{e} pi\`{u} vantaggioso perch\'{e} $L$ e $%
U $ sono matrici triangolari, quindi \`{e} pi\`{u} facile risolvere i
sistemi associati.

Lo stesso vale se $A$ non \`{e} quadrata e $U$ \`{e} a scala.

E' poi opportuno chiedersi come applicare la fattorizzazione LU nel caso in
cui siano necessari scambi di riga per ridurre $A$ a scala: questa
situazione non entra nelle ipotesi del teorema. Si pu\`{o} per\`{o} fare una
prima riduzione a scala identificando gli scambi necessari, che si possono
effettuare con le matrici $S_{1},...,S_{t}$, del tipo 1. Si calcola $%
S_{t}....S_{2}S_{1}A$: questa matrice pu\`{o} essere ridotta a scala senza
scambi, e ad essa si applica il teorema: $S_{t}....S_{2}S_{1}A=LU$.

\subsection{Calcolo della matrice inversa}

Considero il caso di matrici quadrate: devo risolvere $A\mathbf{x}=\mathbf{b}
$, con $A\in M_{\mathbf{K}}\left( n,n\right) $. Se $A$ fosse invertibile a
sinistra, potrei premoltiplicare a sinistra e a destra per $A^{-1}$: $A^{-1}A%
\mathbf{x}=A^{-1}\mathbf{b}\Longleftrightarrow Id_{n}\mathbf{\mathbf{x}}%
=A^{-1}\mathbf{\mathbf{b}}\Longleftrightarrow \mathbf{x}=A^{-1}\mathbf{b}$.
Quindi conoscere l'inversa permette di risolvere molto in fretta il sistema,
facendo il prodotto tra $A^{-1}\in M_{\mathbf{K}}\left( n,n\right) $ e $%
\mathbf{b}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$. Questo metodo \`{e} vantaggioso solo in alcuni casi, come si vedr\`{a}
ora.

Sia $A\in M_{\mathbf{K}}\left( 2,2\right) $, $A=\left[ 
\begin{array}{cc}
a & b \\ 
c & d%
\end{array}%
\right] $, per quali $a,b,c,d\in \mathbf{K}$ esiste $A^{-1}$? Cerco
l'inversa destra $X:AX=Id_{2}$, $X=\left[ 
\begin{array}{cc}
x_{11} & x_{12} \\ 
x_{21} & x_{22}%
\end{array}%
\right] $. Dev'essere $AX=\left[ 
\begin{array}{cc}
ax_{11}+bx_{21} & ax_{12}+bx_{22} \\ 
cx_{11}+dx_{21} & cx_{12}+dx_{22}%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
1 & 0 \\ 
0 & 1%
\end{array}%
\right] $: apparentemente cercare l'inversa destra si \`{e} tradotto in un
sistema lineare a quattro incognite e quattro equazioni. In realt\`{a} per%
\`{o} \`{e} un insieme di due sistemi, ciascuno dei quali a due incognite e
due equazioni: $\left[ 
\begin{array}{c}
ax_{11}+bx_{21} \\ 
cx_{11}+dx_{21}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
1 \\ 
0%
\end{array}%
\right] $ e $\left[ 
\begin{array}{c}
ax_{12}+bx_{22} \\ 
cx_{12}+dx_{22}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
1%
\end{array}%
\right] $. La matrice dei coefficienti \`{e} la stessa.

Se ne ricava che calcolare l'inversa destra di una matrice $n\times n$
equivale a risolvere $n$ sistemi lineari quadrati, cio\`{e} con $n$
equazioni in $n$ incognite.%
\begin{equation*}
A\mathbf{x}=Id_{n}\Longleftrightarrow A\left[ 
\begin{array}{c}
x_{11} \\ 
x_{21} \\ 
... \\ 
x_{n1}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
1 \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right] \text{, }A\left[ 
\begin{array}{c}
x_{12} \\ 
x_{22} \\ 
... \\ 
x_{n2}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
1 \\ 
... \\ 
0%
\end{array}%
\right] \text{,..., }A\left[ 
\begin{array}{c}
x_{1n} \\ 
x_{2n} \\ 
... \\ 
x_{nn}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
0 \\ 
... \\ 
1%
\end{array}%
\right]
\end{equation*}

Tutti condividono la matrice dei coefficienti e tutti, per il teorema di
Cramer, sono risolvibili se (e solo se?) $r\left( A\right) =n$.

Poich\'{e} invertire $A$ richiede di risolvere $n$ sistemi lineari, \`{e}
conveniente risolvere dei sistemi lineari con il metodo dell'inversa solo se
occorre risolvere pi\`{u} di $n$ sistemi.

Tornando al caso $2\times 2$, voglio risolvere simultaneamente due sistemi: $%
\left[ 
\begin{array}{ccc}
a & b & 1 \\ 
c & d & 0%
\end{array}%
\right] $ e $\left[ 
\begin{array}{ccc}
a & b & 0 \\ 
c & d & 1%
\end{array}%
\right] $; dal momento che il processo di riduzione \`{e} determinato
unicamente dalla matrice dei coefficienti, i due sistemi possono essere
risolti in parallelo usando un'unica matrice $\left[ 
\begin{array}{cccc}
a & b & 1 & 0 \\ 
c & d & 0 & 1%
\end{array}%
\right] =\left[ A|Id_{2}\right] $. Applico il MEG. Suppongo che la prima
colonna sia non nulla e $a\neq 0$; faccio $\left( 1\right) =\frac{1}{a}%
\left( 1\right) $ e $\left( 2\right) =\left( 2\right) -c\left( 1\right) $:
ottengo $\left[ 
\begin{array}{cccc}
1 & \frac{b}{a} & \frac{1}{a} & 0 \\ 
0 & d-c\frac{b}{a} & -\frac{c}{a} & 1%
\end{array}%
\right] $.

Se $d-c\frac{b}{a}=0$, la seconda equazione del secondo sistema diventa $0=1$%
, quindi il secondo sistema \`{e} impossibile e la matrice non \`{e}
invertibile.

Se $d-c\frac{b}{a}\neq 0$ ($ad-bc\neq 0$), faccio $\left( 2\right) =\frac{1}{%
d-\frac{bc}{a}}\left( 2\right) $ e ottengo $\left[ 
\begin{array}{cccc}
1 & \frac{b}{a} & \frac{1}{a} & 0 \\ 
0 & 1 & -\frac{c}{ad-bc} & \frac{a}{ad-bc}%
\end{array}%
\right] $: uso il MEGJ e faccio $\left( 1\right) =\left( 1\right) -\frac{b}{a%
}\left( 2\right) $, quindi ho $\left[ 
\begin{array}{cccc}
1 & 0 & \frac{d}{ad-bc} & -\frac{b}{ad-bc} \\ 
0 & 1 & -\frac{c}{ad-bc} & \frac{a}{ad-bc}%
\end{array}%
\right] =\left[ Id_{2}|A^{-1}\right] $. La formula generica per l'inversa
destra di una matrice $2\times 2$ \`{e} quindi $A^{-1}=\frac{1}{ad-bc}\left[ 
\begin{array}{cc}
d & -b \\ 
-c & a%
\end{array}%
\right] $. La matrice $A^{\ast }=\left[ 
\begin{array}{cc}
d & -b \\ 
-c & a%
\end{array}%
\right] $ \`{e} detta matrice aggiunta o matrice dei complementi algebrici.
Si nota che $A^{\ast }=\left( trA\right) Id-A$.

Si nota che per l'invertibilit\`{a} \`{e} opportuno supporre che la prima
colonna sia non nulla, perch\'{e} affinch\'{e} la formula valga, dovendo
essere $ad-bc\neq 0$, se $a=0$ allora $b,c\neq 0$. Nel caso di $a=0$ e $%
c\neq 0$ si ottiene infatti la stessa formula. Inoltre nel caso $2\times 2$
l'invertibilit\`{a} dipende da un unico numero calcolato con gli elementi
della matrice: $ad-bc$.

\subsection{Determinante}

\textbf{Def} Il determinante \`{e} una funzione scalare $\det :M_{\mathbf{K}%
}\left( n,n\right) \rightarrow \mathbf{K}$, che associa a ogni matrice
quadrata $A$ un numero, definita ricorsivamente nel modo seguente:

\begin{description}
\item[-] se $n=1$ $\det A=\det \left( \left[ a_{11}\right] \right) =a_{11}$

\item[-] se $n>1$ $\det A=\sum_{j=1}^{n}\left( -1\right) ^{i+j}a_{ij}\det 
\hat{A}_{ij}$
\end{description}

dove $i$ \`{e} un indice di riga fissato, $a_{ij}$ \`{e} l'elemento di $A$
alla i-esima riga e j-esima colonna, $\hat{A}_{ij}$ \`{e} una matrice $%
\left( n-1\right) \times \left( n-1\right) $ ottenuta a partire da $A$,
cancellando l'i-esima riga e la j-esima colonna. Il cappello sopra una
lettera indica un oggetto ricavato da un altro mediante cancellazioni.
Equivalentemente $\det A=\sum_{i=1}^{n}\left( -1\right) ^{i+j}a_{ij}\det 
\hat{A}_{ij}$, dove $j$ \`{e} un indice di colonna fissato: il determinante
non dipende dalla scelta della riga o della colonna.

\begin{enumerate}
\item Considero $A\in M_{\mathbf{K}}\left( 2,2\right) $, $A=\left[ 
\begin{array}{cc}
a_{11} & a_{12} \\ 
a_{21} & a_{22}%
\end{array}%
\right] $, e calcolo $\det A$ usando la riga $2$. $\det
A_{i=2}=\sum_{j=1}^{2}\left( -1\right) ^{2+j}a_{2j}\det \hat{A}_{2j}=\left(
-1\right) ^{2+1}a_{21}\det \hat{A}_{21}+\left( -1\right) ^{2+2}a_{22}\det 
\hat{A}_{22}=-a_{21}a_{12}+a_{22}a_{11}=a_{11}a_{22}-a_{12}a_{21}$. Se
chiamo $A=\left[ 
\begin{array}{cc}
a & b \\ 
c & d%
\end{array}%
\right] $, $\det A=ad-bc$. Ora calcolo $\det A$ usando la colonna $1$. $\det
A_{j=1}=\sum_{i=1}^{2}\left( -1\right) ^{i+1}a_{i1}\det \hat{A}_{i1}=\left(
-1\right) ^{1+1}a_{11}\det \hat{A}_{11}+\left( -1\right) ^{2+1}a_{21}\det 
\hat{A}_{21}=a_{11}a_{22}-a_{21}a_{12}$.

\item Considero $A\in M_{\mathbf{K}}\left( 3,3\right) $, $A=\left[ 
\begin{array}{ccc}
9 & 2 & -2 \\ 
2 & 2 & 0 \\ 
-2 & 0 & 2%
\end{array}%
\right] $, e calcolo $\det A$ usando la colonna $1$. $\det
A_{j=1}=\sum_{i=1}^{3}\left( -1\right) ^{i+1}a_{i1}\det \hat{A}_{i1}=\left(
-1\right) ^{1+1}9\det \left[ 
\begin{array}{cc}
2 & 0 \\ 
0 & 2%
\end{array}%
\right] +\left( -1\right) ^{2+1}2\det \left[ 
\begin{array}{cc}
2 & -2 \\ 
0 & 2%
\end{array}%
\right] +\left( -1\right) ^{3+1}\left( -2\right) \det \left[ 
\begin{array}{cc}
2 & -2 \\ 
2 & 0%
\end{array}%
\right] $, cio\`{e}, usando la formula appena trovata, $9\cdot 4-2\cdot
4-2\left( 4\right) =20$. Calcolo ora $\det A$ usando la riga $2$: $\det
A_{i=2}=\sum_{j=1}^{3}\left( -1\right) ^{2+j}a_{2j}\det \hat{A}_{2j}=$ $%
\left( -1\right) ^{2+1}2\det \left[ 
\begin{array}{cc}
2 & -2 \\ 
0 & 2%
\end{array}%
\right] +\left( -1\right) ^{2+2}2\det \left[ 
\begin{array}{cc}
9 & -2 \\ 
-2 & 2%
\end{array}%
\right] +\left( -1\right) ^{2+3}0\det \left[ 
\begin{array}{cc}
9 & 2 \\ 
-2 & 0%
\end{array}%
\right] =-2\cdot 4+2\left( 18-4\right) =20$.
\end{enumerate}

Ha senso chiedersi come scegliere la linea con cui calcolare il
determinante. Usando la riga 2 nell'esempio precedente, grazie alla presenza
di uno zero, si \`{e} potuto evitare di calcolare il determinante del terzo
addendo: in generale, \`{e} sensato usare la linea con pi\`{u} zeri.

\subsubsection{Propriet\`{a} e MEG}

\begin{enumerate}
\item Se $A$ ha una linea (riga o colonna) nulla, $\det A=0$. Si dimostra
calcolando il determinante usando quella linea.

\item Se $U$ \`{e} triangolare, $\det U=u_{11}u_{22}...u_{nn}$. Se $U$ \`{e}
triangolare superiore, si dimostra calcolando il determinante usando la
prima colonna: $\det U_{j=1}=\left( -1\right) ^{1+1}u_{11}\det \hat{U}%
_{11}=u_{11}\left( \left( -1\right) ^{2+2}u_{22}\det \hat{U}_{22}\right)
=...=u_{11}u_{22}...u_{nn}$. Se $U$ \`{e} triangolare inferiore, si dimostra
usando la prima riga.

\begin{enumerate}
\item Essendo $Id_{n}$ triangolare, $\det Id_{n}=1$: questo fatto si esprime
dicendo che il determinante \`{e} normalizzato, o si chiama propriet\`{a} di
normalizzazione del determinante; il determinante \`{e} una funzione che
associa l'elemento neutro del prodotto in $M_{\mathbf{K}}\left( n,n\right) $
all'elemento neutro del prodotto in $\mathbf{K}$.

\item Se $U$ \`{e} una matrice a scala (quadrata, altrimenti non ha senso
parlare di determinante) con $r\left( U\right) =n$, $U=\left[ 
\begin{array}{cccc}
p_{1} &  &  &  \\ 
& p_{2} &  &  \\ 
&  & ... &  \\ 
&  &  & p_{n}%
\end{array}%
\right] $ e, essendo $U$ triangolare superiore, $\det
U=p_{1}p_{2}...p_{n}\neq 0$ per definizione di pivot. Se $r\left( U\right)
<n $, c'\`{e} almeno una colonna priva di pivot e l'elemento di quella
colonna che giace sulla diagonale principale \`{e} nullo. Quindi $\det U=0$.
\end{enumerate}

\item Come si comporta il determinante rispetto al prodotto matriciale?

\textbf{Teorema (Binet)}%
\begin{eqnarray*}
\text{Hp}\text{: } &&A,B\in M_{\mathbf{K}}\left( n,n\right)  \\
\text{Ts}\text{: } &&\det \left( AB\right) =\det A\det B
\end{eqnarray*}

\textbf{Corollario}%
\begin{eqnarray*}
\text{Hp} &\text{: }&A,B\in M_{\mathbf{K}}\left( n,n\right) \\
\text{Ts} &\text{: }&\det \left( AB\right) =\det \left( BA\right)
\end{eqnarray*}

Infatti $\det \left( AB\right) =\det A\det B=\det B\det A=\det \left(
BA\right) $, perch\'{e} il prodotto sul campo $\mathbf{K}$ \`{e} commutativo.

\item Come si comporta il determinante rispetto al prodotto per scalari?

Data $A\in M_{\mathbf{K}}\left( n,n\right) $, $\lambda \in \mathbf{K}$, $%
\det \left( \lambda A\right) =\lambda ^{n}\det A$. Infatti $\det \left(
\lambda A\right) =\det \left( \lambda Id_{n}A\right) =\det \left( \lambda
Id_{n}\right) \det A=\lambda ^{n}\det A$, perch\'{e} $\lambda Id_{n}$ \`{e}
una matrice con tutti $\lambda $ sulla diagonale e zero altrove. Ne segue
che il determinante non \`{e} lineare rispetto ad $A$.

\item Come si comporta il determinante rispetto alla somma?

Data $A\in M_{\mathbf{K}}\left( n,n\right) $, scrivo la prima riga di $A$
come somma dei vettori $\mathbf{v}$ e $\mathbf{w}$.%
\begin{equation*}
A=\left[ 
\begin{array}{c}
\mathbf{v+w} \\ 
R_{2}\left( A\right) \\ 
... \\ 
R_{n}\left( A\right)%
\end{array}%
\right]
\end{equation*}

Definisco $A_{\mathbf{v}}=\left[ 
\begin{array}{c}
\mathbf{v} \\ 
R_{2}\left( A\right) \\ 
... \\ 
R_{n}\left( A\right)%
\end{array}%
\right] $, $A_{\mathbf{w}}=\left[ 
\begin{array}{c}
\mathbf{w} \\ 
R_{2}\left( A\right) \\ 
... \\ 
R_{n}\left( A\right)%
\end{array}%
\right] $. Allora $\det A=\det A_{\mathbf{v}}+\det A_{\mathbf{w}}$. Lo
stesso vale per una qualsiasi riga o colonna di $A$ che possa essere scritta
come somma di due vettori: si dice che il determinante \`{e} multilineare,
cio\`{e}, fissate arbitrariamente tutte le righe (colonne) tranne una, \`{e}
una funzione lineare della riga (colonna) rimanente.

Ne segue che, sostituendo una linea con la sua somma con un'altra linea, il
determinante non cambia.

\begin{description}
\item[-] $A=\left[ 
\begin{array}{ccc}
a_{11} & a_{12} & a_{13} \\ 
a_{21} & a_{22} & a_{23} \\ 
a_{31} & a_{32} & a_{33}%
\end{array}%
\right] $. E' possibile scrivere $\left[ 
\begin{array}{c}
a_{11} \\ 
a_{21} \\ 
a_{31}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
b_{11} \\ 
b_{21} \\ 
b_{31}%
\end{array}%
\right] +\left[ 
\begin{array}{c}
c_{11} \\ 
c_{21} \\ 
c_{31}%
\end{array}%
\right] $: $A=\left[ 
\begin{array}{ccc}
b_{11}+c_{11} & a_{12} & a_{13} \\ 
b_{21}+c_{21} & a_{22} & a_{23} \\ 
b_{31}+c_{31} & a_{32} & a_{33}%
\end{array}%
\right] $. Calcolo $\det A$ usando la prima colonna: $\det A_{j=1}=\left(
-1\right) ^{1+1}\left( b_{11}+c_{11}\right) \det \hat{A}_{11}+\left(
-1\right) ^{2+1}\left( b_{21}+c_{21}\right) \det \hat{A}_{21}+\left(
-1\right) ^{3+1}\left( b_{31}+c_{31}\right) \det \hat{A}_{31}$. Usando la
propriet\`{a} distributiva, si ha $\det A=$ $\left( -1\right)
^{1+1}b_{11}\det \hat{A}_{11}+\left( -1\right) ^{2+1}b_{21}\det \hat{A}%
_{21}+\left( -1\right) ^{3+1}b_{31}\det \hat{A}_{31}+\left( -1\right)
^{1+1}c_{11}\det \hat{A}_{11}+\left( -1\right) ^{2+1}c_{21}\det \hat{A}%
_{21}+\left( -1\right) ^{3+1}c_{31}\det \hat{A}_{31}$, che \`{e} $\det A_{%
\mathbf{b}}+\det A_{\mathbf{c}}$, ciascuno calcolato usando la prima colonna.
\end{description}

\item Come si comporta il determinante rispetto al MEG?

\begin{enumerate}
\item \textbf{Scambio di due righe}

Data la matrice $A$, ottengo la matrice $B$ da $A$ scambiandone la riga $%
R_{i}$ con la riga $R_{j}$. Si pu\`{o} scrivere $B=S_{ij}A$, dove $S_{ij}$ 
\`{e} una matrice del tipo 1. Per il teorema di Binet, $\det B=\det \left(
S_{ij}A\right) =\det S_{ij}\det A$. Calcolo $\det S_{ij}$ usando una colonna
che contiene $1$ sulla diagonale: $\det S_{ij}=\left( -1\right) ^{1+1}1\cdot
\det \hat{S}_{ij}$. Gli $1$ fuori dalla diagonale sono gli unici elementi
non nulli sulla loro linea. Continuo a ridurre il problema scegliendo righe
con $1$ sulla diagonale ($i+j\equiv 0\func{mod}2$). Poich\'{e} continuo a
moltiplicare per $1$, infine avr\`{o} $\det S_{ij}=\left( -1\right)
^{1+1}1\cdot \det \hat{S}_{ij}=...=\det \left[ 
\begin{array}{cc}
0 & 1 \\ 
1 & 0%
\end{array}%
\right] $, cio\`{e} il determinante \`{e} lo stesso della matrice che si usa
per scambiare le due righe di una matrice $2\times 2$. Poich\'{e} $\det %
\left[ 
\begin{array}{cc}
0 & 1 \\ 
1 & 0%
\end{array}%
\right] =-1=\det S_{ij}$, $\det B=-\det A$ (propriet\`{a} di alternanza del
determinante). Lo stesso vale se si scambiano due colonne invece di due
righe.

Di conseguenza, se una matrice ha due linee uguali, scambiandole si ottiene $%
B=A$: $\det A=-\det A\Longleftrightarrow \det A=0_{\mathbf{K}}$. Infatti, se
una matrice ha due righe uguali, sottraendole si ottiene una matrice con una
riga nulla, che se usata per calcolare il determinante produce zero.

\item \textbf{Moltiplicazione di una riga per }$\lambda \neq 0$

Data la matrice $A$, ottengo la matrice $B$ da $A$ moltiplicando la riga $%
R_{i}$ per $\lambda $. Si pu\`{o} scrivere $B=M_{\lambda }A$, dove $%
M_{\lambda }$ \`{e} una matrice del tipo 2. Per il teorema di Binet, $\det
B=\det \left( M_{\lambda }A\right) =\det M_{\lambda }\det A=\lambda \det A$,
perch\'{e} $M_{\lambda }$ \`{e} una matrice triangolare il cui determinante 
\`{e} il prodotto degli elementi sulla diagonale.

\item \textbf{Sostituzione di un riga con la sua somma con il multiplo di
un'altra}

Data la matrice $A$, ottengo la matrice $B$ da $A$ sostituendo alla riga $%
R_{i}$ $R_{i}+\lambda R_{j}$. Si pu\`{o} scrivere $B=E_{i,\lambda ,j}A$,
dove $E_{i,\lambda ,j}$ \`{e} una matrice del tipo 3. Per il teorema di
Binet, $\det B=\det \left( E_{i,\lambda ,j}A\right) =\det E_{i,\lambda
,j}\det A=\det A$, perch\'{e} $E_{i,\lambda ,j}$ \`{e} una matrice
triangolare inferiore ($i>j$) il cui determinante \`{e} il prodotto degli
elementi sulla diagonale.
\end{enumerate}
\end{enumerate}

\textbf{Def} Data $A\in M_{\mathbf{K}}\left( n,n\right) $, si dice matrice
dei complementi algebrici o matrice dei cofattori di $A$, e si indica con cof%
$A$, la matrice in $M_{\mathbf{K}}\left( n,n\right) $ avente come elemento $%
ij$ $\left( -1\right) ^{i+j}\det \hat{A}_{ij}$, dove $\left( -1\right)
^{i+j}\det \hat{A}_{ij}$ \`{e} detto cofattore di $A$ relativo alla
posizione $i,j$.

Per una matrice $A\in M_{\mathbf{K}}\left( n,n\right) $ vale $A^{-1}=\frac{1%
}{\det A}\left( \text{cof}A\right) ^{T}$.

\subsection{Invertibilit\`{a} di una matrice}

\begin{gather*}
\text{Hp}\text{: }A\in M_{\mathbf{K}}\left( n,n\right) \\
\text{Ts}\text{: le seguenti affermazioni sono equivalenti:} \\
\text{(i) }A\text{ \`{e} invertibile} \\
\text{(ii) }r\left( A\right) =n \\
\text{(iii) }\det A\neq 0
\end{gather*}

Questo significa (i)$\Longleftrightarrow $(ii)$\Longleftrightarrow $(iii).
Per mostrare ci\`{o} \`{e} sufficiente dimostrare (i)$\Longrightarrow $(iii)$%
\Longrightarrow $(ii)$\Longrightarrow $(i).

\textbf{Dim} (1) Considero $A$ invertibile e mostro che $\det A\neq 0$.

Per ipotesi (scelgo l'inversa destra, ma sarebbe analogo con l'inversa
sinistra) $\exists $ $A^{-1}:AA^{-1}=Id_{n}$. Per il teorema di Binet $\det
\left( AA^{-1}\right) =\det Id_{n}\Longleftrightarrow \det A\det A^{-1}=1$.
Questo significa che $\det A$ \`{e} un elemento invertibile nel campo $%
\mathbf{K}$: $\det A=\frac{1}{\det A^{-1}}$. Poich\'{e} su un campo gli
elementi invertibili sono tutti e soli gli elementi non nulli, $\det A\neq 0$%
.

(2) Considero $\det A\neq 0$ e mostro che $r\left( A\right) =n$.

Si \`{e} gi\`{a} verificato nel caso di $A$ a scala. Nel caso di $A$
generica, riduco $A$ a scala con il MEG, usando tutti e tre i tipi di
operazioni: se effettuo $s$ scambi e $k$ moltiplicazioni per scalare, ho $%
\det U=\left( -1\right) ^{s}\lambda _{1}\cdot ...\cdot \lambda _{k}\det
A\Longleftrightarrow \det A=\left( -1\right) ^{s}\frac{1}{\lambda _{1}}\cdot
...\cdot \frac{1}{\lambda _{k}}\det U$. Poich\'{e} $\det A\neq
0\Longleftrightarrow \det U\neq 0$ e $r\left( A\right) =r\left( U\right) $
per definizione di rango, dall'ipotesi $\det A\neq 0$ segue $\det U\neq 0$,
il che implica $r\left( U\right) =n$ e anche $r\left( A\right) =n$.

(3) Considero $r\left( A\right) =n$ e mostro che $A$ \`{e} invertibile.

Mostro che esiste l'inversa destra e che, se esiste l'inversa destra, allora
esiste anche l'inversa sinistra. Inizio cercando l'inversa destra, cio\`{e} $%
X:AX=Id_{n}$: questa equazione \`{e} equivalente agli $n$ sistemi, ciascuno
dei quali con $n$ equazioni in $n$ incognite, $A\left[ 
\begin{array}{c}
x_{11} \\ 
x_{21} \\ 
... \\ 
x_{n1}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
1 \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right] $, $A\left[ 
\begin{array}{c}
x_{12} \\ 
x_{22} \\ 
... \\ 
x_{n2}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
1 \\ 
... \\ 
0%
\end{array}%
\right] $,..., $A\left[ 
\begin{array}{c}
x_{1n} \\ 
x_{2n} \\ 
... \\ 
x_{nn}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
0 \\ 
... \\ 
1%
\end{array}%
\right] $. Per il teorema di Cramer applicato a ciascuno dei sistemi
(essendo $r\left( A\right) =n$), per ciascuno di essi esiste un unico
vettore soluzione: aggregando i vettori soluzione si ottiene l'unica matrice
inversa destra $B$. Adesso dovrei cercare $Y:YA=Id_{n}$: cerco invece $%
Y:BY=Id_{n}$ e mi chiedo qual \`{e} $r\left( B\right) $, in modo da poter
applicare il teorema di Cramer. Osservo
che $r\left( B\right) =n\Longleftrightarrow B\mathbf{x=0}$ ha un'unica
soluzione (si ricava dai teoremi di Cramer e Rouch\'{e}-Capelli); mostro
allora che $B\mathbf{x=0}$ ha un'unica soluzione. $B\mathbf{x=0}%
\Longrightarrow A\left( B\mathbf{x}\right) =A\mathbf{0}\Longleftrightarrow
\left( AB\right) \mathbf{x}=\mathbf{0}\Longleftrightarrow Id_{n}\mathbf{%
x=0\Longleftrightarrow x=0}$: dunque $r\left( B\right) =n$ e per il teorema
di Cramer esiste un'unica $Y:BY=Id_{n}$; chiamo $Y=C$. Quindi $B$ risulta
invertibile: ha come inversa sinistra $A$ e come inversa destra $C$: per
unicit\`{a} dell'inversa si ha $A=C$ e, essendo $AB=Id_{n}$ e $BC=BA=Id_{n}$%
, $B$ \`{e} anche inversa sinistra di $A$ e $A$ ha come unica inversa $B$. $%
\blacksquare $

Se invece si considerano $A,B\in M_{\mathbf{K}}\left( n,n\right) $, le
ipotesi affinch\'{e} $AB$ sia invertibile si ricavano dal teorema: $AB$ \`{e}
invertibile $\Longleftrightarrow $ $\det \left( AB\right) =\det A\det B\neq
0 $, che \`{e} equivalente a $\det A,\det B\neq 0$ (perch\'{e} su $\mathbf{K}
$ vale la legge di annullamento del prodotto): $\det A,\det B\neq 0$ \`{e}
equivalente a $A$ e $B$ invertibili. Quindi $AB$ \`{e} invertibile se e solo
se $A$ e $B$ sono invertibili.

Se $AB$ \`{e} invertibile, qual \`{e} l'inversa? Cerco $X:\left( AB\right)
X=X\left( AB\right) =Id_{n}$. Noto che $X=B^{-1}A^{-1}$ \`{e} tale che $%
\left( AB\right) \left( B^{-1}A^{-1}\right) =A\left( BB^{-1}\right)
A^{-1}=AId_{n}A^{-1}=AA^{-1}=Id_{n}$, e analogamente se si calcola $X\left(
AB\right) $. Dunque l'inversa del prodotto \`{e} il prodotto, in ordine
scambiato, delle inverse.

\begin{enumerate}
\item Calcolo l'inversa di $B=\left[ 
\begin{array}{ccc}
9 & 2 & -2 \\ 
2 & 2 & 0 \\ 
-2 & 0 & 2%
\end{array}%
\right] $. $\det B=20$, quindi \`{e} invertibile. Calcolo l'inversa
risolvendo i tre sistemi in parallelo: $\left[ 
\begin{array}{cccccc}
9 & 2 & -2 & 1 & 0 & 0 \\ 
2 & 2 & 0 & 0 & 1 & 0 \\ 
-2 & 0 & 2 & 0 & 0 & 1%
\end{array}%
\right] $. Scambio le prime due righe e normalizzo la nuova prima riga: $%
\left[ 
\begin{array}{cccccc}
1 & 1 & 0 & 0 & \frac{1}{2} & 0 \\ 
9 & 2 & -2 & 1 & 0 & 0 \\ 
-2 & 0 & 2 & 0 & 0 & 1%
\end{array}%
\right] $. Faccio $\left( 2\right) =\left( 2\right) -9\left( 1\right) $ e $%
\left( 3\right) =\left( 3\right) +2\left( 1\right) $: $\left[ 
\begin{array}{cccccc}
1 & 1 & 0 & 0 & \frac{1}{2} & 0 \\ 
0 & -7 & -2 & 1 & -\frac{9}{2} & 0 \\ 
0 & 2 & 2 & 0 & 1 & 1%
\end{array}%
\right] $; normalizzo la terza riga e scambio la seconda e terza riga: $%
\left[ 
\begin{array}{cccccc}
1 & 1 & 0 & 0 & \frac{1}{2} & 0 \\ 
0 & 1 & 1 & 0 & \frac{1}{2} & \frac{1}{2} \\ 
0 & -7 & -2 & 1 & -\frac{9}{2} & 0%
\end{array}%
\right] $; faccio $\left( 3\right) =\left( 3\right) +7\left( 2\right) $ e
ottengo $\left[ 
\begin{array}{cccccc}
1 & 1 & 0 & 0 & \frac{1}{2} & 0 \\ 
0 & 1 & 1 & 0 & \frac{1}{2} & \frac{1}{2} \\ 
0 & 0 & 5 & 1 & -1 & \frac{7}{2}%
\end{array}%
\right] $; $\left( 3\right) =\frac{1}{5}\left( 3\right) $: $\left[ 
\begin{array}{cccccc}
1 & 1 & 0 & 0 & \frac{1}{2} & 0 \\ 
0 & 1 & 1 & 0 & \frac{1}{2} & \frac{1}{2} \\ 
0 & 0 & 1 & \frac{1}{5} & -\frac{1}{5} & \frac{7}{10}%
\end{array}%
\right] $; $\left( 2\right) =\left( 2\right) -\left( 3\right) $: $\left[ 
\begin{array}{cccccc}
1 & 1 & 0 & 0 & \frac{1}{2} & 0 \\ 
0 & 1 & 0 & -\frac{1}{5} & \frac{7}{10} & -\frac{1}{5} \\ 
0 & 0 & 1 & \frac{1}{5} & -\frac{1}{5} & \frac{7}{10}%
\end{array}%
\right] $; $\left( 1\right) =\left( 1\right) -\left( 2\right) $: $\left[ 
\begin{array}{cccccc}
1 & 0 & 0 & \frac{1}{5} & -\frac{1}{5} & \frac{1}{5} \\ 
0 & 1 & 0 & -\frac{1}{5} & \frac{7}{10} & -\frac{1}{5} \\ 
0 & 0 & 1 & \frac{1}{5} & -\frac{1}{5} & \frac{7}{10}%
\end{array}%
\right] $. La matrice inversa \`{e} la matrice $3\times 3$ a destra.
\end{enumerate}

\section{Geometria delle rette e dei piani nel piano e nello spazio euclideo}

\textbf{Def} Un vettore applicato \`{e} un segmento con un verso assegnato.

Un vettore applicato $A\vec{B}$ \`{e} caratterizzato da un punto di
applicazione ($A$), una direzione, un verso e un modulo.

Direzione e verso individuano una semiretta che parte dal punto di
applicazione; il modulo \`{e} la distanza del secondo estremo dal punto di
applicazione.

\textbf{Def} Un vettore libero \`{e} una grandezza caratterizzata da
direzione, verso e modulo.

[Una relazione di equivalenza \`{e} una relazione tra due elementi di un
insieme che ha le propriet\`{a}:

\begin{enumerate}
\item riflessiva: ogni elemento $A\vec{B}$ \`{e} equivalente a se stesso

\item simmetrica: se $A\vec{B}$ \`{e} equivalente a $C\vec{D}$, allora $C%
\vec{D}$ \`{e} equivalente ad $A\vec{B}$

\item transitiva: se $A\vec{B}$ \`{e} equivalente a $C\vec{D}$, e $C\vec{D}$ 
\`{e} equivalente a $E\vec{F}$, allora $A\vec{B}$ \`{e} equivalente a $E\vec{%
F}$]
\end{enumerate}

Si chiama $V_{A}$ l'insieme dei vettori applicati. Dati due elementi di $%
V_{A}$ $A\vec{B}$ e $C\vec{D}$, si dice che $A\vec{B}$ \`{e} equivalente a $C%
\vec{D}$ $\Longleftrightarrow $ $A\vec{B}$ e $C\vec{D}$ hanno stessa
direzione, verso e modulo. Una direzione nello spazio \`{e} una classe di
equivalenza di rette parallele: due rette hanno la stessa direzione se sono
parallele (l'insieme di tutte le rette nello spazio pu\`{o} essere ripartito
in classi di equivalenza di rette parallele).

Dato un vettore $A\vec{B}$, si definisce $\left[ A\vec{B}\right] $ l'insieme
dei vettori equivalenti ad $A\vec{B}$, $\left[ A\vec{B}\right] $ si dice
classe di equivalenza di $A\vec{B}$ e $A\vec{B}$ si dice rappresentante
della classe.

Se $C\vec{D}\in \left[ A\vec{B}\right] $, allora $A\vec{B}\in \left[ C\vec{D}%
\right] $, cio\`{e} $\left[ A\vec{B}\right] =\left[ C\vec{D}\right] $. $%
V_{A} $ si pu\`{o} dunque dividere in infinite classi di equivalenza, che
non hanno intersezione tra loro (altrimenti si violerebbe la propriet\`{a}
transitiva). Quindi si pu\`{o} meglio dare la definizione di vettore libero:

\textbf{Def} Un vettore libero \`{e} una classe di equivalenza (di vettori
applicati, cio\`{e} di $V_{A}$).

Se si fissa un sistema di riferimento con origine $O$, si possono
identificare:

\begin{description}
\item[-] i punti dello spazio, del tipo $\left( x,y,z\right) $

\item[-] i vettori applicati $O\vec{P}$ del tipo $x\mathbf{u}+y\mathbf{v}+z%
\mathbf{w}$

\item[-] i vettori liberi $\left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] $
\end{description}

\subsection{Operazioni tra vettori liberi}

\subsubsection{Somma tra vettori e propriet\`{a}}

Se $\mathbf{v}=O\vec{P}$ e $\mathbf{w}=O\vec{Q}$, il vettore somma \`{e} la
diagonale del parallelogramma i cui due lati non paralleli sono i vettori da
sommare: \`{e} il vettore $O\vec{R}$, dove $R$ \`{e} il quarto vertice del
parallelogramma.

$\forall $ terna di vettori liberi $\mathbf{v,u,w}$ valgono le propriet\`{a}:

\begin{description}
\item[-] Associativa: $\left( \mathbf{v+w}\right) +\mathbf{u=v}+\left( 
\mathbf{w+u}\right) $.

\item[-] Commutativa: $\mathbf{v+w=w+v}$.

\item[-] Esistenza dell'elemento neutro: $O\vec{O}=\vec{O}$ (vettore che
parte nell'origine e arriva nell'origine).

\item[-] Esistenza dell'elemento opposto: $\forall $ $\mathbf{v}$ $\exists $ 
$-\mathbf{v}:\mathbf{v-v}=\vec{O}$. $-\mathbf{v}$ ha uguale direzione e
modulo e verso opposto rispetto a $\mathbf{v}$.
\end{description}

\subsubsection{Prodotto per scalari e propriet\`{a}}

Dato $t\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, $\mathbf{v}$ vettore libero, si ha $t\mathbf{v}=0$ se $t=0$; altrimenti $t%
\mathbf{v}$ ha modulo pari al modulo di $\mathbf{v}$ per $\left\vert
t\right\vert $, stessa direzione di $\mathbf{v}$, stesso verso di $\mathbf{v}
$ se $t>0$, verso opposto se $t<0$. Ha le seguenti propriet\`{a}:

\begin{description}
\item[-] Distributiva a sinistra: $\left( t+s\right) \mathbf{v}=t\mathbf{v}+s%
\mathbf{v}$.

\item[-] Distributiva a destra: $t\left( \mathbf{v+w}\right) =t\mathbf{v}+t%
\mathbf{w}$.

\item[-] Associativa: $\left( ts\right) \mathbf{v}=t\left( s\mathbf{v}%
\right) $.

\item[-] Esistenza dell'elemento neutro: $1_{\mathbf{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}}\mathbf{v=v}$.
\end{description}

Sono operazioni perfettamente analoghe a quelle viste per matrici.

\subsubsection{Sistemi di riferimento}

\textbf{Piano }$H$

Un sistema di riferimento di $H$ \`{e} dato dalla scelta di un punto del
piano, che funger\`{a} da origine del sistema, e di due vettori liberi non
paralleli (non con la stessa direzione: devono essere linearmente
indipendenti per poter costituire una base) $\mathbf{u}$ e $\mathbf{v}$, che
fungeranno da assi del sistema. Una tale scelta divide il piano in quattro
parti. Per ogni punto $P$ del piano, \`{e} possibile trovare un
parallelogramma con lati paralleli a $\mathbf{u}$ e $\mathbf{v}$ e un
vertice coincidente con $P$: allora $O\vec{P}=\alpha \mathbf{u}+\beta 
\mathbf{v}$, $\alpha ,\beta \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$. A seconda della regione di piano in cui si trova $P$, $\alpha $ o $\beta $
o entrambi dovranno essere negativi.

Dunque i punti del piano $H$ sono descritti da tutte le possibili scelte di $%
\alpha ,\beta \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, attraverso l'identificazione $O\vec{P}=\alpha \mathbf{u}+\beta \mathbf{v}$%
.

$\left\{ \mathbf{u,v}\right\} $ si dice base del sistema di riferimento.

\textbf{Spazio }$S$

Un sistema di riferimento di $S$ \`{e} dato dalla scelta di un punto dello
spazio, che funger\`{a} da origine del sistema, e di tre vettori liberi $%
\mathbf{u},\mathbf{v,w}$, tali che $\mathbf{u,v}$ non sono paralleli e $%
\mathbf{w}$ non appartiene al piano individuato da $\mathbf{u,v}$ (devono
essere linearmente indipendenti); $\mathbf{u,v,w}$ fungeranno da assi del
sistema. Una tale scelta divide lo spazio in otto parti. Per ogni punto $P$
dello spazio, \`{e} possibile trovare un parallelepipedo con lati paralleli
a $\mathbf{u,v},\mathbf{w}$ e un vertice coincidente con $P$: allora $O\vec{P%
}=\alpha \mathbf{u}+\beta \mathbf{v+}\gamma \mathbf{w}$, $\alpha ,\beta
,\gamma \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$. A seconda della regione di piano in cui si trova $P$, alcuni dei
coefficienti possono essere negativi.

Dunque i punti dello spazio $S$ sono descritti da tutte le possibili scelte
di $\alpha ,\beta ,\gamma \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, attraverso l'identificazione $O\vec{P}=\alpha \mathbf{u}+\beta \mathbf{v+}%
\gamma \mathbf{w}$. $\left\{ \mathbf{u,v,w}\right\} $ si dice base del
sistema di riferimento.

\subsubsection{Prodotto scalare e propriet\`{a}}

Per ogni coppia di vettori liberi $\mathbf{v,w}$, si definisce prodotto
scalare (o prodotto interno) di $\mathbf{v}$ e $\mathbf{w}$ il numero reale%
\begin{equation*}
\mathbf{v\cdot w}:=\left\vert \left\vert \mathbf{v}\right\vert \right\vert
\left\vert \left\vert \mathbf{w}\right\vert \right\vert \cos \theta
\end{equation*}

dove $\theta $ \`{e} l'angolo compreso tra i due vettori e $\left\vert
\left\vert \mathbf{v}\right\vert \right\vert $ \`{e} il modulo di $\mathbf{v}
$. Ha le seguenti propriet\`{a}:

\begin{description}
\item[-] Bilinearit\`{a}: $\left( \mathbf{u}+\mathbf{v}\right) \cdot \mathbf{%
w=u\cdot w+v\cdot w}$; $\mathbf{u}\cdot \left( \mathbf{v}+\mathbf{w}\right) =%
\mathbf{u\cdot v+u\cdot w}$; $\left( t\mathbf{u}\right) \cdot \mathbf{v}%
=t\left( \mathbf{u\cdot v}\right) =\mathbf{u}\cdot \left( t\mathbf{v}\right) 
$

\item[-] Commutativit\`{a}: $\mathbf{u\cdot v=v\cdot u}$

\item[-] Positivit\`{a}: $\mathbf{v\cdot v}\geq 0$; $\mathbf{v\cdot v}%
=0\Longleftrightarrow \mathbf{v=0}$. Infatti $\mathbf{v\cdot v}=\left\vert
\left\vert \mathbf{v}\right\vert \right\vert \left\vert \left\vert \mathbf{v}%
\right\vert \right\vert \cos 0=\left\vert \left\vert \mathbf{v}\right\vert
\right\vert ^{2}\geq 0$.
\end{description}

Il prodotto scalare d\`{a} un criterio di perpendicolarit\`{a}: due vettori
sono perpendicolari se e solo se il loro prodotto scalare \`{e} nullo (il
vettore nullo \`{e} considerato perpendicolare a tutti i vettori).

Usando la perpendicolarit\`{a} si possono introdurre le coordinate
cartesiane come sistema di riferimento del piano e dello spazio.

Nel piano la terna che descrive il sistema di riferimento \`{e} $\left\{ O,%
\mathbf{i,j}\right\} $, dove $\mathbf{i}$ \`{e} un vettore di modulo $1$ e $%
\mathbf{j}$ \`{e} il vettore che si ottiene ruotando $\mathbf{i}$ di $\frac{%
\pi }{2}$ in senso antiorario.

Nello spazio il sistema di riferimento \`{e} $\left\{ O,\mathbf{i,j,k}%
\right\} $, dove $\mathbf{i},\mathbf{j}$ hanno le stesse caratteristiche di
prima e $\mathbf{k}$ ha modulo $1$, \`{e} perpendicolare a $\mathbf{i,j}$ -
quindi al piano che li contiene - e verso dato dalla regola della mano
destra ($\mathbf{i,j,k}$ si dice infatti terna destrorsa). Questo sistema di
riferimento semplifica i calcoli e permette di ottenere una formula
analitica per il prodotto scalare, sfruttando l'ortogonalit\`{a} dei tre
assi: dati $\mathbf{v}=x_{1}\mathbf{i}+y_{1}\mathbf{j}+z_{1}\mathbf{k}$, $%
\mathbf{w}=x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}\mathbf{k}$: 
\begin{eqnarray*}
\mathbf{v\cdot w} &=&\left( x_{1}\mathbf{i}+y_{1}\mathbf{j}+z_{1}\mathbf{k}%
\right) \cdot \left( x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}\mathbf{k}\right)
=x_{1}\mathbf{i}\cdot \left( x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}\mathbf{k}%
\right) + \\
&&+y_{1}\mathbf{j}\cdot \left( x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}\mathbf{k%
}\right) +z_{1}\mathbf{k}\cdot \left( x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}%
\mathbf{k}\right) \\
&=&x_{1}x_{2}\mathbf{i}\cdot \mathbf{i}+x_{1}y_{2}\mathbf{i\cdot j}%
+x_{1}z_{2}\mathbf{i}\cdot \mathbf{k+}x_{2}y_{1}\mathbf{i}\cdot \mathbf{j}%
+y_{1}y_{2}\mathbf{j\cdot j}+y_{1}z_{2}\mathbf{j}\cdot \mathbf{k+}z_{1}x_{2}%
\mathbf{k}\cdot \mathbf{i}+z_{1}y_{2}\mathbf{k\cdot j}+z_{1}z_{2}\mathbf{k}%
\cdot \mathbf{k=} \\
&=&x_{1}x_{2}+0+0+0+y_{1}y_{2}+0+0+0+z_{1}z_{2}=x_{1}x_{2}+y_{1}y_{2}+z_{1}z_{2}
\end{eqnarray*}

Questa formula \`{e} conseguenza della definizione geometrica di prodotto
scalare e della scelta del sistema di riferimento. Si pu\`{o} interpretare
il prodotto scalare anche come prodotto ta matrici:%
\begin{equation*}
\mathbf{v\cdot w}=\left[ 
\begin{array}{ccc}
x_{1} & y_{1} & z_{1}%
\end{array}%
\right] \left[ 
\begin{array}{c}
x_{2} \\ 
y_{2} \\ 
z_{1}%
\end{array}%
\right]
\end{equation*}

\subsubsection{Prodotto vettoriale e propriet\`{a}}

Per ogni coppia di vettori liberi $\mathbf{v,w}$, si dice prodotto
vettoriale (o prodotto esterno) di $\mathbf{v}$ e $\mathbf{w}$ il vettore
libero $\mathbf{v\times w}$ (indicato equivalentemente con $\mathbf{v\wedge w%
}$), definito come segue.

Se $\mathbf{v=0}$ o $\mathbf{w=0}$ o $\mathbf{v,w}$ sono paralleli, $\mathbf{%
v\times w=0}$. Se $\mathbf{v,w\neq 0}$ e $\mathbf{v,w}$ non sono paralleli,
il modulo di $\mathbf{v\times w}$ \`{e} l'area del parallelogramma di lati $%
\mathbf{v,w}$, la direzione \`{e} perpendicolare al piano contenente $%
\mathbf{v,w}$, il verso \`{e} tale che $\left\{ \mathbf{v,w,v\times w}%
\right\} $ sia una terna destrorsa. Ha le seguenti propriet\`{a}:

\begin{description}
\item[-] Bilinearit\`{a}: $\left( \mathbf{u}+\mathbf{v}\right) \times 
\mathbf{w=u\times w+v\times w}$; $\mathbf{u}\times \left( \mathbf{v}+\mathbf{%
w}\right) =\mathbf{u\times v+u\times w}$; $\left( t\mathbf{u}\right) \times 
\mathbf{v}=t\left( \mathbf{u\times v}\right) $

\item[-] Anticommutativit\`{a}: $\mathbf{u\times v=-v\times u}$
\end{description}

Dalla definizione segue $\mathbf{i\times i=j\times j=k\times k=0}$, perch%
\'{e} sono coppie di vettori paralleli; $\mathbf{i\times j=k}$ ($\mathbf{i}$
e $\mathbf{j}$ sono lati di un quadrato), $\mathbf{j\times k=i}=-\mathbf{%
k\times j}$, $\mathbf{k\times i=j=-i\times k}$.

Dalle propriet\`{a} segue che $\mathbf{r\times }\left( \lambda \mathbf{r}%
\right) =\mathbf{0}$ e viceversa se due vettori hanno prodotto vettoriale
nullo sono paralleli, quindi il prodotto vettoriale d\`{a} un criterio di
parallelismo: due vettori sono paralleli se e solo se il loro prodotto
vettoriale \`{e} nullo (il vettore nullo \`{e} considerato parallelo a tutti
i vettori).

Di nuovo, le coordinate cartesiane permettono di ottenere una formula
analitica per il prodotto vettoriale, sfruttando l'ortogonalit\`{a} dei tre
assi: dati $\mathbf{v}=x_{1}\mathbf{i}+y_{1}\mathbf{j}+z_{1}\mathbf{k}$, $%
\mathbf{w}=x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}\mathbf{k}$: 
\begin{gather*}
\mathbf{v\times w}=\left( x_{1}\mathbf{i}+y_{1}\mathbf{j}+z_{1}\mathbf{k}%
\right) \times \left( x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}\mathbf{k}\right)
=x_{1}\mathbf{i}\times \left( x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}\mathbf{k}%
\right) + \\
+y_{1}\mathbf{j}\times \left( x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}\mathbf{k}%
\right) +z_{1}\mathbf{k}\times \left( x_{2}\mathbf{i}+y_{2}\mathbf{j}+z_{2}%
\mathbf{k}\right)  \\
=x_{1}x_{2}\mathbf{i}\times \mathbf{i}+x_{1}y_{2}\mathbf{i\times j}%
+x_{1}z_{2}\mathbf{i\times k+}y_{1}x_{2}\mathbf{j\times i}+y_{1}y_{2}\mathbf{%
j\times j}+y_{1}z_{2}\mathbf{j\times k+}z_{1}x_{2}\mathbf{k\times i}%
+z_{1}y_{2}\mathbf{k\times j}+z_{1}z_{2}\mathbf{k\times k=} \\
=\left( x_{1}y_{2}-x_{2}y_{1}\right) \mathbf{k}-\left(
x_{1}z_{2}-x_{2}z_{1}\right) \mathbf{j}+\left( y_{1}z_{2}-z_{1}y_{2}\right) 
\mathbf{i=}\det \left[ 
\begin{array}{cc}
x_{1} & x_{2} \\ 
y_{1} & y_{2}%
\end{array}%
\right] \mathbf{k-}\det \left[ 
\begin{array}{cc}
x_{1} & x_{2} \\ 
z_{1} & z_{2}%
\end{array}%
\right] \mathbf{j+}\det \left[ 
\begin{array}{cc}
y_{1} & y_{2} \\ 
z_{1} & z_{2}%
\end{array}%
\right] \mathbf{i}
\end{gather*}

che \`{e} uguale, con abuso di notazione, a $\det \left[ 
\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\ 
x_{1} & y_{1} & z_{1} \\ 
x_{2} & y_{2} & z_{2}%
\end{array}%
\right] $ (\`{e} un'uguaglianza solo formale).

Senza abuso di notazione, se $\mathbf{v}=\left( 
\begin{array}{c}
x_{1} \\ 
y_{1} \\ 
z_{1}%
\end{array}%
\right) $ e $\mathbf{w}=\left( 
\begin{array}{c}
x_{2} \\ 
y_{2} \\ 
z_{2}%
\end{array}%
\right) $, $\mathbf{v\times w}=\left( 
\begin{array}{c}
y_{1}z_{2}-z_{1}y_{2} \\ 
x_{2}z_{1}-x_{1}z_{2} \\ 
x_{1}y_{2}-x_{2}y_{1}%
\end{array}%
\right) =\left( 
\begin{array}{c}
\det \left[ R_{2},R_{3}\right] \\ 
\det \left[ R_{3},R_{1}\right] \\ 
\det \left[ R_{1},R_{2}\right]%
\end{array}%
\right) $, dove $\left[ R_{i},R_{j}\right] $ \`{e} la matrice che si ottiene
accostando la riga $i$ e la riga $j$, in quest'ordine. Dati $\mathbf{v,w\neq
0}$, $\left\langle \mathbf{v},\mathbf{v\times w}\right\rangle =0$ e $%
\left\langle \mathbf{w},\mathbf{v\times w}\right\rangle =0$, coerentemente
con la richiesta che $\mathbf{v}\times \mathbf{w}$ sia ortogonale sia a $%
\mathbf{v}$ che a $\mathbf{w}$; quindi $\mathbf{v,v\times w}$ sono
linearmente indipendenti e $\mathbf{w,v\times w}$ sono linearmente
indipendenti.

$\mathbf{v\times w=0}$ se e solo se $\mathbf{v,w}$ sono paralleli: infatti $%
\mathbf{v\times w=0}$ significa che ogni sottomatrice $2\times 2$ di $\left[ 
\begin{array}{cc}
x_{1} & x_{2} \\ 
y_{1} & y_{2} \\ 
z_{1} & z_{2}%
\end{array}%
\right] $ ha determinante nullo, quindi per il teorema di Kronecker il rango
della matrice non pu\`{o} essere $2$.

\subsubsection{Prodotto misto}

Per ogni terna di vettori liberi $\mathbf{u,v,w}$, si definisce prodotto
misto di $\mathbf{u,v}$ e $\mathbf{w}$ il numero reale 
\begin{equation*}
\left\langle \mathbf{u,v\times w}\right\rangle
\end{equation*}

E' noto che $\mathbf{v\times w}=\det \left[ 
\begin{array}{cc}
y_{1} & z_{1} \\ 
y_{2} & z_{2}%
\end{array}%
\right] \mathbf{i-}\det \left[ \ 
\begin{array}{cc}
x_{1} & z_{1} \\ 
x_{2} & z_{2}%
\end{array}%
\right] \mathbf{j+}\det \left[ 
\begin{array}{cc}
x_{1} & y_{1} \\ 
x_{2} & y_{2}%
\end{array}%
\right] \mathbf{k}$. Dato $\mathbf{u}=x_{0}\mathbf{i}+y_{0}\mathbf{j}+z_{0}%
\mathbf{k}$, si ha%
\begin{equation*}
\left\langle \mathbf{u,v\times w}\right\rangle =x_{0}\det \left[ 
\begin{array}{cc}
y_{1} & z_{1} \\ 
y_{2} & z_{2}%
\end{array}%
\right] -y_{0}\det \left[ \ 
\begin{array}{cc}
x_{1} & z_{1} \\ 
x_{2} & z_{2}%
\end{array}%
\right] +z_{0}\det \left[ 
\begin{array}{cc}
x_{1} & y_{1} \\ 
x_{2} & y_{2}%
\end{array}%
\right] =\det \left[ 
\begin{array}{ccc}
x_{0} & y_{0} & z_{0} \\ 
x_{1} & y_{1} & z_{1} \\ 
x_{2} & y_{2} & z_{2}%
\end{array}%
\right]
\end{equation*}

Ne segue che $\left\langle \mathbf{u,v\times w}\right\rangle =\left\langle 
\mathbf{v\times w,u}\right\rangle =\left\langle \mathbf{w}\times \mathbf{u,v}%
\right\rangle $, perch\'{e} scambiando un numero pari di righe il
determinante non cambia.

$\left\langle \mathbf{u,v\times w}\right\rangle =0$ se e solo se $\mathbf{u}$
appartiene al piano che contiene $\mathbf{w}$ e $\mathbf{v}$, cio\`{e} se $%
\mathbf{u,v,w}$ sono complanari (linearmente dipendenti): in tal caso
infatti $\mathbf{v\times w}$ d\`{a} un vettore perpendicolare al piano che
li contiene, e se il prodotto scalare \`{e} nullo significa che $\mathbf{u}$ 
\`{e} ortogonale a tale vettore, cio\`{e} $\mathbf{u}$ appartiene allo
stesso piano di $\mathbf{w}$ e $\mathbf{v}$. Se $\left\langle \mathbf{%
u,v\times w}\right\rangle \neq 0$, $\mathbf{u,v,w}$ non sono complanari e $%
\left\{ \mathbf{u,v,w}\right\} $ pu\`{o} essere la base di un sistema di
riferimento: il prodotto misto \`{e} diverso da zero se e solo se i vettori
sono linearmente indipendenti. Come si vedr\`{a} pi\`{u} avanti, tre vettori
di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$ sono linearmente indipendenti se e solo se il loro prodotto misto \`{e}
non nullo, cio\`{e} il determinante della matrice che essi formano \`{e} non
nullo, cio\`{e} il suo rango \`{e} massimo.

Il prodotto misto \`{e} quindi un criterio per capire se una terna di
vettori pu\`{o} essere scelta come base di un sistema di riferimento nello
spazio.

\subsection{Rette}

Una retta \`{e} univocamente determinata da due suoi punti distinti, oppure
da un suo punto e una direzione.

Nel \textbf{caso 1}, se ho due punti distinti $A$ e $B$, posso ricondurmi al
secondo caso: la direzione si ricava dalla direzione del vettore applicato $A%
\vec{B}$. Altrimenti posso scrivere direttamente le equazioni cartesiane
della retta: cio\`{e} equazioni che, dato un generico punto $P$, permettano
di determinare se $P$ appartiene alla retta o no.

Per appartenere alla retta $P$ dev'essere tale che $A\vec{P}//A\vec{B}$.

Se sono nel piano ($n=2$), $A=\left( x_{A},y_{A}\right) $, $B=\left(
x_{B},y_{B}\right) $, $P=\left( x,y\right) $. Il vettore $A\vec{B}$ \`{e} il
vettore che va da $A$ a $B$, che, se applicato all'origine, \`{e} il vettore
differenza ed \`{e} $A\vec{B}=\left[ 
\begin{array}{c}
x_{B}-x_{A} \\ 
y_{B}-y_{A}%
\end{array}%
\right] $. Analogamente $A\vec{P}=\left[ 
\begin{array}{c}
x-x_{A} \\ 
y-y_{A}%
\end{array}%
\right] $.

I due vettori sono paralleli se e solo se il loro prodotto vettoriale \`{e}
nullo. Con abuso di notazione,%
\begin{equation*}
A\vec{B}\times A\vec{P}=\det \left[ 
\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\ 
x_{B}-x_{A} & y_{B}-y_{A} & 0 \\ 
x-x_{A} & y-y_{A} & 0%
\end{array}%
\right] =\mathbf{k}\left[ \left( x_{B}-x_{A}\right) \left( y-y_{A}\right)
-\left( x-x_{A}\right) \left( y_{B}-y_{A}\right) \right]
\end{equation*}

E' nullo se e solo se $\left( x_{B}-x_{A}\right) \left( y-y_{A}\right)
-\left( x-x_{A}\right) \left( y_{B}-y_{A}\right) =0\Longleftrightarrow \frac{%
y-y_{A}}{y_{B}-y_{A}}=\frac{x-x_{A}}{x_{B}-x_{A}}\Longleftrightarrow y=\frac{%
y_{B}-y_{A}}{x_{B}-x_{A}}\left( x-x_{A}\right) +y_{A}$, se $x_{A}\neq
x_{B},y_{A}\neq y_{B}$.

Se sono nello spazio ($n=3$), $A=\left( x_{A},y_{A},z_{A}\right) $, $%
B=\left( x_{B},y_{B},z_{B}\right) $, $P=\left( x,y,z\right) $. $A\vec{B}=%
\left[ 
\begin{array}{c}
x_{B}-x_{A} \\ 
y_{B}-y_{A} \\ 
z_{B}-z_{A}%
\end{array}%
\right] $, $A\vec{P}=\left[ 
\begin{array}{c}
x-x_{A} \\ 
y-y_{A} \\ 
z-z_{A}%
\end{array}%
\right] $.

I due vettori sono paralleli se e solo se il loro prodotto vettoriale \`{e}
nullo. Con abuso di notazione,%
\begin{equation*}
A\vec{B}\times A\vec{P}=\det \left[ 
\begin{array}{ccc}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\ 
x_{B}-x_{A} & y_{B}-y_{A} & z_{B}-z_{A} \\ 
x-x_{A} & y-y_{A} & z-z_{A}%
\end{array}%
\right]
\end{equation*}

E' nullo se e solo se $\det \left[ 
\begin{array}{cc}
x_{B}-x_{A} & y_{B}-y_{A} \\ 
x-x_{A} & y-y_{A}%
\end{array}%
\right] =\det \left[ 
\begin{array}{cc}
x_{B}-x_{A} & z_{B}-z_{A} \\ 
x-x_{A} & z-z_{A}%
\end{array}%
\right] =\det \left[ 
\begin{array}{cc}
y_{B}-y_{A} & z_{B}-z_{A} \\ 
y-y_{A} & z-z_{A}%
\end{array}%
\right] =0$. Se $x_{A}\neq x_{B},y_{A}\neq y_{B},z_{A}\neq z_{B}$, si
ottiene $\frac{y-y_{A}}{y_{B}-y_{A}}=\frac{x-x_{A}}{x_{B}-x_{A}}$ e $\frac{%
x-x_{A}}{x_{B}-x_{A}}=\frac{z-z_{A}}{z_{B}-z_{A}}$ (la terza \`{e}
ridondante).

In generale, una retta \`{e} definita da $n-1$ equazioni lineari, dove $n$ 
\`{e} la dimensione dello spazio ambiente e $1$ \`{e} il numero di parametri
che servono per descrivere la retta. Quindi nello spazio una retta \`{e}
descritta da un sistema lineare la cui matrice incompleta ha due equazioni,
tre incognite ($x,y,z$) e rango uguale a $2$ (la soluzione del sistema deve
dipendere da un parametro).

Nel \textbf{caso 2}, ho un punto $A$ e una direzione, che \`{e} data da un
vettore libero $\mathbf{v}$. Allora un generico punto $P$ appartiene alla
retta se e solo se $\exists $ $t\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
:A\vec{P}=t\mathbf{v}$.

Se sono nel piano ($n=2$), $A=\left( x_{A},y_{A}\right) $, $P=\left(
x,y\right) $, $A\vec{P}=\left[ 
\begin{array}{c}
x-x_{A} \\ 
y-y_{A}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
x \\ 
y%
\end{array}%
\right] -\left[ 
\begin{array}{c}
x_{A} \\ 
y_{A}%
\end{array}%
\right] $. $A\vec{P}=t\mathbf{v\Longleftrightarrow }\left[ 
\begin{array}{c}
x \\ 
y%
\end{array}%
\right] -\left[ 
\begin{array}{c}
x_{A} \\ 
y_{A}%
\end{array}%
\right] =t\left[ 
\begin{array}{c}
v_{1} \\ 
v_{2}%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{c}
x \\ 
y%
\end{array}%
\right] =\left[ 
\begin{array}{c}
x_{A} \\ 
y_{A}%
\end{array}%
\right] +t\left[ 
\begin{array}{c}
v_{1} \\ 
v_{2}%
\end{array}%
\right] $. Coerentemente con la solita equazione della retta, la direzione
determina il coefficiente angolare e individua una direzione (una classe di
rette parallele), il punto $A$ individua la specifica retta (opportunamente
traslata).

Se sono nello spazio ($n=3$), $A=\left( x_{A},y_{A},z_{A}\right) $, $%
P=\left( x,y,z\right) $, $A\vec{P}=\left[ 
\begin{array}{c}
x-x_{A} \\ 
y-y_{A} \\ 
z-z_{A}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] -\left[ 
\begin{array}{c}
x_{A} \\ 
y_{A} \\ 
z_{A}%
\end{array}%
\right] $. $A\vec{P}=t\mathbf{v\Longleftrightarrow }\left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] -\left[ 
\begin{array}{c}
x_{A} \\ 
y_{A} \\ 
z_{A}%
\end{array}%
\right] =t\left[ 
\begin{array}{c}
v_{1} \\ 
v_{2} \\ 
v_{3}%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] =\left[ 
\begin{array}{c}
x_{A} \\ 
y_{A} \\ 
z_{A}%
\end{array}%
\right] +t\left[ 
\begin{array}{c}
v_{1} \\ 
v_{2} \\ 
v_{3}%
\end{array}%
\right] $. La retta \`{e} descritta con un parametro libero. Si pu\`{o}
passare dalla rappresentazione cartesiana a quella parametrica (con un
parametro) risolvendo il sistema di $n-1$ equazioni e usando il teorema di
Rouch\'{e}-Capelli. Essendo le $n-1$ equazioni indipendenti per ipotesi ed
avendo $3$ incognite, il vettore soluzione si scrive con $n-\left(
n-1\right) +1=2$ vettori, uno dei quali \`{e} il coefficiente di un
parametro libero: $A$ \`{e} il $\mathbf{v}_{0}$, la direzione \`{e} il
vettore $\mathbf{v}_{1}$. Se $A=\left( 0,0,0\right) $, il sistema che si
risolve \`{e} un sistema omogeneo e $\left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] =t\left[ 
\begin{array}{c}
v_{1} \\ 
v_{2} \\ 
v_{3}%
\end{array}%
\right] $ \`{e} l'unica retta di direzione assegnata passante per l'origine.
Per passare dalla rappresentazione parametrica a quella cartesiana, si
esplicita il parametro in un'equazione aggiuntiva e si sostituisce nelle
altre due.

\subsection{Piani}

In equazioni cartesiane ci sono $n-2$ equazioni, in equazioni parametriche
ci sono due parametri liberi: il piano \`{e} il luogo dei punti $\left(
x,y,z\right) $ generato da due direzioni $\mathbf{v,w}$ e un punto $A$ al
variare di $t,s\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$:%
\begin{equation*}
\left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] =\left[ 
\begin{array}{c}
x_{A} \\ 
y_{A} \\ 
z_{A}%
\end{array}%
\right] +t\left[ 
\begin{array}{c}
v_{1} \\ 
v_{2} \\ 
v_{3}%
\end{array}%
\right] +s\left[ 
\begin{array}{c}
w_{1} \\ 
w_{2} \\ 
w_{3}%
\end{array}%
\right]
\end{equation*}

Un piano \`{e} univocamente determinato da tre suoi punti $A,B,C$ non
allineati (un punto $P$ appartiene al piano se e solo se $A\vec{B}$, $A\vec{C%
}$, $A\vec{P}$ sono complanari, cio\`{e} se $A\vec{B}\cdot \left( A\vec{C}%
\times A\vec{P}\right) =0$) o un suo punto e due direzioni non parallele ($%
\mathbf{v,w}$ nella scrittura precedente), o un suo punto $A$ e una
direzione $\mathbf{v}$ ortogonale al piano (un punto $P$ appartiene al piano
se e solo se $A\vec{P}\bot \mathbf{v}$, cio\`{e} $A\vec{P}\cdot \mathbf{v}=0$%
). La prima e la terza condizione si usano per scrivere le equazioni
cartesiane, la seconda per scrivere le equazioni parametriche.

\subsection{Posizione reciproca delle rette in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$}

Date due rette $r,s$, esse si dicono:

\begin{enumerate}
\item se non sono complanari, sghembe

\item se sono complanari:

\begin{enumerate}
\item se non si incontrano, parallele e distinte

\item se si incontrano in un solo punto, incidenti

\item se si incontrano in infiniti punti, parallele e coincidenti
\end{enumerate}
\end{enumerate}

Considero $r,s$ in equazioni cartesiane: a ogni retta - attraverso le sue
equazioni cartesiane - \`{e} associato un sistema lineare, a $r$ $\left[
A_{1}|\mathbf{b}_{1}\right] $ e a $s$ $\left[ A_{2}|\mathbf{b}_{2}\right] $. 
$A_{1},A_{2}\in M_{\mathbf{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}}\left( 2,3\right) $ e $r\left( A_{1}\right) =r\left( A_{2}\right) =2$ (il
sistema lineare deve avere una soluzione dipendente da un parametro). Cerco
eventuali punti d'intersezione, i. e. punti $\left( x,y,z\right) $ che
soddisfano entrambi i sistemi e quindi appartengono a entrambe le rette.
Accosto quindi i due sistemi: devo risolvere $\left[ 
\begin{array}{cc}
A_{1} & \mathbf{b}_{1} \\ 
A_{2} & \mathbf{b}_{2}%
\end{array}%
\right] \in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( 4,4\right) $. Ogni eventuale soluzione del sistema soddisfa entrambi
i sistemi originari, quindi appartiene a entrambe le rette.

Per il teorema di Rouch\'{e}-Capelli, affinch\'{e} la soluzione esista \`{e}
necessario $r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) =r\left( 
\begin{array}{cc}
A_{1} & \mathbf{b}_{1} \\ 
A_{2} & \mathbf{b}_{2}%
\end{array}%
\right) $.

\begin{description}
\item[2b] Affinch\'{e} la soluzione sia unica \`{e} inoltre necessario che
il rango sia pari al numero di incognite, cio\`{e} $r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) =3$. In tal caso le rette sono complanari e incidenti.

\item[2c] Affinch\'{e} le soluzioni siano infinite (cio\`{e} la soluzione
sia l'equazione di una retta) ci dev'essere un parametro libero, cio\`{e} $%
r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) =r\left( 
\begin{array}{cc}
A_{1} & \mathbf{b}_{1} \\ 
A_{2} & \mathbf{b}_{2}%
\end{array}%
\right) =2$. In tal caso le rette sono parallele e coincidenti.
\end{description}

Se $r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) \neq r\left( 
\begin{array}{cc}
A_{1} & \mathbf{b}_{1} \\ 
A_{2} & \mathbf{b}_{2}%
\end{array}%
\right) $, non esiste soluzione. Poich\'{e} la differenza tra i due ranghi pu%
\`{o} solo essere $1$ (pu\`{o} esserci al massimo un pivot nella colonna dei
termini noti) e $r\left( A_{1}\right) =r\left( A_{2}\right) =2$, $2\leq
r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) \leq 3$ e $2\leq r\left( 
\begin{array}{cc}
A_{1} & \mathbf{b}_{1} \\ 
A_{2} & \mathbf{b}_{2}%
\end{array}%
\right) \leq 4$, dunque ci sono solo due possibilit\`{a}: $r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) =2\neq r\left( 
\begin{array}{cc}
A_{1} & \mathbf{b}_{1} \\ 
A_{2} & \mathbf{b}_{2}%
\end{array}%
\right) =3$ e $r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) =3\neq r\left( 
\begin{array}{cc}
A_{1} & \mathbf{b}_{1} \\ 
A_{2} & \mathbf{b}_{2}%
\end{array}%
\right) =4$. Allora considero i sistemi omogenei associati: significa che
ora ciascuno dei sistemi $\left[ A_{1}|\mathbf{0}\right] $ e $\left[ A_{2}|%
\mathbf{0}\right] $ descrive la retta di prima traslata in modo da passare
per l'origine: infatti $\left( 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right) =\left( 
\begin{array}{c}
0 \\ 
0 \\ 
0%
\end{array}%
\right) $ \`{e} sempre soluzione di un sistema omogeneo.

\begin{description}
\item[2a] Se $r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) =2=r\left( 
\begin{array}{cc}
A_{1} & \mathbf{0} \\ 
A_{2} & \mathbf{0}%
\end{array}%
\right) $, ci sono infinite soluzioni dipendenti da un parametro. Questo
significa che le rette di partenza erano parallele distinte: traslate
nell'origine coincidono, e la soluzione del sistema omogeneo \`{e} appunto
una retta.

\item[1] Se $r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) =3=r\left( 
\begin{array}{cc}
A_{1} & \mathbf{0} \\ 
A_{2} & \mathbf{0}%
\end{array}%
\right) $, c'\`{e} un'unica soluzione. Questo significa che le rette di
partenza erano sghembe: traslate nell'origine si intersecano in un solo
punto, e la soluzione del sistema omogeneo \`{e} appunto unica (l'origine).
\end{description}

Non pu\`{o} essere $r\left( 
\begin{array}{c}
A_{1} \\ 
A_{2}%
\end{array}%
\right) \neq r\left( 
\begin{array}{cc}
A_{1} & \mathbf{0} \\ 
A_{2} & \mathbf{0}%
\end{array}%
\right) $, perch\'{e} nella colonna dei termini noti, essendoci solo zeri,
non possono esserci pivot.

\section{Spazi vettoriali}

Finora si sono incontrati ripetutamente insiemi in cui sono definite
operazioni con le medesime propriet\`{a}: matrici, vettori liberi, soluzioni
di un sistema omogeneo... Tutti questi insiemi sono casi particolari di un
oggetto generale.

\textbf{Def} Un insieme $V$ si dice spazio vettoriale su $\mathbf{K}$ se $V$ 
\`{e} dotato (cio\`{e} in $V$ sono definite) due operazioni: un'operazione
interna $+:V\times V\rightarrow V$ e un'operazione esterna $\cdot :\mathbf{K}%
\times V\rightarrow V$ che soddisfano le seguenti propriet\`{a}:

\begin{description}
\item[V1] Propriet\`{a} associativa della somma: $\forall $ $\mathbf{v}_{1}%
\mathbf{,v}_{2}\mathbf{,v}_{3}\in V$ $\left( \mathbf{v}_{1}+\mathbf{v}%
_{2}\right) +\mathbf{v}_{3}=\mathbf{v}_{1}\mathbf{+}\left( \mathbf{v}_{2}+%
\mathbf{v}_{3}\right) $

\item[V2] Propriet\`{a} commutativa della somma: $\forall $ $\mathbf{v}_{1}%
\mathbf{,v}_{2}\in V$ $\mathbf{v}_{1}+\mathbf{v}_{2}=\mathbf{v}_{2}+\mathbf{v%
}_{1}$

\item[V3] Esistenza dell'elemento neutro della somma: $\forall $ $\mathbf{v}%
\in V$ $\exists $ $\mathbf{0}_{V}:\mathbf{v}+\mathbf{0}_{V}=\mathbf{0}_{V}+%
\mathbf{v}=\mathbf{v}$

\item[V4] Esistenza dell'elemento opposto della somma: $\forall $ $\mathbf{v}%
\in V$ $\exists $ $\mathbf{v}^{\prime }\in V:\mathbf{v+v}^{\prime }\mathbf{=v%
}^{\prime }+\mathbf{v}=\mathbf{0}_{V}$

\item[V5] Propriet\`{a} distributiva a destra: $\forall $ $\lambda \in 
\mathbf{K}$, $\forall $ $\mathbf{v}_{1}\mathbf{,v}_{2}\in V$, $\lambda
\left( \mathbf{v}_{1}+\mathbf{v}_{2}\right) =\lambda \mathbf{v}_{1}+\lambda 
\mathbf{v}_{2}$

\item[V6] Propriet\`{a} distributiva a sinistra: $\forall $ $\lambda ,\mu
\in \mathbf{K}$, $\forall $ $\mathbf{v}\in V$, $\left( \lambda +\mu \right) 
\mathbf{v}=\lambda \mathbf{v}+\mu \mathbf{v}$

\item[V7] Propriet\`{a} associativa del prodotto: $\forall $ $\lambda ,\mu
\in \mathbf{K}$, $\forall $ $\mathbf{v}\in V$, $\left( \lambda \mu \right) 
\mathbf{v}=\lambda \left( \mu \mathbf{v}\right) =\left( \lambda \mathbf{v}%
\right) \mu $

\item[V8] Esistenza dell'elemento neutro del prodotto: $\forall $ $\mathbf{v}%
\in V$ $\exists $ $\mathbf{1}_{\mathbf{K}}:\mathbf{1}_{\mathbf{K}}\mathbf{v}=%
\mathbf{v}$.
\end{description}

\begin{enumerate}
\item $\mathbf{K}=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, $V=\left\{ f:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right\} =\tciFourier \left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $ \`{e} uno spazio vettoriale su $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$. Infatti, date $f,g\in V$, si definisce $\left( f+g\right) \left( x\right) 
$ come la funzione $f\left( x\right) +g\left( x\right) $ (\`{e} una somma
puntuale su $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$: si valuta $\forall $ $x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$); data $f\in V$ e $\lambda \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, si definisce $\left( \lambda f\right) \left( x\right) $ come la funzione $%
\lambda f\left( x\right) $ $\forall $ $x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ (prodotto puntuale in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$). Per tali operazioni valgono le propriet\`{a} viste, perch\'{e} le
operazioni sono definite tramite le operazioni di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, che \`{e} uno spazio vettoriale.

\item $C^{0}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $, $C^{1}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $,..., $C^{k}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $ sono spazi vettoriali.

\item Dato un generico campo $\mathbf{K}$, si definisce l'insieme dei polinomi nella variabile $t$ a coefficienti in $\mathbf{K}$ $\mathbf{K}\left[ t%
\right] =\left\{ a_{0}+a_{1}t+...+a_{d}t^{d}\text{, al variare di 
}a_{0},a_{1},...,a_{d}\in \mathbf{K}\right\} $. Si definisce la somma $%
\sum_{i=1}^{d}a_{i}t^{i}+\sum_{i=1}^{e}b_{i}t^{i}:=\sum_{i=1}^{\max \left\{
d,e\right\} }\left( a_{i}+b_{i}\right) t^{i}$; il prodotto per scalari \`{e} 
$\lambda \left( \sum_{i=1}^{d}a_{i}t^{i}\right) :=\sum_{i=1}^{d}\left(
\lambda a_{i}\right) t^{i}$. $\mathbf{K}\left[ t\right] $ \`{e} uno spazio
vettoriale su $\mathbf{K}$.

\item L'insieme delle soluzioni di un sistema omogeneo $S=\left\{ \mathbf{x}%
\in \mathbf{K}^{n}:A\mathbf{x=0}\right\} $ \`{e} uno spazio vettoriale.
Infatti contiene $\mathbf{0}$ e inoltre, dati $\mathbf{x}_{1}\mathbf{,x}%
_{2}\in S$, $A\left( \mathbf{x}_{1}\mathbf{+x}_{2}\right) =A\mathbf{x}_{1}+A%
\mathbf{x}_{2}=\mathbf{0+0}$, quindi anche $\mathbf{x}_{1}\mathbf{+x}_{2}\in
S$. Analogamente, dato $\mathbf{x}\in S$, $t\in \mathbf{K}$, $A\left( t%
\mathbf{x}\right) =t\left( A\mathbf{x}\right) =t\cdot \mathbf{0=0}$, dunque $%
t\mathbf{x}\in S$.
\end{enumerate}

Le propriet\`{a} delle operazioni definite in uno spazio vettoriale hanno
varie conseguenze.

In primo luogo, si possono risolverere equazioni lineari: dati $\mathbf{x,v}%
\in V$, $\lambda ,\mu \in \mathbf{K}$, tali che $\lambda \mathbf{x}+\mu 
\mathbf{v}=\mathbf{0}$, vale%
\begin{gather*}
\lambda \mathbf{x}+\mu \mathbf{v}=\mathbf{0}\Longleftrightarrow \lambda 
\mathbf{x}+\mu \mathbf{v+}\left( -\mu \mathbf{v}\right) =\mathbf{0+}\left(
-\mu \mathbf{v}\right) \text{ (V4)} \\
\lambda \mathbf{x}+\left( \mu \mathbf{v}-\mu \mathbf{v}\right) =\mathbf{0}%
-\mu \mathbf{v}\text{ (V1)} \\
\lambda \mathbf{x+0}=\mathbf{0}-\mu \mathbf{v}\text{ (V4)} \\
\lambda \mathbf{x}\mathbf{=}-\mu \mathbf{v}\text{ (V3)} \\
\text{se }\lambda \neq 0\text{, }\left( \frac{1}{\lambda }\right) \lambda 
\mathbf{x}=\left( \frac{1}{\lambda }\right) \left( -\mu \mathbf{v}\right) 
\text{ perch\'{e} }\mathbf{K}\text{ \`{e} un campo} \\
\left( \frac{1}{\lambda }\lambda \right) \mathbf{x}=\left( \frac{1}{\lambda }%
\left( -\mu \right) \right) \mathbf{v}\text{ (V7)} \\
\mathbf{1x}\mathbf{=}\frac{-\mu }{\lambda }\mathbf{v} \\
\mathbf{x}=-\frac{\mu }{\lambda }\mathbf{v}\text{ (V8)}
\end{gather*}

Inoltre l'elemento opposto di $\mathbf{v}$ \`{e} $\left( -1\right) \mathbf{v}
$ ed \`{e} indicato con $-\mathbf{v}$.

Vale poi la legge di annullamento del prodotto:%
\begin{equation*}
\lambda \mathbf{v}=\mathbf{0}\Longrightarrow \lambda =0\vee \mathbf{v}=%
\mathbf{0}
\end{equation*}

Alcuni spazi vettoriali visti negli esempi precedenti sono sottinsiemi di
altri spazi vettoriali: e. g. $C^{k}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) \subseteq C^{k-1}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) \subseteq ...\subseteq C^{1}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) \subseteq C^{0}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) \subseteq \tciFourier \left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $, e l'insieme dei polinomi a coefficienti in $\mathbf{K}$ di grado
al pi\`{u} $d$ \`{e} $\mathbf{K}\left[ t\right] _{\leq d}\subseteq \mathbf{K}%
\left[ t\right] $; l'insieme delle soluzioni di un sistema omogeneo \`{e} un
sottinsieme di $\mathbf{K}^{n}$.

\textbf{Def} Sia $\left( V,+,\cdot \right) $ uno spazio vettoriale sul campo 
$\mathbf{K}$. Un sottinsieme non vuoto di $V$ $H\subseteq V$ si dice
sottospazio vettoriale se $H$ \`{e} uno spazio vettoriale con le operazioni
ereditate da $V$.

Tutti gli esempi fatti prima sono sottospazi vettoriali. La definizione
precedente \`{e} equivalente a chiedere che $H$ sia chiuso rispetto alle
operazioni di $V$, cio\`{e} che, usando le operazioni di $V$ in $H$, si
ottenga ancora un elemento di $H$.

Come capire se un insieme \`{e} un sottospazio vettoriale o no?

Una condizione necessaria affinch\'{e} $H$ sia un sottospazio vettoriale 
\`{e} $\mathbf{0}\in H$: infatti, se $t=0$, $t\mathbf{v}=\mathbf{0}$, e se $%
\mathbf{0}\notin H$ $H$ non \`{e} chiuso rispetto al prodotto per scalare.
Quindi $\mathbf{0}\notin H$ implica che $H$ non \`{e} un sottospazio
vettoriale.

\begin{enumerate}
\item $S=\left\{ \mathbf{x\in K}^{n}:A\mathbf{x}=\mathbf{b}\text{, con }%
\mathbf{b\neq 0}\right\} $ non \`{e} uno spazio vettoriale perch\'{e}
non contiene $\mathbf{0}_{V}$.

\item Se $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$, $H_{1}=\left\{ x\mathbf{i}+y\mathbf{j}:x\geq 0,y\geq 0\right\} $ non 
\`{e} un sottospazio vettoriale di $V$ perch\'{e} \`{e} chiuso rispetto alla
somma ma non rispetto al prodotto per scalare: $\left( -1\right) \left( 
\begin{array}{c}
1 \\ 
1%
\end{array}%
\right) =\left( 
\begin{array}{c}
-1 \\ 
-1%
\end{array}%
\right) \notin H$; $H_{1}=\left\{ x\vec{i}+y\vec{j}:xy\geq 0\right\} $, cio%
\`{e} primo e terzo quadrante, non \`{e} un sottospazio vettoriale di $V$
perch\'{e} \`{e} chiuso rispetto al prodotto per scalare ma non rispetto
alla somma: $\left( 
\begin{array}{c}
-3 \\ 
0%
\end{array}%
\right) +\left( 
\begin{array}{c}
0 \\ 
3%
\end{array}%
\right) =\left( 
\begin{array}{c}
-3 \\ 
3%
\end{array}%
\right) \notin H$.

Tutti e soli i sottospazi di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$ sono le rette passanti per l'origine e l'origine stessa, che in
equazioni cartesiane sono infatti le soluzioni di un sistema omogeneo.

\item $V=\{A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}(n,n):\det (A)=0\}\subset M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}(n,n)$ non \`{e} un sottospazio vettoriale. Infatti $A=\left[ 
\begin{array}{cc}
1 & 0 \\ 
2 & 0%
\end{array}%
\right] $ e $B=\left[ 
\begin{array}{cc}
0 & 3 \\ 
0 & 4%
\end{array}%
\right] $ hanno determinante nullo ma la loro somma no.

\item Se $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$, $H=\left\{ x,y\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}:xy=0\right\} $ non \`{e} un sottospazio vettoriale di $V$, perch\'{e}
non \`{e} chiuso rispetto alla somma: infatti $H=\left\{ \mathbf{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}:x=0\right\} \cup \left\{ \mathbf{y}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}:y=0\right\} $, e, come si vedr\`{a} pi\`{u} avanti, l'unione di due
sottospazi in generale non \`{e} un sottospazio perch\'{e} non \`{e} chiusa
rispetto alla somma.

\item $%
%TCIMACRO{\U{211a} }%
%BeginExpansion
\mathbb{Q}
%EndExpansion
$ \`{e} uno spazio vettoriale sul campo $%
%TCIMACRO{\U{211a} }%
%BeginExpansion
\mathbb{Q}
%EndExpansion
$, ma non \`{e} un sottospazio vettoriale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, perch\'{e} non \`{e} chiuso rispetto al prodotto per scalare.
\end{enumerate}

In verit\`{a} i sottospazi sono tutti e soli gli insiemi che si possono
scrivere come nucleo di una matrice.

[\textbf{Def} Un insieme $G$ si dice gruppo rispetto all'operazione $\ast $
se $G$ \`{e} dotato di (cio\`{e} in $G$ \`{e} definita) un'operazione
interna $\ast :G\times G\rightarrow G$ che soddisfa le seguenti propriet\`{a}%
:

\begin{description}
\item[G1] Propriet\`{a} associativa: $\forall $ $\mathbf{g}_{1}\mathbf{,g}%
_{2}\mathbf{,g}_{3}\in G$ $\left( \mathbf{g}_{1}\ast \mathbf{g}_{2}\right)
\ast \mathbf{g}_{3}=\mathbf{g}_{1}\mathbf{\ast }\left( \mathbf{g}_{2}\ast 
\mathbf{g}_{3}\right) $

\item[G2] Esistenza dell'elemento neutro: $\forall $ $\mathbf{g}\in G$ $%
\exists $ $e:\mathbf{g}\ast e=e\ast \mathbf{g}=\mathbf{g}$

\item[G3] Esistenza dell'elemento inverso: $\forall $ $\mathbf{g}\in G$ $%
\exists $ $\mathbf{g}^{\prime }\in G:\mathbf{g\ast g}^{\prime }\mathbf{=}e$
\end{description}

Un gruppo che soddisfa anche la propriet\`{a} commutativa ($\forall $ $%
\mathbf{g}_{1}\mathbf{,g}_{2}\in G$ $\mathbf{g}_{1}\ast \mathbf{g}_{2}=%
\mathbf{g}_{2}\ast \mathbf{g}_{1}$) si dice gruppo abeliano o commutativo.
Un gruppo \`{e} quindi una coppia $\left( G,\ast \right) $.

\begin{enumerate}
\item L'insieme delle funzioni biunivoche $f$ nella variabile $x$ \`{e} un
gruppo rispetto al prodotto di composizione, con elemento neutro la funzione
identit\`{a} $x$ ed elemento inverso $f^{-1}$.

\item $%
%TCIMACRO{\U{2124} }%
%BeginExpansion
\mathbb{Z}
%EndExpansion
,%
%TCIMACRO{\U{211a} }%
%BeginExpansion
\mathbb{Q}
%EndExpansion
,%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
,%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
$ sono gruppi abeliani rispetto alla somma.

\item $%
%TCIMACRO{\U{211a} }%
%BeginExpansion
\mathbb{Q}
%EndExpansion
-\left\{ 0\right\} ,%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
-\left\{ 0\right\} ,%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
-\left\{ 0\right\} $ sono gruppi abeliani rispetto al prodotto.

\item Ogni spazio vettoriale \`{e} un gruppo abeliano rispetto alla somma
tra vettori.]
\end{enumerate}

\subsection{Combinazioni lineari, generatori, dipendenza lineare}

\textbf{Def} Sia $\left( V,+,\cdot \right) $ uno spazio vettoriale sul campo 
$\mathbf{K}$, siano $\left\{ \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}%
_{n}\right\} $ vettori di $V$. Si dice combinazione lineare dei vettori $%
\mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}$ un vettore del tipo $%
\lambda _{1}\mathbf{v}_{1}+\lambda _{2}\mathbf{v}_{2}+...+\lambda _{n}%
\mathbf{v}_{n}$, con $\lambda _{1},\lambda _{2},...,\lambda _{n}\in \mathbf{K%
}$. Si dice spazio vettoriale generato (\textit{linear span}) da $\mathbf{v}%
_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}$ l'insieme di tutte le combinazioni
lineari di $\mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}$. Tale insieme 
\`{e} $\left\{ \lambda _{1}\mathbf{v}_{1}+\lambda _{2}\mathbf{v}%
_{2}+...+\lambda _{n}\mathbf{v}_{n}\text{, al variare di }\lambda
_{1},\lambda _{2},...,\lambda _{n}\in \mathbf{K}\right\} $ e si denota
equivalentemente con $Span\left( \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}%
_{n}\right) $ o $\langle \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}%
_{n}\rangle $ o $\tciLaplace \langle \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{%
,...,v}_{n}\rangle $.

Se $\mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}$ sono vettori di $V$, $%
Span\left( \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}\right) $ \`{e}
un sottospazio vettoriale di $V$. Infatti contiene $\mathbf{0}$, per le
propriet\`{a} V1, V2, V6 la somma di due combinazioni lineari di $\mathbf{v}%
_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}$ \`{e} una combinazione lineare di $%
\mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}$ e il prodotto di una
combinazione lineare di $\mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}$
per uno scalare \`{e} una combinazione lineare di $\mathbf{v}_{1}\mathbf{,v}%
_{2}\mathbf{,...,v}_{n}$, per le propriet\`{a} V5 e V7.%
\begin{eqnarray*}
\left( \lambda _{1}\mathbf{v}_{1}+\lambda _{2}\mathbf{v}_{2}+...+\lambda _{n}%
\mathbf{v}_{n}\right) +\left( \mu _{1}\mathbf{v}_{1}+\mu _{2}\mathbf{v}%
_{2}+...+\mu _{n}\mathbf{v}_{n}\right) &=&\left( \lambda _{1}+\mu
_{1}\right) \mathbf{v}_{1}+\left( \lambda _{2}+\mu _{2}\right) \mathbf{v}%
_{2}+...+\left( \lambda _{n}+\mu _{n}\right) \mathbf{v}_{n} \\
\mu \left( \lambda _{1}\mathbf{v}_{1}+\lambda _{2}\mathbf{v}_{2}+...+\lambda
_{n}\mathbf{v}_{n}\right) &=&\mu \lambda _{1}\mathbf{v}_{1}+\mu \lambda _{2}%
\mathbf{v}_{2}+...+\mu \lambda _{n}\mathbf{v}_{n}
\end{eqnarray*}

\begin{enumerate}
\item Un generico polinomio di $\mathbf{K}\left[ t\right] _{\leq d}$ si
scrive $a_{0}+a_{1}t+a_{2}t^{2}+...+a_{d}t^{d}$. Questa scrittura \`{e}
analoga a $\lambda _{1}\mathbf{v}_{1}+\lambda _{2}\mathbf{v}_{2}+...+\lambda
_{n}\mathbf{v}_{n}$. Infatti $Span\left( 1,t,t^{2},...,t^{d}\right) =\mathbf{%
K}\left[ t\right] _{\leq d}$: $1,t,t^{2},...,t^{d}$ si dicono vettori
generatori di $\mathbf{K}\left[ t\right] _{\leq d}$.

\item Se $A$ \`{e} una matrice $m\times n$, $C_{1}\left( A\right)
,...,C_{n}\left( A\right) $ sono le sue colonne e $\mathbf{v\in K}^{n}$, il
prodotto $A\mathbf{v}$ \`{e} una combinazione lineare delle colonne di $A$.
Se $A$ \`{e} una matrice $m\times n$, $R_{1}\left( A\right) ,...,R_{n}\left(
A\right) $ sono le sue righe e $\mathbf{v\in K}^{m}$, il prodotto $\mathbf{v}%
A$ \`{e} una combinazione lineare delle righe di $A$, del tipo $v_{1}\left(
a_{11}|a_{12}|...|a_{1n}\right) +v_{2}\left( a_{21}|a_{22}|...|a_{2n}\right)
+....+v_{m}\left( a_{m1}|a_{m2}|...|a_{mn}\right) $.
\end{enumerate}

\textbf{Def} Un insieme di vettori $\mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{%
,...,v}_{n}$ di uno spazio vettoriale $V$ si dice insieme di generatori di $%
V $ se $Span\left( \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}\right)
=V $.

\begin{enumerate}
\item $Span\left( 1,t,t^{2},...,t^{d}\right) =\mathbf{K}\left[ t\right]
_{\leq d}$: $1,t,t^{2},...,t^{d}$ sono vettori generatori di $\mathbf{K}%
\left[ t\right] _{\leq d}$.

\item $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$ \`{e} lo spazio vettoriale dei vettori liberi nello spazio. Considero $%
\left\{ \mathbf{i,j}\right\} $: $Span\left( \mathbf{i,j}\right) =\left\{ x%
\mathbf{i}+y\mathbf{j}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}:x,y\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right\} $. $Span\left( \mathbf{i,j}\right) \varsubsetneq V$. Infatti $%
\mathbf{k}=\left( 
\begin{array}{c}
0 \\ 
0 \\ 
1%
\end{array}%
\right) \notin Span\left( \mathbf{i,j}\right) $. Quindi $\left\{ \mathbf{i,j}%
\right\} $ non \`{e} un insieme di generatori di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$; lo \`{e} $\left\{ \mathbf{i,j,k}\right\} $. Osservo che i tre vettori
sono "indispensabili": ogni vettore libero $\left( 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right) $ si descrive in modo unico come una combinazione lineare di $%
\mathbf{i,j,k}$: si prendono $\lambda _{1}=x$, $\lambda _{2}=y$, $\lambda
_{3}=z$.

\item $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}=M_{\mathbf{K}}\left( 3,1\right) $. Considero $S=\left\{ \left( 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right) ,\left( 
\begin{array}{c}
-2 \\ 
1 \\ 
0%
\end{array}%
\right) ,\left( 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right) ,\left( 
\begin{array}{c}
1 \\ 
-1 \\ 
2%
\end{array}%
\right) \right\} $. $S$ \`{e} un insieme di generatori di $V$? Questo
equivale a chiedersi se $\forall $ $\mathbf{v}=\left( 
\begin{array}{c}
a \\ 
b \\ 
c%
\end{array}%
\right) \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$ esistono $\lambda _{1},\lambda _{2},\lambda _{3},\lambda _{4}:\lambda
_{1}\left( 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right) +\lambda _{2}\left( 
\begin{array}{c}
-2 \\ 
1 \\ 
0%
\end{array}%
\right) +\lambda _{3}\left( 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right) +\lambda _{4}\left( 
\begin{array}{c}
1 \\ 
-1 \\ 
2%
\end{array}%
\right) =\left( 
\begin{array}{c}
a \\ 
b \\ 
c%
\end{array}%
\right) $ (questa \`{e} la rappresentazione del primo tipo di un sistema
lineare), cio\`{e} se il seguente sistema lineare ha soluzione: $\left[ 
\begin{array}{cccc}
1 & -2 & 0 & 1 \\ 
0 & 1 & 1 & -1 \\ 
0 & 0 & 1 & 2%
\end{array}%
\right] \left[ 
\begin{array}{c}
\lambda _{1} \\ 
\lambda _{2} \\ 
\lambda _{3} \\ 
\lambda _{4}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
a \\ 
b \\ 
c%
\end{array}%
\right] \Longleftrightarrow \left[ 
\begin{array}{ccccc}
1 & -2 & 0 & 1 & a \\ 
0 & 1 & 1 & -1 & b \\ 
0 & 0 & 1 & 2 & c%
\end{array}%
\right] $. $A$ \`{e} gi\`{a} ridotta a scala; per il teorema di Rouch\'{e}%
-Capelli esistono infinite soluzioni dipendenti da $n-r=4-3$ parametri.
Quindi ogni vettore di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$ pu\`{o} essere scritto in infiniti modi come combinazione lineare dei
vettori dati, che costituiscono dunque un insieme di generatori. Tutti i
vettori sono indispensabili?

Considero $\left[ 
\begin{array}{c}
a \\ 
b \\ 
c%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
0 \\ 
0%
\end{array}%
\right] $. Risolvo il sistema lineare aggiungendo il parametro $t$ per la
variabile indipendente e uso il MEGJ. Risolvere il sistema omogeneo
esplicita le relazioni lineari tra le colonne. $\left[ 
\begin{array}{ccccc}
1 & -2 & 0 & 1 & 0 \\ 
0 & 1 & 1 & -1 & 0 \\ 
0 & 0 & 1 & 2 & 0 \\ 
0 & 0 & 0 & 1 & t%
\end{array}%
\right] \rightarrow \left[ 
\begin{array}{ccccc}
1 & -2 & 0 & 0 & -t \\ 
0 & 1 & 1 & 0 & t \\ 
0 & 0 & 1 & 0 & -2t \\ 
0 & 0 & 0 & 1 & t%
\end{array}%
\right] \rightarrow \left[ 
\begin{array}{ccccc}
1 & -2 & 0 & 0 & -t \\ 
0 & 1 & 0 & 0 & 3t \\ 
0 & 0 & 1 & 0 & -2t \\ 
0 & 0 & 0 & 1 & t%
\end{array}%
\right] \rightarrow \left[ 
\begin{array}{ccccc}
1 & 0 & 0 & 0 & 5t \\ 
0 & 1 & 0 & 0 & 3t \\ 
0 & 0 & 1 & 0 & -2t \\ 
0 & 0 & 0 & 1 & t%
\end{array}%
\right] $. $\left[ 
\begin{array}{c}
\lambda _{1} \\ 
\lambda _{2} \\ 
\lambda _{3} \\ 
\lambda _{4}%
\end{array}%
\right] =\left( 
\begin{array}{c}
5 \\ 
3 \\ 
-2 \\ 
1%
\end{array}%
\right) t$. Scelgo $t=1$: $5\left( 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right) +3\left( 
\begin{array}{c}
-2 \\ 
1 \\ 
0%
\end{array}%
\right) -2\left( 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right) +1\left( 
\begin{array}{c}
1 \\ 
-1 \\ 
2%
\end{array}%
\right) =\left( 
\begin{array}{c}
0 \\ 
0 \\ 
0%
\end{array}%
\right) $. Esplicito la combinazione lineare rispetto alla variabile
indipendente (ho scelto $t=1$ in modo da poterlo fare comodamente): $\left( 
\begin{array}{c}
1 \\ 
-1 \\ 
2%
\end{array}%
\right) =-5\left( 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right) -3\left( 
\begin{array}{c}
-2 \\ 
1 \\ 
0%
\end{array}%
\right) +2\left( 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right) $. Quindi il quarto generatore, quello cui corrisponde la colonna
priva di pivot nella matrice ridotta a scala, \`{e} $\left( 
\begin{array}{c}
1 \\ 
-1 \\ 
2%
\end{array}%
\right) \in Span\left( \left( 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right) ,\left( 
\begin{array}{c}
-2 \\ 
1 \\ 
0%
\end{array}%
\right) ,\left( 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right) \right) $, dunque \`{e} ridondante: non aggiunge nuove combinazioni
lineari e gli altri tre vettori sono sufficienti per generare $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$.
\end{enumerate}

\textbf{Def} Un insieme di vettori $\left\{ \mathbf{v}_{1}\mathbf{,...,v}%
_{n}\right\} $ di uno spazio vettoriale $\left( V,+,\cdot \right) $ su $%
\mathbf{K}$ si dice insieme di vettori linearmente indipendenti se $\lambda
_{1}\mathbf{v}_{1}+...+\lambda _{n}\mathbf{v}_{n}=\mathbf{0}_{V}$ implica $%
\lambda _{1}=...=\lambda _{n}=0$. Se al contrario esiste una soluzione non
nulla dell'equazione $\lambda _{1}\mathbf{v}_{1}+...+\lambda _{n}\mathbf{v}%
_{n}=\mathbf{0}_{V}$, i vettori si dicono linearmente dipendenti.

Nel secondo caso c'\`{e} almeno un vettore ridondante, cio\`{e} che non
aggiunge combinazioni lineari.

\begin{enumerate}
\item I vettori $\left( 
\begin{array}{c}
1 \\ 
-1 \\ 
2%
\end{array}%
\right) ,\left( 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right) ,\left( 
\begin{array}{c}
-2 \\ 
1 \\ 
0%
\end{array}%
\right) ,\left( 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right) $ sono linearmente dipendenti: infatti una soluzione non nulla
dell'equazione $\lambda _{1}\left( 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right) +\lambda _{2}\left( 
\begin{array}{c}
-2 \\ 
1 \\ 
0%
\end{array}%
\right) +\lambda _{3}\left( 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right) +\lambda _{4}\left( 
\begin{array}{c}
1 \\ 
-1 \\ 
2%
\end{array}%
\right) =\left( 
\begin{array}{c}
0 \\ 
0 \\ 
0%
\end{array}%
\right) $ \`{e} quella appena vista $\left( 
\begin{array}{c}
5 \\ 
3 \\ 
-2 \\ 
1%
\end{array}%
\right) $.

\item Dato un insieme di $n$ vettori, per verificare se sono linearmente
indipendenti \`{e} sufficiente risolvere il sistema omogeneo associato: se
l'unica soluzione \`{e} il vettore nullo allora sono indipendenti. Questo
accade quando il rango della matrice ottenuta accostando gli $n$ vettori
colonna \`{e} $r\left( A\right) =n$, per il teorema di Rouch\'{e}-Capelli.

\item $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}=Span\left( \mathbf{i,j,k}\right) $. $\left\{ \mathbf{i,j,k}\right\} $
sono linearmente indipendenti. Infatti $\lambda _{1}\left( 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right) +\lambda _{2}\left( 
\begin{array}{c}
0 \\ 
1 \\ 
0%
\end{array}%
\right) +\lambda _{3}\left( 
\begin{array}{c}
0 \\ 
0 \\ 
1%
\end{array}%
\right) =\left( 
\begin{array}{c}
\lambda _{1} \\ 
\lambda _{2} \\ 
\lambda _{3}%
\end{array}%
\right) =\left( 
\begin{array}{c}
0 \\ 
0 \\ 
0%
\end{array}%
\right) $ ha come unica soluzione $\lambda _{1}=\lambda _{2}=\lambda _{3}=0$.
\end{enumerate}

L'indipendenza lineare di un insieme di generatori garantisce che ogni
vettore dello spazio generato possa essere rappresentato in modo unico come
combinazione lineare di tali vettori, come si spiega nel teorema successivo.

\subsection{Basi e dimensione}

\textbf{Def} Sia $\left( V,+,\cdot \right) $ uno spazio vettoriale su $%
\mathbf{K}$, sia $\left\{ \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}%
_{n}\right\} $ un insieme di vettori generatori di $V$. Se $\mathbf{v}_{1}%
\mathbf{,v}_{2}\mathbf{,...,v}_{n}$ sono linearmente indipendenti, l'insieme 
$\left\{ \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}_{n}\right\} $ si dice
base di $V$.

\textbf{Teorema (unicit\`{a} della rappresentazione rispetto a una base)}%
\begin{gather*}
\text{Hp}\text{: }B=\left\{ \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}%
_{n}\right\} \text{ \`{e} una base di }V \\
\text{Ts}\text{: ogni vettore }\mathbf{v}\in V\text{ si scrive in modo unico 
} \\
\text{come combinazione lineare di }\mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{%
,...,v}_{n}
\end{gather*}

La tesi pu\`{o} anche essere espressa dicendo che il sistema lineare $\left[ 
\mathbf{v}_{1}\mathbf{|v}_{2}\mathbf{|...|v}_{n}\right] \left[ 
\begin{array}{c}
\lambda _{1} \\ 
\lambda _{2} \\ 
... \\ 
\lambda _{n}%
\end{array}%
\right] =\mathbf{v}$ ha un'unica soluzione per ogni $\mathbf{v}\in V$. Come
si vedr\`{a} pi\`{u} avanti, questo significa che l'applicazione lineare $%
\tciLaplace _{A}:\mathbf{K}^{n}\rightarrow V$ che usa $A=\left[ \mathbf{v}%
_{1}\mathbf{|v}_{2}\mathbf{|...|v}_{n}\right] $ \`{e} biunivoca, e
garantisce che la mappa delle coordinate sia una funzione.

\textbf{Dim} Per mostrare che esiste un unico modo di scrivere $\mathbf{v}$
come combinazione lineare dei vettori della base, considero due modi e
mostro che coincidono. Poich\'{e} per ipotesi $\mathbf{v}_{1}\mathbf{,v}_{2}%
\mathbf{,...,v}_{n}$ sono generatori di $V$, $\forall $ $\mathbf{v}\in V$ $%
\exists $ $\lambda _{1},\lambda _{2},...,\lambda _{n}:\lambda _{1}\mathbf{v}%
_{1}+\lambda _{2}\mathbf{v}_{2}+...+\lambda _{n}\mathbf{v}_{n}=\mathbf{v}$.
Suppongo di considerare una seconda combinazione lineare $\mu _{1}\mathbf{v}%
_{1}+\mu _{2}\mathbf{v}_{2}+...+\mu _{n}\mathbf{v}_{n}=\mathbf{v}$ e mostro
che i coefficienti devono coincidere uno a uno. Infatti, sottraendo la
seconda dalla prima, si ha $\left( \lambda _{1}-\mu _{1}\right) \mathbf{v}%
_{1}+\left( \lambda _{2}-\mu _{2}\right) \mathbf{v}_{2}+...+\left( \lambda
_{n}-\mu _{n}\right) \mathbf{v}_{n}=\mathbf{0}$. Ma poich\'{e} i vettori
sono linearmente indipendenti, per definizione l'unica soluzione a tale
equazione \`{e} il vettore nullo e quindi tutti i coefficienti sono nulli,
cio\`{e} $\lambda _{1}-\mu _{1}=0,...,\lambda _{n}-\mu _{n}=0$. Dunque i
coefficienti coincidono uno a uno e le due rappresentazioni sono uguali. $%
\blacksquare $

\begin{enumerate}
\item Per verificare se un insieme di vettori $\left\{ \mathbf{v}_{1},...,%
\mathbf{v}_{n}\right\} $ \`{e} una base di $\mathbf{K}^{n}$ bisogna
verificare che la matrice $A=\left[ \mathbf{v}_{1}|...|\mathbf{v}_{n}\right] 
$ abbia rango uguale al numero di righe (affinch\'{e} il sistema lineare $A%
\mathbf{x=v}$ abbia soluzione per ogni $\mathbf{v}$, cio\`{e} $\mathbf{v}%
_{1},...,\mathbf{v}_{n}$ siano generatori) e al numero di colonne (affinch%
\'{e} la soluzione sia unica, per il teorema di Cramer).
\end{enumerate}

Tutti gli spazi vettoriali hanno una base? In generale, quante basi hanno
gli spazi vettoriali?

\textbf{Lemma fondamentale}%
\begin{gather*}
\text{Hp}\text{: }\left( V,+,\cdot \right) \text{ \`{e} uno spazio
vettoriale; }T=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} \text{ \`{e}
un insieme di generatori di }V\text{;} \\
S=\left\{ \mathbf{w}_{1}\mathbf{,...,w}_{m}\right\} \text{ \`{e} un insieme
di }m\text{ vettori di }V\text{, con }m>n \\
\text{Ts}\text{: }S\text{ \`{e} un insieme di vettori linearmente dipendenti}
\end{gather*}

\textbf{Dim} Per ipotesi $V=Span\left( \mathbf{v}_{1}\mathbf{,...,v}%
_{n}\right) $. In particolare $\mathbf{w}_{1}=a_{11}\mathbf{v}_{1}+\mathbf{%
...+}a_{1n}\mathbf{v}_{n}$,..., $\mathbf{w}_{m}=a_{m1}\mathbf{v}_{1}+\mathbf{%
...+}a_{mn}\mathbf{v}_{n}$, con $a_{ij}\in \mathbf{K}$. Cerco di capire se i
vettori di $S$ sono linearmente dipendenti o no: considero $\lambda _{1}%
\mathbf{w}_{1}+\mathbf{...+}\lambda _{m}\mathbf{w}_{m}=\mathbf{0}$ e mi
chiedo se esistano soluzioni $\left[ 
\begin{array}{c}
\lambda _{1} \\ 
... \\ 
\lambda _{m}%
\end{array}%
\right] $ non nulle. L'equazione \`{e} equivalente a $\lambda _{1}\left(
a_{11}\mathbf{v}_{1}+\mathbf{...+}a_{1n}\mathbf{v}_{n}\right) +...+\lambda
_{m}\left( a_{m1}\mathbf{v}_{1}+\mathbf{...+}a_{mn}\mathbf{v}_{n}\right) =%
\mathbf{0}\Longleftrightarrow \left( \lambda _{1}a_{11}+...+\lambda
_{m}a_{m1}\right) \mathbf{v}_{1}+\mathbf{...+}\left( \lambda
_{1}a_{1n}+...+\lambda _{m}a_{mn}\right) \mathbf{v}_{n}=\mathbf{0}$. Questa
equazione \`{e} sicuramente soddisfatta se si annullano simultaneamente
tutti i coefficienti: questa condizione permette quindi di trovare un
sottinsieme delle soluzioni dell'equazione originaria.%
\begin{equation*}
\left\{ 
\begin{array}{c}
\lambda _{1}a_{11}+...+\lambda _{m}a_{m1}=0 \\ 
... \\ 
\lambda _{1}a_{1n}+...+\lambda _{m}a_{mn}=0%
\end{array}%
\right. \Longleftrightarrow \left[ 
\begin{array}{ccc}
a_{11} & ... & a_{1m} \\ 
... & ... & ... \\ 
a_{1n} & ... & a_{mn}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\lambda _{1} \\ 
... \\ 
\lambda _{m}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
... \\ 
0%
\end{array}%
\right]
\end{equation*}

Questo \`{e} un sistema lineare con $n$ righe e $m$ colonne. Poich\'{e} per
ipotesi $m>n$, $r\left( A\right) \leq n<m$; per il teorema di Rouch\'{e}%
-Capelli, le soluzioni dipendono quindi da $m-r\left( A\right) >0$ parametri
liberi (almeno uno). Dunque ci sono infinite soluzioni non nulle
dell'equazione $\lambda _{1}\mathbf{w}_{1}+\mathbf{...+}\lambda _{m}\mathbf{w%
}_{m}=\mathbf{0}$ e $S$ \`{e} un insieme di vettori linearmente dipendenti. $%
\blacksquare $

Questo lemma \`{e} fondamentale per dimostrare il prossimo teorema.

\textbf{Teorema della dimensione}%
\begin{eqnarray*}
\text{Hp}\text{: } &&\left( V,+,\cdot \right) \text{ \`{e} uno spazio
vettoriale su }\mathbf{K}\text{; }T=\left\{ \mathbf{v}_{1}\mathbf{,...,v}%
_{n}\right\} \text{, }S=\left\{ \mathbf{w}_{1}\mathbf{,...,w}_{m}\right\} 
\text{ }\text{sono basi di }V \\
\text{Ts}\text{: } &&m=n
\end{eqnarray*}

cio\`{e} tutte le basi di uno spazio vettoriale hanno la stessa cardinalit%
\`{a}.

\textbf{Dim }Dimostrare $m=n$ \`{e} equivalente a dimostrare $m\leq n$ e $%
n\leq m$.

Dimostro $m\leq n$. Vedo $T$ come un insieme di generatori, $S$ come un
insieme di vettori linearmente indipendenti. Se fosse $m>n$ i vettori di $S$
sarebbero linearmente dipendenti per il lemma fondamentale, ma questo \`{e}
assurdo, quindi dev'essere $m\leq n$.

Dimostro $n\leq m$. Vedo $S$ come un insieme di generatori, $T$ come un
insieme di vettori linearmente indipendenti. Se fosse $n>m$ i vettori di $T$
sarebbero linearmente dipendenti per il lemma fondamentale, ma questo \`{e}
assurdo, quindi dev'essere $n\leq m$. Dunque $n=m$. $\blacksquare $

\textbf{Def} Sia $\left( V,+,\cdot \right) $ uno spazio vettoriale su $%
\mathbf{K}$. Si dice dimensione di $V$, e si indica con $\dim V$, il numero
di elementi di una base di $V$.

La definizione \`{e} ben data perch\'{e} tutte le basi hanno la stessa
cardinalit\`{a}. Se $V$ contiene solo il vettore nullo, la dimensione \`{e} $%
0$ per convenzione.

\begin{enumerate}
\item $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$; $\left\{ \mathbf{i,j}\right\} $ \`{e} una base di $V$. Dunque $\dim
V=2$.

\item $V=\mathbf{K}\left[ t\right] _{\leq d}$. $\left\{
1,t,t^{2},...,t^{d}\right\} $ \`{e} una base di $V$. Dunque $\dim V=d+1$. La
base $\left\{ 1,t,t^{2},...,t^{d}\right\} $ viene detta base canonica.

\item $V=M_{\mathbf{K}}\left( 2,3\right) $. La generica matrice di $M_{%
\mathbf{K}}\left( 2,3\right) $ \`{e} $\left[ 
\begin{array}{ccc}
a_{11} & a_{12} & a_{13} \\ 
a_{21} & a_{22} & a_{23}%
\end{array}%
\right] =a_{11}\left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 0 & 0%
\end{array}%
\right] +a_{12}\left[ 
\begin{array}{ccc}
0 & 1 & 0 \\ 
0 & 0 & 0%
\end{array}%
\right] +...+a_{32}\left[ 
\begin{array}{ccc}
0 & 0 & 0 \\ 
0 & 0 & 1%
\end{array}%
\right] $. Quindi $\left\{ \left[ 
\begin{array}{ccc}
1 & 0 & 0 \\ 
0 & 0 & 0%
\end{array}%
\right] ,\left[ 
\begin{array}{ccc}
0 & 1 & 0 \\ 
0 & 0 & 0%
\end{array}%
\right] ,\left[ 
\begin{array}{ccc}
0 & 0 & 1 \\ 
0 & 0 & 0%
\end{array}%
\right] ,\left[ 
\begin{array}{ccc}
0 & 0 & 0 \\ 
1 & 0 & 0%
\end{array}%
\right] ,\left[ 
\begin{array}{ccc}
0 & 0 & 0 \\ 
0 & 1 & 0%
\end{array}%
\right] ,\left[ 
\begin{array}{ccc}
0 & 0 & 0 \\ 
0 & 0 & 1%
\end{array}%
\right] \right\} $ \`{e} una base di $V$. In generale, $\dim \left( M_{%
\mathbf{K}}\left( m,n\right) \right) =mn$.
\end{enumerate}

Ci sono ulteriori conseguenze del teorema della dimensione e del lemma
fondamentale.

Se $\dim V=n$, $n$ \`{e} il massimo numero di vettori linearmente
indipendenti in $V$ (per il lemma fondamentale in forma negata) e $n$ \`{e}
il numero minimo di vettori generatori di $V$: infatti, se si avessero meno
vettori generatori, se essi fossero linearmente indipendenti sarebbero una
base e $\dim V$ sarebbe minore di $n$, se fossero linearmente dipendenti
sarebbe possibile toglierne alcuni in modo da avere una base e $\dim V$
sarebbe sempre minore di $n$.

Inoltre, considero $T=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{k}\right\} $,
con $k<n$, linearmente indipendenti. $T$ \`{e} un insieme di generatori? Per
le conseguenze del lemma fondamentale, esiste $\mathbf{w}:S=T\cup \left\{ 
\mathbf{w}\right\} $ \`{e} un insieme di vettori linearmente indipendenti.
Quindi $w\notin Span\left( \mathbf{v}_{1}\mathbf{,...,v}_{k}\right) $: porta
un'informazione nuova, perci\`{o} $Span\left( \mathbf{v}_{1}\mathbf{,...,v}%
_{k}\right) \subsetneqq V$ e $T$ non \`{e} un insieme di generatori.

In generale, col teorema della dimensione e il lemma fondamentale si
dimostra che se $\dim V=n$ e un insieme di vettori di $V$ di cardinalit\`{a} 
$n$ \`{e} un insieme di vettori linearmente indipendenti, allora tale
insieme \`{e} una base; se $\dim V=n$ e un insieme di vettori di $V$ di
cardinalit\`{a} $n$ \`{e} un insieme di generatori, allora tale insieme \`{e}
una base (perch\'{e} ogni insieme di generatori contiene una base).

Ci si chiedeva prima se tutti gli spazi vettoriali avessero una base. Ci
sono spazi vettoriali con insiemi di vettori linearmente indipendenti di
cardinalit\`{a} arbitraria; tali spazi non hanno una base con un numero
finito di elementi, perch\'{e} non c'\`{e} un massimo numero di vettori
linearmente indipendenti.

\begin{enumerate}
\item $V=\mathbf{K}\left[ t\right] $ \`{e} lo spazio dei polinomi, senza
limitazioni sul grado. $T_{d}=\left\{ 1,t,t^{2},...,t^{d}\right\} $ \`{e}
una base di $\mathbf{K}\left[ t\right] _{\leq d}$: in questo caso per\`{o}
la cardinalit\`{a} dell'insieme dev'essere infinita; se ci si fermasse a $d$
non si potrebbero generare i polinomi di grado superiore. Si dice quindi che 
$\mathbf{K}\left[ t\right] $ \`{e} uno spazio vettoriale di dimensione
infinita.
\end{enumerate}

Uno spazio vettoriale con base si dice finitamente generato; altrimenti si
dice non finitamente generato.

\begin{enumerate}
\item Considero $V=\mathbf{K}\left[ t\right] _{\leq 2}$; mi chiedo se $%
S=\left\{ 1+t,1+t-t^{2},2-t^{2}\right\} $ \`{e} una base di $\mathbf{K}\left[
t\right] $. Per stabilirlo devo verificare due propriet\`{a}: se i vettori
sono linearmente indipendenti (cio\`{e} $\lambda _{1}\left( 1+t\right)
+\lambda _{2}\left( 1+t-t^{2}\right) +\lambda _{3}\left( 2-t^{2}\right)
=0_{V}$ implica $\lambda _{1}=\lambda _{2}=\lambda _{3}=0$) e se sono
generatori di $V$ (cio\`{e} $\forall $ $a_{0}+a_{1}t+a_{2}t^{2}$ $\exists $ $%
\lambda _{1},\lambda _{2},\lambda _{3}:\lambda _{1}\left( 1+t\right)
+\lambda _{2}\left( 1+t-t^{2}\right) +\lambda _{3}\left( 2-t^{2}\right)
=a_{0}+a_{1}t+a_{2}t^{2}$). Per quanto detto sulle conseguenze del teorema
fondamentale, dato che la dimensione di $V$ \`{e} nota, una di queste due
condizioni \`{e} sufficiente.

\begin{enumerate}
\item $\lambda _{1}\left( 1+t\right) +\lambda _{2}\left( 1+t-t^{2}\right)
+\lambda _{3}\left( 2-t^{2}\right) =0_{V}\Longleftrightarrow \lambda
_{1}+\lambda _{2}+2\lambda _{3}+\left( \lambda _{1}+\lambda _{2}\right)
t+\left( -\lambda _{2}-\lambda _{3}\right) t^{2}=0_{V}$. Un polinomio \`{e}
nullo se i coefficienti di tutti i suoi termini sono nulli, quindi risolvo
il sistema omogeneo $\left\{ 
\begin{array}{c}
\lambda _{1}+\lambda _{2}+2\lambda _{3}=0 \\ 
\lambda _{1}+\lambda _{2}=0 \\ 
-\lambda _{2}-\lambda _{3}=0%
\end{array}%
\right. $ e, usando il MEG e il teorema di Rouch\'{e}-Capelli, se capisco
che c'\`{e} un'unica soluzione, allora \`{e} $\lambda _{1}=\lambda
_{2}=\lambda _{3}=0$ e i vettori sono linearmente indipendenti.

\item $\forall $ $a_{0}+a_{1}t+a_{2}t^{2}$ $\exists $ $\lambda _{1},\lambda
_{2},\lambda _{3}:\lambda _{1}\left( 1+t\right) +\lambda _{2}\left(
1+t-t^{2}\right) +\lambda _{3}\left( 2-t^{2}\right) =a_{0}+a_{1}t+a_{2}t^{2}$%
, cio\`{e} $\lambda _{1}+\lambda _{2}+2\lambda _{3}+\left( \lambda
_{1}+\lambda _{2}\right) t+\left( -\lambda _{2}-\lambda _{3}\right)
t^{2}=a_{0}+a_{1}t+a_{2}t^{2}$? Equivale a chiedersi se il sistema lineare $%
\left\{ 
\begin{array}{c}
\lambda _{1}+\lambda _{2}+2\lambda _{3}=a_{0} \\ 
\lambda _{1}+\lambda _{2}=a_{1} \\ 
-\lambda _{2}-\lambda _{3}=a_{2}%
\end{array}%
\right. $ ammette almeno una soluzione per ogni $a_{0},a_{1},a_{2}$. La
matrice dei coefficienti \`{e} $A=\left[ 
\begin{array}{ccc}
1 & 1 & 2 \\ 
1 & 1 & 0 \\ 
0 & -1 & -1%
\end{array}%
\right] $. E' evidente che la prima condizione \`{e} equivalente a questa,
perch\'{e} se i vettori sono linearmente indipendenti il sistema omogeneo
considerato ha un'unica soluzione e $r\left( A\right) =n$ (in questo caso $3$%
), per il teorema di Rouch\'{e}-Capelli. Allora, per il teorema di Cramer, c'%
\`{e} un'unica soluzione per ogni $a_{0},a_{1},a_{2}$.

In questo caso, riducendo a scala $A$ si ottiene $\left[ 
\begin{array}{ccc}
1 & 1 & 2 \\ 
0 & -1 & -1 \\ 
0 & 0 & -2%
\end{array}%
\right] $: essendo effettivamente $r\left( A\right) =3$, entrambe le
condizioni sono soddisfatte e la terna di polinomi data \`{e} una base di $V$%
.

\item $n$ vettori di $\mathbf{K}^{n}$ sono una sua base se e solo se il
rango della matrice quadrata che essi formano \`{e} $n$, che \`{e}
equivalente a chiedere che il suo determinante sia non nullo.

\item \textit{Sia }$V=\{p(x)=a+bx+cx^{2}:a,b,c\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\}$\textit{\ lo spazio vettoriale dei polinomi reali di grado }$\leq 2$%
\textit{, e sia }$H=\{p(x)\in V:p(2)=0\}$\textit{\ un suo sottospazio. Si
determini una base di }$H$\textit{\ il cui primo elemento sia }$p_{1}(x)=x-2$%
\textit{.}

Il generico polinomio di $H$ ha coefficienti $a,b,c:a+2b+4c=0$, quindi \`{e}
della forma $-2b-4c+bx+cx^{2}$. Risolvendo il sistema lineare aggiungendo i
due parametri liberi si trova $\left( 
\begin{array}{c}
a \\ 
b \\ 
c%
\end{array}%
\right) =t\left( 
\begin{array}{c}
-4 \\ 
0 \\ 
1%
\end{array}%
\right) +s\left( 
\begin{array}{c}
-2 \\ 
1 \\ 
0%
\end{array}%
\right) $, e sostituendo in $a+bx+cx^{2}$ si ottiene $t\left( x^{2}-4\right)
+s\left( x-2\right) $. $\left\{ x^{2}-4,x-2\right\} $ \`{e} una base di $H$:
i vettori sono linearmente indipendenti e generatori di $H$ (per ottenere il
generico polinomio basta prendere $t=c$, $s=b$), e $\dim H=2$. In realt\`{a}%
, avendo intuito che $\dim H=2$ (la condizione $p\left( 2\right) =0$ toglie
un grado di libert\`{a} in $V$), per trovare una base di $H$ contenente $x-2$
\`{e} sufficiente prendere un qualsiasi polinomio di $H$ linearmente
indipendente (cio\`{e} non multiplo) da $x-2$, e. g. $x^{2}+x-6$.
\end{enumerate}
\end{enumerate}

Questo esempio mostra come un problema presentato nello spazio dei polinomi
si traduca naturalmente in un problema matriciale.

\subsection{Spazi generati da una matrice}

\textbf{Def} Data $A\in M_{\mathbf{K}}\left( m,n\right) $, si definisce
spazio delle righe di $A$, e si indica con $\func{row}\left( A\right) $, il
sottospazio vettoriale di $\mathbf{K}^{n}$ (cio\`{e} $M_{\mathbf{K}}\left(
1,n\right) $) generato dalle righe di $A$; si definisce spazio delle colonne
di $A$, e si indica con $\func{col}\left( A\right) $, il sottospazio
vettoriale di $\mathbf{K}^{m}$ (cio\`{e} $M_{\mathbf{K}}\left( m,1\right) $)
generato dalle colonne di $A$; si definisce nucleo di $A$, e si indica con $%
\ker \left( A\right) $, il sottospazio vettoriale di $\mathbf{K}^{n}$ (cio%
\`{e} $M_{\mathbf{K}}\left( n,1\right) $) costituito dalle soluzioni del
sistema omogeneo avente $A$ come matrice dei coefficienti.

\begin{enumerate}
\item $A=\left[ 
\begin{array}{ccc}
1 & 2 & 3 \\ 
4 & 5 & 6%
\end{array}%
\right] $. $\func{row}\left( A\right) =Span\left( \left[ 
\begin{array}{c}
1 \\ 
2 \\ 
3%
\end{array}%
\right] ,\left[ 
\begin{array}{c}
4 \\ 
5 \\ 
6%
\end{array}%
\right] \right) \subseteq 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$. $\func{col}\left( A\right) =Span\left( \left[ 
\begin{array}{c}
1 \\ 
4%
\end{array}%
\right] ,\left[ 
\begin{array}{c}
2 \\ 
5%
\end{array}%
\right] ,\left[ 
\begin{array}{c}
3 \\ 
6%
\end{array}%
\right] \right) \subseteq 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$. $\ker \left( A\right) =\left\{ \left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}:A\left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
0 \\ 
0%
\end{array}%
\right] \right\} $. Risolvo il sistema omogeneo per scrivere esplicitamente
il nucleo: riducendo a scala $A$ ottengo $\left[ 
\begin{array}{ccc}
1 & 2 & 3 \\ 
0 & 1 & 2%
\end{array}%
\right] $, e aggiungendo il parametro per la variabile indipendente e
risolvendo col MEGJ si ottiene $\left[ 
\begin{array}{cccc}
1 & 0 & 0 & t \\ 
0 & 1 & 0 & -2t \\ 
0 & 0 & 1 & t%
\end{array}%
\right] $: $\left[ 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right] =t\left( 
\begin{array}{c}
1 \\ 
-2 \\ 
1%
\end{array}%
\right) $, quindi $\ker \left( A\right) =Span\left( \left[ 
\begin{array}{c}
1 \\ 
-2 \\ 
1%
\end{array}%
\right] \right) $.

\item $V=\mathbf{K}^{3}$. $S=\left\{ \left[ 
\begin{array}{c}
1 \\ 
-1 \\ 
2%
\end{array}%
\right] ,\left[ 
\begin{array}{c}
3 \\ 
4 \\ 
7%
\end{array}%
\right] ,\left[ 
\begin{array}{c}
0 \\ 
-1 \\ 
2%
\end{array}%
\right] ,\left[ 
\begin{array}{c}
1 \\ 
0 \\ 
-2%
\end{array}%
\right] \right\} $. $S$ \`{e} un insieme di generatori di $V$? Se s\`{\i},
come eliminare la ridondanza (dato che $\dim V=3$), cio\`{e} come estrarne
una base? Ci sono due possibilit\`{a}:%
\begin{eqnarray*}
A &=&\left[ 
\begin{array}{ccc}
1 & -1 & 2 \\ 
3 & 4 & 7 \\ 
0 & -1 & 2 \\ 
1 & 0 & -2%
\end{array}%
\right] \text{ e }\func{row}\left( A\right) =Span\left( S\right) \\
A &=&\left[ 
\begin{array}{cccc}
1 & 3 & 0 & 1 \\ 
-1 & 4 & -1 & 0 \\ 
2 & 7 & 2 & -2%
\end{array}%
\right] \text{ e }\func{col}\left( A\right) =Span\left( S\right)
\end{eqnarray*}
\end{enumerate}

\textbf{Lemma (invarianza dello spazio delle righe rispetto al MEG) }%
\begin{eqnarray*}
\text{Hp}\text{: } &&A\in M_{\mathbf{K}}\left( m,n\right) \text{; }B\in M_{%
\mathbf{K}}\left( m,n\right) \text{ \`{e} ottenuta da }A\text{ con una
sequenza di operazioni del MEG} \\
\text{Ts}\text{: } &&\func{row}\left( A\right) =\func{row}\left( B\right) 
\end{eqnarray*}

Ha senso che lo spazio delle righe che si conservi, perch\'{e} con le
operazioni del MEG si annullano le righe che sono combinazioni lineari delle
altre, cio\`{e} che non aggiungono nessuna combinazione lineare.

\textbf{Dim} $\func{row}\left( A\right) =Span\left( R_{1}\left( A\right)
,R_{2}\left( A\right) ,...,R_{m}\left( A\right) \right) $; $\func{row}\left(
B\right) =Span\left( R_{1}\left( B\right) ,R_{2}\left( B\right)
,...,R_{m}\left( B\right) \right) $. Mostrare che $\func{row}\left( A\right)
=\func{row}\left( B\right) $ equivale a mostrare $\func{row}\left( A\right)
\subseteq \func{row}\left( B\right) $ e $\func{row}\left( B\right) \subseteq 
\func{row}\left( A\right) $.

Mostro $\func{row}\left( A\right) \subseteq \func{row}\left( B\right) $, i.
e. che ogni combinazione lineare delle righe di $A$ \`{e} anche una
combinazione lineare delle righe di $B$: \`{e} sufficiente mostrare che
tutti i generatori di $\func{row}\left( A\right) $ appartengono a $\func{row}%
\left( B\right) $, cio\`{e} $R_{i}\left( A\right) \in \func{row}\left(
B\right) $ $\forall $ $i=1,...,m$, perch\'{e} allora anche ogni loro
combinazione lineare apparterr\`{a} a $\func{row}\left( B\right) $, essendo $%
\func{row}\left( B\right) $ un sottospazio. Lo stesso vale per mostrare $%
\func{row}\left( B\right) \subseteq \func{row}\left( A\right) $. E'
sufficiente inoltre verificarlo per una singola operazione del MEG (mi
permette di scrivere una catena di uguaglianze con $\func{row}\left(
A\right) $ all'inizio e $\func{row}\left( B\right) $ alla fine), cio\`{e}
come se $B$ fosse stata ottenuta da $A$ con una singola operazione del MEG.

E' ovvio nel caso dello scambio di righe: sta cambiando solo l'ordine dei
generatori, che \`{e} irrilevante. Nel caso della moltiplicazione per $%
\lambda \neq 0$, si ha che $R_{i}\left( B\right) =\lambda R_{i}\left(
A\right) $: in particolare vale $R_{i}\left( B\right) =0R_{1}\left( A\right)
+...+\lambda R_{i}\left( A\right) +...+0R_{m}\left( A\right) $, l'unico
generatore che differisce nei due sottospazi \`{e} $R_{i}\left( B\right) $,
e $R_{i}\left( B\right) \in Span\left( R_{1}\left( A\right) ,...,R_{m}\left(
A\right) \right) =\func{row}\left( A\right) $. Inoltre $R_{i}\left( A\right)
=\frac{1}{\lambda }R_{i}\left( B\right) $, quindi analogamente $R_{i}\left(
A\right) \in \func{row}\left( B\right) $. Nel caso della sostituzione di una
riga con la sua somma con un multiplo di un'altra, si ha che $R_{i}\left(
B\right) =R_{i}\left( A\right) +\lambda R_{j}\left( A\right) \in \func{row}%
\left( A\right) $ e $R_{i}\left( A\right) =R_{i}\left( B\right) -\lambda
R_{j}\left( B\right) \in \func{row}\left( B\right) $ (perch\'{e} $R_{j}$ 
\`{e} uguale sia in $A$ che in $B$). $\blacksquare $

Quindi, se riduco $A$ a scala e ottengo $U$, le righe non nulle di $U$ sono
una base di $\func{row}\left( U\right) =\func{row}\left( A\right) $. Infatti
le righe di $A$ sono un insieme di generatori, ma possibilmente non
linearmente indipendenti: le operazioni del MEG fanno s\`{\i} che quelle che
sono combinazione lineari delle altre diventino nulle, cio\`{e} eliminano
quelle ridondanti, per cui le rimanenti costituiscono una base. Se $U$ non
ha righe nulle, cio\`{e} $r\left( U\right) =\min \left\{ m,n\right\} $,
significa che le righe di $A$ erano linearmente indipendenti.

\textbf{Teorema del rango}%
\begin{gather*}
\text{Hp}\text{: }A\in M_{\mathbf{K}}\left( m,n\right) \text{, }r=r\left(
A\right) \\
\text{Ts}\text{:}\text{ (}\func{row}\left( A\right) =\func{row}\left(
U\right) \text{)} \\
\text{(i) }\dim \left( \func{row}\left( U\right) \right) =\dim \left( \func{%
row}\left( A\right) \right) =r \\
\text{(ii) }\dim \left( \ker \left( A\right) \right) =n-r \\
\text{(iii) }\dim \left( \func{col}\left( A\right) \right) =r
\end{gather*}

Quindi il rango di una matrice \`{e} la dimensione dello spazio delle righe
(o delle colonne).

\textbf{Dim} Dall'ultima lemma visto \`{e} noto $\func{row}\left( A\right) =%
\func{row}\left( U\right) $.

(i) Voglio mostrare che le righe contenenti i pivot (che sono $r$) formano
una base di $\func{row}\left( A\right) $. Mostro che sono generatori di $%
\func{row}\left( U\right) $. Un generico vettore $\mathbf{v}\in \func{row}%
\left( U\right) $ si scrive come $\lambda _{1}R_{1}\left( U\right)
+...+\lambda _{r}R_{r}\left( U\right) +\lambda _{r+1}0+...+\lambda _{m}0$:
quindi $Span\left( R_{1}\left( U\right) ,...,R_{m}\left( U\right) \right)
=Span\left( R_{1}\left( U\right) ,...,R_{r}\left( U\right) \right) $. Mostro
che sono linearmente indipendenti. Prendo la matrice ridotta a scala $U$ e
tolgo le righe nulle: voglio mostrare che $\lambda _{1}R_{1}\left( U\right)
+...+\lambda _{r}R_{r}\left( U\right) =\mathbf{0}\Longrightarrow \lambda
_{1}=...=\lambda _{r}=0$. Mostro che $\lambda _{1}=0$ considerando la
posizione della colonna contenente il primo pivot. Dal momento che nella
colonna del primo pivot $p_{1}$ \`{e} l'unico elemento non nullo, in tal
colonna si deve avere $\lambda _{1}p_{1}+0=0$, dunque $\lambda _{1}=0$. Per
dimostrare $\lambda _{2}=0$ considero la colonna del secondo pivot; so che $%
\lambda _{1}=0$. Allora la somma sar\`{a} $\lambda _{1}u_{1k}+\lambda
_{2}p_{2}+0=0$: $\lambda _{2}=0$ e cos\`{\i} via. Dunque $\lambda
_{1}=...=\lambda _{r}=0$ e $R_{1}\left( U\right) ,...,R_{r}\left( U\right) $
sono linearmente indipendenti: perci\`{o} $\dim \left( \func{row}\left(
U\right) \right) =r$.

(ii) Per definizione $\ker \left( A\right) =\left\{ \mathbf{x\in K}^{n}:A%
\mathbf{x=0}\right\} $. Per il teorema di Rouch\'{e}-Capelli $\exists $ $%
\mathbf{v}_{1}\mathbf{,...,v}_{n-r}\in \mathbf{K}^{n}:\mathbf{x}=\lambda _{1}%
\mathbf{v}_{1}+...+\lambda _{n-r}\mathbf{v}_{n-r}\in \ker \left( A\right) $.
Quindi $Span\left( \mathbf{v}_{1}\mathbf{,...,v}_{n-r}\right) =\ker \left(
A\right) $: sono generatori. Mostro che $\mathbf{v}_{1}\mathbf{,...,v}_{n-r}$
sono linearmente indipendenti. Supponiamo che le variabili indipendenti del
sistema $A\mathbf{x=0}$ siano le ultime $n-r$: $r\left( A\right) =r$. Scrivo
il generico elemento del kernel $\lambda _{1}\mathbf{v}_{1}+...+\lambda
_{n-r}\mathbf{v}_{n-r}=\left[ 
\begin{array}{c}
\ast \\ 
\lambda _{1} \\ 
\lambda _{2} \\ 
... \\ 
\lambda _{n-r}%
\end{array}%
\right] =\lambda _{1}\left[ 
\begin{array}{c}
\ast \\ 
1 \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right] +...+\lambda _{n-r}\left[ 
\begin{array}{c}
\ast \\ 
0 \\ 
... \\ 
0 \\ 
1%
\end{array}%
\right] $ dove $\ast $ sono le componenti dei vettori nelle righe $1,...,r$. 
$\lambda _{1}\mathbf{v}_{1}+...+\lambda _{n-r}\mathbf{v}_{n-r}=\mathbf{0}$
implica (nella riga $r+1$) $\lambda _{1}=0$, (nella riga $r+2$) $\lambda
_{2}=0,...,\lambda _{n-r}=0$.

(iii) [In generale $\func{col}\left( A\right) \neq \func{col}\left( U\right) 
$: ad esempio se $A=\left[ 
\begin{array}{cc}
1 & 1 \\ 
2 & 2%
\end{array}%
\right] $ e $U=\left[ 
\begin{array}{cc}
1 & 1 \\ 
0 & 0%
\end{array}%
\right] $, $\func{col}\left( A\right) =Span\left( \left[ 
\begin{array}{c}
1 \\ 
2%
\end{array}%
\right] \right) $ \`{e} una retta passante per l'origine con pendenza $2$, $%
\func{col}\left( U\right) =Span\left( \left[ 
\begin{array}{c}
1 \\ 
0%
\end{array}%
\right] \right) $.]

$\ker \left( A\right) =\ker \left( U\right) $ perch\'{e} l'insieme delle
soluzioni del sistema omogeneo $A\mathbf{x=0}$ (e in generale di un sistema
lineare) \`{e} lasciato immutato dalle operazioni elementari sulle righe. $%
\ker \left( A\right) =\left\{ \text{relazioni di dipendenza lineare tra le
colonne di }A\right\} $: infatti $\ker \left( A\right) $ \`{e} l'insieme
degli $\left( 
\begin{array}{c}
x_{1} \\ 
... \\ 
x_{n}%
\end{array}%
\right) $ che soddisfano $\left\{ 
\begin{array}{c}
a_{11}x_{1}+a_{12}x_{2}+...+a_{1n}x_{n}=0 \\ 
a_{21}x_{1}+a_{22}x_{2}+...+a_{2n}x_{n}=0 \\ 
... \\ 
a_{m1}x_{1}+a_{m2}x_{2}+...+a_{mn}x_{n}=0%
\end{array}%
\right. $: $\left[ 
\begin{array}{c}
a_{11} \\ 
a_{21} \\ 
... \\ 
a_{m1}%
\end{array}%
\right] x_{1}+\left[ 
\begin{array}{c}
a_{12} \\ 
a_{22} \\ 
... \\ 
a_{m2}%
\end{array}%
\right] x_{2}+...+\left[ 
\begin{array}{c}
a_{1n} \\ 
a_{2n} \\ 
... \\ 
a_{nn}%
\end{array}%
\right] x_{n}=\mathbf{0}$.

$\mathbf{x}\in \ker \left( A\right) $ se e solo se soddisfa tale equazione.
Dire che $\ker \left( A\right) =\ker \left( U\right) $ \`{e} equivalente a
dire che se $\left\{ C_{i_{1}}\left( A\right) ,...,C_{i_{k}}\left( A\right)
\right\} $ (dove $C_{i_{1}}\left( A\right) $ identifica la prima colonna di $%
A$ scelta arbitrariamente) \`{e} un insieme di vettori linearmente
indipendenti, allora anche $\left\{ C_{i_{1}}\left( U\right)
,...,C_{i_{k}}\left( U\right) \right\} $ \`{e} un insieme di vettori
linearmente indipendenti. [Per esempio le prime tre colonne di $A$ sono
indipendenti se e solo se le prime tre colonne di $U$ sono indipendenti,
perch\'{e} questo significa che nel nucleo non si trova nessun vettore con
le prime tre componenti non tutte nulle e le altre uguali a zero.] Si pu\`{o}
interpretare $\dim \left( \func{col}\left( A\right) \right) $ come il
massimo numero di vettori colonna linearmente indipendenti di $A$. Allora,
scegliendo come $\left\{ C_{i_{1}}\left( A\right) ,...,C_{i_{k}}\left(
A\right) \right\} $ l'insieme dei vettori colonna linearmente indipendenti
di cardinalit\`{a} massima, anche $\left\{ C_{i_{1}}\left( U\right)
,...,C_{i_{k}}\left( U\right) \right\} $ \`{e} un insieme di vettori
linearmente indipendenti di $U$ di cardinalit\`{a} massima, quindi $\dim
\left( \func{col}\left( A\right) \right) =\dim \left( \func{col}\left(
U\right) \right) $. Ora mostro che $\dim \left( \func{col}\left( U\right)
\right) =r$. Voglio mostrare che le colonne non contenenti un pivot si
esprimono come combinazione lineare delle colonne contenenti i pivot. Prendo
la soluzione di $U\mathbf{x=0}$ con $t_{1}=1$, $t_{2}=...=t_{n-r}=0$: $%
\mathbf{v}_{1}=\left( 
\begin{array}{c}
v_{11} \\ 
... \\ 
v_{1r} \\ 
1 \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right) $. Allora $v_{11}C_{1}\left( U\right) +...+v_{1r}C_{r}\left(
U\right) +1\cdot C_{r+1}\left( U\right) +...+0C_{n}\left( U\right) =\mathbf{0%
}$ perch\'{e} $\mathbf{v}_{1}$ \`{e} una soluzione (sto supponendo che i
pivot siano tutti a sinistra). Quindi $C_{r+1}\left( U\right)
=-v_{11}C_{1}\left( U\right) -...-v_{1r}C_{r}\left( U\right) $. ... Prendo
la soluzione di $U\mathbf{x=0}$ con $t_{1}=...=t_{n-r-1}=0$, $t_{n-r}=1$: $%
\mathbf{v}_{n-r}=\left( 
\begin{array}{c}
v_{n-r,1} \\ 
... \\ 
v_{n-r,r} \\ 
0 \\ 
0 \\ 
... \\ 
1%
\end{array}%
\right) $. Allora $v_{n-r,1}C_{1}\left( U\right) +...+v_{n-r,r}C_{r}\left(
U\right) +0C_{r+1}\left( U\right) +...+0C_{n-1}\left( U\right) +1C_{n}\left(
U\right) =\mathbf{0}$ perch\'{e} $\mathbf{v}_{n-r}$ \`{e} una soluzione.
Quindi $C_{n}\left( U\right) =-v_{n-r,1}C_{1}\left( U\right)
-...-v_{n-r,r}C_{r}\left( U\right) $. Ho mostrato che $C_{1}\left( U\right)
,...,C_{r}\left( U\right) $ sono generatori di $\func{col}\left( U\right) $:
le ultime $n-r$ colonne non aggiungono combinazioni lineari, perch\'{e} sono
combinazioni lineari delle altre. Sono anche linearmente indipendenti:
mostro che $\lambda _{1}C_{1}\left( U\right) +...+\lambda _{r}C_{r}\left(
U\right) =\mathbf{0}$ implica $\lambda _{1}=...=\lambda _{r}=0$.%
\begin{equation*}
\lambda _{1}\left[ 
\begin{array}{c}
p_{1} \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right] +\lambda _{2}\left[ 
\begin{array}{c}
\ast \\ 
p_{2} \\ 
... \\ 
0%
\end{array}%
\right] +...+\lambda _{r}\left[ 
\begin{array}{c}
\ast \\ 
... \\ 
p_{r} \\ 
0%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0 \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right]
\end{equation*}

Considero la riga $r$: corrisponde all'equazione $\lambda _{r}p_{r}=0$, $%
p_{r}\neq 0$, quindi dev'essere $\lambda _{r}=0$. Considero la riga $r-1$: $%
\lambda _{r-1}p_{r-1}+\lambda _{r}u_{r-1},_{r}=0$, ma $\lambda _{r}=0$ e $%
p_{r-1}\neq 0$, quindi $\lambda _{r-1}=0$, e cos\`{\i} via fino alla riga $1$%
. Quindi i vettori sono linearmente indipendenti, quindi $\dim \left( \func{%
col}\left( U\right) \right) =r=\dim \left( \func{col}\left( A\right) \right) 
$. $\blacksquare $

Ne segue che:

\begin{description}
\item[-] Possiamo usare il MEG per l'estrazione di una base trasformando i
vettori riga in vettori colonna, oppure no. Se $S=\left\{ \mathbf{v}_{1}%
\mathbf{,...,v}_{k}\right\} $, $\mathbf{v}_{i}\in \mathbf{K}^{n}$, e tali
vettori sono generatori dello spazio vettoriale di cui si cerca la base
(quindi $k>n$), si considera $A=\left[ \mathbf{v}_{1}\mathbf{|v}_{2}\mathbf{%
|...|v}_{k}\right] $. Col MEG si ottiene $U$: la base estratta \`{e} formata
dai vettori $\mathbf{v}_{i}$ corrispondenti alle colonne contenenti i pivot,
perch\'{e} lo spazio delle colonne non si preserva ma l'indipendenza lineare
s\`{\i}. Se invece si
considera $A=\left[ 
\begin{array}{c}
\mathbf{v}_{1} \\ 
\mathbf{v}_{2} \\ 
... \\ 
\mathbf{v}_{k}%
\end{array}%
\right] $ usando i vettori come righe, le righe non nulle (= le righe che
contengono i pivot) di $U$ sono i vettori che formano la base (posso anche
prendere le righe originarie nelle posizione delle righe coi pivot, ma sono
meno semplici da usare perch\'{e} hanno meno componenti nulle).

\item[-] Vogliamo completare un insieme di vettori linearmente indipendenti
in modo che siano una base. Se $S=\left\{ \mathbf{v}_{1}\mathbf{,...,v}%
_{k}\right\} $, $\mathbf{v}_{i}\in \mathbf{K}^{n}$, e tali vettori sono
linearmente indipendenti (quindi $k<n$), si considera $A=\left[ 
\begin{array}{c}
\mathbf{v}_{1} \\ 
... \\ 
\mathbf{v}_{k} \\ 
\mathbf{e}_{1} \\ 
... \\ 
\mathbf{e}_{n}%
\end{array}%
\right] $, a cui ho aggiunto gli $n$ vettori della base canonica di $\mathbf{%
K}^{n}$; $A$ ha $k+n$ righe. Riduco $A$ a scala col MEG. $U$ ha $k$ pivot
nella prima met\`{a} perch\'{e} i vettori $\mathbf{v}_{1}\mathbf{,...,v}_{k}$
erano linearmente indipendenti. La parte con la base canonica ha $n-k$ pivot
e $k$ righe nulle: gli elementi della base canonica superflui sono $k$. Gli $%
n-k$ sono quelli necessari per ottenere una base insieme ai $k$ vettori
originari: insieme hanno cardinalit\`{a} $n$ e sono linearmente
indipendenti. Le righe da $k+1$ a $n$ forniscono il completamento, che
consiste di $n-k$ vettori riga. Di fatto il problema del completamento a una
base \`{e} ridotto al problema di estrazione, perch\'{e} ho aggiunto un
insieme di generatori. Poich\'{e} lo spazio dello righe si preserva, posso
scegliere come base, oltre al completamento, le righe da $1$ a $k$ dopo la
riduzione a scala, oppure i $k$ vettori riga originari.

\item Posso fare lo stesso con le colonne, ma alla fine devo scegliere i
vettori originari nelle posizioni corrispondenti alle colonne con i pivot.

\item[-] Qual \`{e} il significato di $\func{col}\left( A\right) $? Prendo
un sistema $A\mathbf{x=b}$ e suppongo $r\left( A\right) =r\left( A|\mathbf{b}%
\right) $, cio\`{e} che il sistema sia risolubile. Allora esiste una
soluzione $\mathbf{v}=\left( 
\begin{array}{c}
v_{1} \\ 
v_{2} \\ 
... \\ 
v_{n}%
\end{array}%
\right) :v_{1}C_{1}\left( A\right) +v_{2}C_{2}\left( A\right)
+...+v_{n}C_{n}\left( A\right) =\mathbf{b}$. Quindi $\func{col}\left(
A\right) $, cio\`{e} l'insieme dei termini generici $v_{1}C_{1}\left(
A\right) +v_{2}C_{2}\left( A\right) +...+v_{n}C_{n}\left( A\right) $ al
variare di $v_{1},...,v_{n}$, \`{e} l'insieme (lo spazio vettoriale) dei
vettori $\mathbf{b}$ per cui il sistema ammette soluzione.
\end{description}

\subsection{Operazioni tra sottospazi}

Sia $\left( V,+,\cdot \right) $ uno spazio vettoriale su $\mathbf{K}$. Siano 
$H_{1},H_{2}$ sottospazi vettoriali di $V$.

\begin{enumerate}
\item $V=\mathbf{K}^{n}$. $H_{1}=\left\{ \mathbf{x\in K}^{n}:A\mathbf{x=0}%
\right\} $, $H_{2}=\left\{ \mathbf{x\in K}^{n}:B\mathbf{x=0}\right\} $.
\end{enumerate}

E' naturale chiedersi quali sono le caratteristiche geometriche degli spazi $%
H_{1}\cap H_{2}$ e $H_{1}\cup H_{2}$.

\subsubsection{Intersezione}

Considero $H_{1}\cap H_{2}$: \`{e} un sottospazio vettoriale di $V$? $%
\mathbf{0}\in H_{1},H_{2}$. $H_{1}\cap H_{2}$ \`{e} chiuso rispetto alle
operazioni di $V$?

Dati $\mathbf{v,w}\in H_{1}\cap H_{2}$, $\mathbf{v+w}\in H_{1}\cap H_{2}$? $%
\mathbf{v,w}\in H_{1}$, $\mathbf{v,w}\in H_{2}$, quindi, poich\'{e} $H_{1}$
e $H_{2}$ sono sottospazi, $\mathbf{v+w}\in H_{1}$ e $\mathbf{v+w}\in H_{2}$%
, quindi $\mathbf{v+w}\in H_{1}\cap H_{2}$.

Dati $\lambda \in \mathbf{K}$ e $\mathbf{v}\in H_{1}\cap H_{2}$, $\lambda 
\mathbf{v}\in H_{1}\cap H_{2}$? $\lambda \mathbf{v}\in H_{1}$, $\lambda 
\mathbf{v}\in H_{2}$, quindi $\lambda \mathbf{v}\in H_{1}\cap H_{2}$. Perci%
\`{o} $H_{1}\cap H_{2}$ \`{e} un sottospazio vettoriale.

\begin{enumerate}
\item Nell'esempio sopra, $H_{1}\cap H_{2}=\left\{ \mathbf{x}\in \mathbf{K}%
^{n}:C\mathbf{x=0}\right\} $, dove $C=\left[ 
\begin{array}{c}
A \\ 
B%
\end{array}%
\right] $.
\end{enumerate}

\subsubsection{Somma}

Considero $H_{1}\cup H_{2}$: \`{e} un sottospazio vettoriale di $V$? $%
\mathbf{0}\in H_{1},H_{2}$. $H_{1}\cup H_{2}$ \`{e} chiuso rispetto alle
operazioni di $V$?

Dati $\lambda \in \mathbf{K}$, $\mathbf{v}\in H_{1}\cup H_{2}$, $\lambda 
\mathbf{v}\in H_{1}\cup H_{2}$? $\lambda \mathbf{v}\in H_{1}$ o $\lambda 
\mathbf{v}\in H_{2}$, quindi $\lambda \mathbf{v}\in H_{1}\cup H_{2}$.

Dati $\mathbf{v,w}\in H_{1}\cup H_{2}$, $\mathbf{v+w}\in H_{1}\cup H_{2}$?
Ci sono quattro casi: $\mathbf{v,w}\in H_{1}$, o $\mathbf{v,w}\in H_{2}$, o $%
\mathbf{v}\in H_{1}$ e $\mathbf{w}\in H_{2}$, o $\mathbf{v}\in H_{2}$ e $%
\mathbf{w}\in H_{1}$. Nei primi due $\mathbf{v+w}\in H_{1}\cup H_{2}$; negli
ultimi due non lo sappiamo.

Se $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$ e considero come sottospazi due rette (la dimensione \`{e} $1$),
sommando due elementi delle rette \`{e} possibile ottenere un elemento che
non appartiene a nessuna delle due. Quindi in generale $H_{1}\cup H_{2}$ non 
\`{e} un sottospazio vettoriale: \`{e} un insieme troppo piccolo per essere
chiuso rispetto alle operazioni. Ma allora, qual \`{e} il pi\`{u} piccolo
sottospazio che contiene $H_{1}\cup H_{2}$?

Se $H_{1}=Span\left( \mathbf{v}_{1}\mathbf{,...,v}_{k}\right) $, $%
H_{2}=Span\left( \mathbf{w}_{1}\mathbf{,...,w}_{h}\right) $, l'insieme dei
vettori che sono combinazione lineare di $\mathbf{v}_{1}\mathbf{,...,v}_{k}$
o di $\mathbf{w}_{1}\mathbf{,...,w}_{h}$ non \`{e} un sottospazio: non \`{e}
chiuso rispetto alla somma. Devo quindi aggiungere tutte le possibile somme:
il pi\`{u} piccolo sottospazio che contiene $H_{1}\cup H_{2}$ \`{e} $%
Span\left( \mathbf{v}_{1}\mathbf{,...,v}_{k},\mathbf{w}_{1}\mathbf{,...,w}%
_{h}\right) $.

Infatti, dato $\mathbf{v}\in H_{1}$, $\mathbf{v}=\lambda _{1}\mathbf{v}_{1}+%
\mathbf{...+}\lambda _{k}\mathbf{v}_{k}$, dato $\mathbf{w}\in H_{1}$, $%
\mathbf{w}=\mu _{1}\mathbf{w}_{1}+\mathbf{...+}\mu _{h}\mathbf{w}_{h}$, $%
\mathbf{v+w}=\lambda _{1}\mathbf{v}_{1}+\mathbf{...+}\lambda _{k}\mathbf{v}%
_{k}+\mu _{1}\mathbf{w}_{1}+\mathbf{...+}\mu _{h}\mathbf{w}_{h}$: affinch%
\'{e} $\mathbf{w+v}$ appartenga al sottospazio l'unione va estesa allo span
di tutti gli elementi.

\textbf{Def} Si definisce spazio vettoriale somma $H_{1}+H_{2}$ lo $%
Span\left( \mathbf{v}_{1},\mathbf{...,v}_{k},\mathbf{w}_{1},\mathbf{...,w}%
_{h}\right) $.

Quindi $H_{1}+H_{2}$ \`{e} l'insieme dei vettori $\mathbf{u}$ che si possono
scrivere come $\mathbf{v+w}$, dove $\mathbf{v}\in H_{1}$ e $\mathbf{w}\in
H_{2}$.

\begin{enumerate}
\item $H_{1}\cup H_{2}$ \`{e} un sottospazio se e solo se $H_{1}\subseteq
H_{2}$ vel $H_{2}\subseteq H_{1}$.

\item L'intersezione $\bigcap_{i}S_{i}$ di un numero finito o infinito di
sottospazi di $V$ \`{e} un sottospazio di $V$.

\item Se $S=\left\{ U_{0},U_{1},...\right\} $ \`{e} un insieme di sottospazi
di $V$ e per ogni coppia di sottospazi $U_{i},U_{j}\in S$ esiste un
sottospazio $U_{k}\in S$ che li contiene entrambi, $S$ si dice filtrante. In
tal caso l'unione dei sottospazi $U_{i}$ \`{e} un sottospazio. Se $%
U_{0}\subseteq U_{1}\subseteq ...$, $S$ si dice catena e anche in tal caso
l'unione dei sottospazi $U_{i}$ \`{e} un sottospazio.

Se $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\left[ x\right] $ e $S$ \`{e} l'insieme dei sottospazi di $V$ di dimensione
finita, $S$ \`{e} filtrante.
\end{enumerate}

\textbf{Teorema (formula di Grassman)}%
\begin{eqnarray*}
\text{Hp}\text{: } &&\left( V,+,\cdot \right) \text{ \`{e} uno spazio
vettoriale su }\mathbf{K}\text{, }H_{1},H_{2}\text{ sono sottospazi di }V \\
\text{Ts}\text{: } &&\dim \left( H_{1}+H_{2}\right) =\dim H_{1}+\dim
H_{2}-\dim \left( H_{1}\cap H_{2}\right) 
\end{eqnarray*}

\textbf{Dim} Sia $B=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} $ una
base di $H_{1}\cap H_{2}$. $\mathbf{v}_{1}\mathbf{,...,v}_{n}$ \`{e} un
insieme di vettori linearmente indipendenti di $H_{1}\cap H_{2}$, quindi
anche di $H_{1}$ e $H_{2}$ singolarmente. Allora posso completare $B$ a una
base di $H_{1}$ e posso completare $B$ a una base di $H_{2}$. $B_{1}=B\cup
\left\{ \mathbf{w}_{1}\mathbf{,...,w}_{p}\right\} =\left\{ \mathbf{v}_{1}%
\mathbf{,...,v}_{n}\right\} \cup \left\{ \mathbf{w}_{1}\mathbf{,...,w}%
_{p}\right\} $ \`{e} una base di $H_{1}$, $B_{2}=\left\{ \mathbf{v}_{1}%
\mathbf{,...,v}_{n}\right\} \cup \left\{ \mathbf{u}_{1}\mathbf{,...,u}%
_{q}\right\} $ \`{e} una base di $H_{2}$. Si sta dicendo che $\dim \left(
H_{1}\cap H_{2}\right) =n$, $\dim H_{1}=n+p$, $\dim H_{2}=n+q$. Voglio
mostrare che $\dim \left( H_{1}+H_{2}\right) =n+p+q$: in particolare, che $%
\bar{B}=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} \cup \left\{ 
\mathbf{w}_{1}\mathbf{,...,w}_{p}\right\} \cup \left\{ \mathbf{u}_{1}\mathbf{%
,...,u}_{q}\right\} $ \`{e} una base di $H_{1}+H_{2}$.

Mostro che tali vettori sono generatori di $H_{1}+H_{2}$. In generale $%
H_{1}+H_{2}$ \`{e} l'insieme dei vettori $\mathbf{v}$ che si possono
scrivere come $\mathbf{w+u}$, dove $\mathbf{w}\in H_{1}$ e $\mathbf{u}\in
H_{2}$: $\mathbf{w}=\lambda _{1}\mathbf{v}_{1}+\mathbf{...+}\lambda _{n}%
\mathbf{v}_{n}+\lambda _{n+1}\mathbf{w}_{1}+\mathbf{...+}\lambda _{n+p}%
\mathbf{w}_{p}$, mentre $\mathbf{u}=\mu _{1}\mathbf{v}_{1}+\mathbf{...+}\mu
_{n}\mathbf{v}_{n}+\mu _{n+1}\mathbf{u}_{1}+\mathbf{...+}\mu _{n+q}\mathbf{u}%
_{q}$. Dunque $\mathbf{w+u}=\lambda _{1}\mathbf{v}_{1}+\mathbf{...+}\lambda
_{n}\mathbf{v}_{n}+\lambda _{n+1}\mathbf{w}_{1}+\mathbf{...+}\lambda _{n+p}%
\mathbf{w}_{p}+\mu _{1}\mathbf{v}_{1}+\mathbf{...+}\mu _{n}\mathbf{v}%
_{n}+\mu _{n+1}\mathbf{u}_{1}+\mathbf{...+}\mu _{n+q}\mathbf{u}_{q}=$

$\left( \lambda _{1}+\mu _{1}\right) \mathbf{v}_{1}+\mathbf{...+}\left(
\lambda _{n}+\mu _{n}\right) \mathbf{v}_{n}+\lambda _{n+1}\mathbf{w}_{1}+%
\mathbf{...+}\lambda _{n+p}\mathbf{w}_{p}+\mu _{n+1}\mathbf{u}_{1}+\mathbf{%
...+}\mu _{n+q}\mathbf{u}_{q}$. Quindi $H_{1}+H_{2}$ \`{e} l'insieme delle
combinazioni lineari dei vettori $\left\{ \mathbf{v}_{1}\mathbf{,...,v}%
_{n}\right\} \cup \left\{ \mathbf{w}_{1}\mathbf{,...,w}_{p}\right\} \cup
\left\{ \mathbf{u}_{1}\mathbf{,...,u}_{q}\right\} $, che dunque generano $%
H_{1}+H_{2}$.

Mostro che sono linearmente indipendenti. Voglio mostrare che $x_{1}\mathbf{v%
}_{1}+\mathbf{...+}x_{n}\mathbf{v}_{n}+y_{1}\mathbf{w}_{1}+\mathbf{...+}y_{p}%
\mathbf{w}_{p}+z_{1}\mathbf{u}_{1}+\mathbf{...+}z_{q}\mathbf{u}_{q}=\mathbf{0%
}$ implica $x_{1}=...=x_{n}=y_{1}=....=y_{p}=z_{1}=...=z_{q}=0$. Chiamo $%
\mathbf{z}$ la somma dei primi $n+p$ elementi (potrei anche prendere il
primo e il terzo gruppo; l'idea \`{e} sfruttare l'indipendenza lineare dei
vettori di una delle basi note): $\mathbf{z}=x_{1}\mathbf{v}_{1}+\mathbf{...+%
}x_{n}\mathbf{v}_{n}+y_{1}\mathbf{w}_{1}+\mathbf{...+}y_{p}\mathbf{w}%
_{p}=-\left( z_{1}\mathbf{u}_{1}+\mathbf{...+}z_{q}\mathbf{u}_{q}\right) $.
Dalla prima equazione si ha che $\mathbf{z}\in H_{1}$, dalla seconda che $%
\mathbf{z}\in Span\left( \mathbf{u}_{1}\mathbf{,...,u}_{q}\right) \subseteq
H_{2}$, dunque $\mathbf{z}\in H_{1}\cap H_{2}$. Dato che $B=\left\{ \mathbf{v%
}_{1}\mathbf{,...,v}_{n}\right\} $ \`{e} una base di $H_{1}\cap H_{2}$,
posso scrivere $\mathbf{z}=\lambda _{1}\mathbf{v}_{1}+\mathbf{...+}\lambda
_{n}\mathbf{v}_{n}$. Allora si ottiene $\mathbf{z}=\lambda _{1}\mathbf{v}%
_{1}+\mathbf{...+}\lambda _{n}\mathbf{v}_{n}=x_{1}\mathbf{v}_{1}+\mathbf{...+%
}x_{n}\mathbf{v}_{n}+y_{1}\mathbf{w}_{1}+\mathbf{...+}y_{p}\mathbf{w}_{p}$,
che \`{e} equivalente a $\left( x_{1}-\lambda _{1}\right) \mathbf{v}_{1}+%
\mathbf{...+}\left( x_{n}-\lambda _{n}\right) \mathbf{v}_{n}+y_{1}\mathbf{w}%
_{1}+\mathbf{...+}y_{p}\mathbf{w}_{p}=\mathbf{0}$. Ma siccome $\mathbf{v}_{1}%
\mathbf{,...,v}_{n},\mathbf{w}_{1}\mathbf{,...,w}_{p}$ sono linearmente
indipendenti perch\'{e} sono una base di $H_{1}$, si ha $x_{1}=\lambda _{1}%
\mathbf{,...,}x_{n}=\lambda _{n}\mathbf{,}y_{1}=0,\mathbf{...,}y_{p}=0$.
Quindi nell'equazione $x_{1}\mathbf{v}_{1}+\mathbf{...+}x_{n}\mathbf{v}%
_{n}+y_{1}\mathbf{w}_{1}+\mathbf{...+}y_{p}\mathbf{w}_{p}+z_{1}\mathbf{u}%
_{1}+\mathbf{...+}z_{q}\mathbf{u}_{q}=\mathbf{0}$ il secondo gruppo si
annulla e si ottiene $x_{1}\mathbf{v}_{1}+\mathbf{...+}x_{n}\mathbf{v}%
_{n}+z_{1}\mathbf{u}_{1}+\mathbf{...+}z_{q}\mathbf{u}_{q}=\mathbf{0}$. Ma $%
\mathbf{v}_{1}\mathbf{,...,v}_{n},\mathbf{u}_{1}\mathbf{,...,u}_{q}$ sono
una base di $H_{2}$, quindi sono linearmente indipendenti e si ha $%
x_{1}=...=x_{n}=z_{1}=...=z_{q}=0$. Dunque $\left\{ \mathbf{v}_{1}\mathbf{%
,...,v}_{n}\right\} \cup \left\{ \mathbf{w}_{1}\mathbf{,...,w}_{p}\right\}
\cup \left\{ \mathbf{u}_{1}\mathbf{,...,u}_{q}\right\} $ \`{e} un insieme di
vettori linearmente indipendenti. $\blacksquare $

Nel caso $H_{1}\cap H_{2}=\left\{ \mathbf{0}\right\} $, $\dim \left(
H_{1}\cap H_{2}\right) =0$ (non c'\`{e} alcuna "informazione" in comune). La
formula di Grassman diventa allora $\dim \left( H_{1}+H_{2}\right) =\dim
H_{1}+\dim H_{2}$.

\textbf{Def} Se $H_{1}\cap H_{2}=\left\{ \mathbf{0}\right\} $, la somma $%
H_{1}+H_{2}$ si dice somma diretta tra $H_{1}$ e $H_{2}$ e si indica con $%
H_{1}\oplus H_{2}$.

\textbf{Teo}

\begin{eqnarray*}
\text{Hp} &\text{: }&H_{1},H_{2}\text{ sono sottospazi di }V\text{, }%
B_{1},B_{2}\text{ sono basi rispettivamente di }H_{1}\text{ e }H_{2} \\
\text{Ts} &\text{: }&H_{1}\oplus H_{2}=V\Longleftrightarrow B=B_{1}\cup B_{2}%
\text{ \`{e} una base di }V
\end{eqnarray*}

Nel caso in cui $H_{1}\oplus H_{2}=V$, se $B_{1}$ \`{e} una base di $H_{1}$
e $B_{2}$ \`{e} una base di $H_{2}$, $B=B_{1}\cup B_{2}$ \`{e} una base di $%
V $. Infatti ogni vettore di $V$ si scrive in modo unico come somma di un
vettore di $H_{1}$ e un vettore di $H_{2}$: i vettori di $B_{1}$ e i vettori
di $B_{2}$ sono linearmente indipendenti, perch\'{e} $H_{1}\cap
H_{2}=\left\{ \mathbf{0}\right\} $.

In generale, se $V=A\oplus B=A\oplus C$, non si pu\`{o} concludere che $B=C$%
: infatti, $B_{B}$ e $B_{C}$ forniscono entrambi un completamento di $B_{A}$
a una base di $V$, ma non necessariamente generano lo stesso sottospazio. E.
g., $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$ \`{e} somma diretta di un piano e di una qualsiasi retta non
appartenente al piano, ma ci sono infinite rette distinte non appartenenti
al piano.

\subsection{Applicazioni lineari}

Come sono fatte le funzioni che si "comportano bene" rispetto alle propriet%
\`{a} caratterizzanti uno spazio vettoriale?

\textbf{Def} Siano $V$ e $W$ spazi vettoriali su un campo $\mathbf{K}$. Una
funzione $L:V\rightarrow W$ si dice applicazione lineare se

(i) ha la propriet\`{a} di additivit\`{a}, i. e. $\forall $ $\mathbf{v}_{1}%
\mathbf{,v}_{2}\in V$, $L\left( \mathbf{v}_{1}\mathbf{+v}_{2}\right)
=L\left( \mathbf{v}_{1}\right) +L\left( \mathbf{v}_{2}\right) $;

(ii) ha la propriet\`{a} di omogeneit\`{a}, i. e. $\forall $ $\mathbf{v}\in V
$, $\forall $ $\lambda \in \mathbf{K}$, $L\left( \lambda \mathbf{v}\right)
=\lambda L\left( \mathbf{v}\right) $.

$L$ quindi "trasporta" la somma da $V$ in $W$ e il prodotto per scalare da $%
V $ in $W$.

\begin{enumerate}
\item $V=\mathbf{K}^{n}$, $W=\mathbf{K}^{m}$. Data $A\in M_{\mathbf{K}%
}\left( m,n\right) $, si definisce $\tciLaplace _{A}:\mathbf{K}%
^{n}\rightarrow \mathbf{K}^{m}$ la funzione che a ogni vettore colonna $%
\mathbf{v}\in \mathbf{K}^{n}$ associa il vettore colonna di $\mathbf{K}^{m}$
che si ottiene calcolando $A\mathbf{v}$. $\tciLaplace _{A}$ \`{e}
un'applicazione lineare, infatti $\tciLaplace _{A}\left( \mathbf{v}_{1}%
\mathbf{+v}_{2}\right) =A\left( \mathbf{v}_{1}\mathbf{+v}_{2}\right) =A%
\mathbf{v}_{1}+A\mathbf{v}_{2}=\tciLaplace _{A}\left( \mathbf{v}_{1}\right)
+\tciLaplace _{A}\left( \mathbf{v}_{2}\right) $, per la propriet\`{a}
distributiva a destra del prodotto matriciale, e $\tciLaplace _{A}\left(
\lambda \mathbf{v}\right) =A\left( \lambda \mathbf{v}\right) =\lambda \left(
A\mathbf{v}\right) =\lambda \tciLaplace _{A}\left( \mathbf{v}\right) $.

Date le coordinate di $\mathbf{x}$, una funzione \`{e} del tipo $\tciLaplace
_{A}$ se le coordinate del vettore trasformato sono polinomi omogenei di
primo grado nelle coordinate di $\mathbf{x}$.

\item $V=M_{\mathbf{K}}\left( m,n\right) $, $W=M_{\mathbf{K}}\left(
n,m\right) $. Si definisce la funzione $Trasposizione:V\rightarrow W$ che a
ogni matrice $A=\left[ 
\begin{array}{c}
R_{1}\left( A\right) \\ 
... \\ 
R_{m}\left( A\right)%
\end{array}%
\right] $ associa la matrice trasposta $A^{T}=\left[ 
\begin{array}{ccc}
R_{1}\left( A\right) | & ... & |R_{m}\left( A\right)%
\end{array}%
\right] $, dove la prima colonna \`{e} la prima riga di $A$ e cos\`{\i} via.
E. g. se $A=\left[ 
\begin{array}{ccc}
1 & 2 & 3 \\ 
4 & 5 & 6%
\end{array}%
\right] $, $A^{T}=\left[ 
\begin{array}{cc}
1 & 4 \\ 
2 & 5 \\ 
3 & 6%
\end{array}%
\right] $. La trasposizione \`{e} un'applicazione lineare.
\end{enumerate}

$L$ \`{e} un'applicazione lineare se e solo se $\forall $ $\mathbf{v}_{1}%
\mathbf{,v}_{2}\in V$, $\forall $ $\lambda _{1},\lambda _{2}\in \mathbf{K}$ $%
L\left( \lambda _{1}\mathbf{v}_{1}\mathbf{+}\lambda _{2}\mathbf{v}%
_{2}\right) =\lambda _{1}L\left( \mathbf{v}_{1}\right) +\lambda _{2}L\left( 
\mathbf{v}_{2}\right) $: si incorporano le due propriet\`{a} in una sola,
che pu\`{o} essere ricondotta ad esse considerando rispettivamente $\lambda
_{1}=\lambda _{2}=1$ e $\lambda _{2}=0$.

Le applicazioni lineari si "comportano bene" rispetto a composizione e
inversione.

\textbf{Proposizione}%
\begin{eqnarray*}
\text{Hp} &\text{: }&V,W,U\text{ sono spazi vettoriali su }\mathbf{K}\text{; 
}L:V\rightarrow W,M:W\rightarrow U\text{ sono due applicazioni lineari} \\
\text{Ts} &\text{: }&M\circ L:V\rightarrow U\text{ \`{e} un'applicazione
lineare}
\end{eqnarray*}

\textbf{Dim} Uso il criterio visto. $\forall $ $\mathbf{v}_{1}\mathbf{,v}%
_{2}\in V$, $\forall $ $\lambda _{1},\lambda _{2}\in \mathbf{K}$, $M\circ
L\left( \lambda _{1}\mathbf{v}_{1}\mathbf{+}\lambda _{2}\mathbf{v}%
_{2}\right) =M\left( L\left( \lambda _{1}\mathbf{v}_{1}\mathbf{+}\lambda _{2}%
\mathbf{v}_{2}\right) \right) =M\left( \lambda _{1}L\left( \mathbf{v}%
_{1}\right) \mathbf{+}\lambda _{2}L\left( \mathbf{v}_{2}\right) \right) $
perch\'{e} $L$ \`{e} un'applicazione lineare. Inoltre $M\left( \lambda
_{1}L\left( \mathbf{v}_{1}\right) \mathbf{+}\lambda _{2}L\left( \mathbf{v}%
_{2}\right) \right) =\lambda _{1}M\left( L\left( \mathbf{v}_{1}\right)
\right) \mathbf{+}\lambda _{2}M\left( L\left( \mathbf{v}_{2}\right) \right) $
perch\'{e} $M$ \`{e} un'applicazione lineare e si ottiene quindi $\lambda
_{1}M\circ L\left( \mathbf{v}_{1}\right) \mathbf{+}\lambda _{2}M\circ
L\left( \mathbf{v}_{2}\right) $. $\blacksquare $

\begin{enumerate}
\item $V=\mathbf{K}^{n}$, $W=\mathbf{K}^{m}$, $U=\mathbf{K}^{p}$. Considero $%
A\in M_{\mathbf{K}}\left( m,n\right) $, $B\in M_{\mathbf{K}}\left(
p,m\right) $. $\left( \tciLaplace _{B}\circ \tciLaplace _{A}\right) \left( 
\mathbf{v}\right) =\tciLaplace _{B}\left( \tciLaplace _{A}\left( \mathbf{v}%
\right) \right) =\tciLaplace _{B}\left( A\mathbf{v}\right) =B\left( A\mathbf{%
v}\right) =\left( BA\right) \mathbf{v}$. In generale la composizione di
applicazioni lineari del tipo $\left( \tciLaplace _{B}\circ \tciLaplace
_{A}\right) \left( \mathbf{v}\right) $ cattura il prodotto matriciale.
\end{enumerate}

\textbf{Proposizione}%
\begin{eqnarray*}
\text{Hp} &\text{: }&V,W\text{ sono spazi vettoriali su }\mathbf{K}\text{; }%
L:V\rightarrow W\text{ \`{e} un'applicazione lineare invertibile} \\
\text{Ts} &\text{: }&L^{-1}:W\rightarrow V\text{ \`{e} un'applicazione
lineare}
\end{eqnarray*}

Nelle ipotesi un'applicazione lineare \`{e} considerata invertibile se lo 
\`{e} in quanto funzione, quindi se \`{e} iniettiva e suriettiva.

\textbf{Dim} Devo mostrare che $\forall $ $\mathbf{w}_{1}\mathbf{,w}_{2}\in
W $, $\forall $ $\lambda _{1},\lambda _{2}\in \mathbf{K}$, $L^{-1}\left(
\lambda _{1}\mathbf{w}_{1}+\lambda _{2}\mathbf{w}_{2}\right) =\lambda
_{1}L^{-1}\left( \mathbf{w}_{1}\right) +\lambda _{2}L^{-1}\left( \mathbf{w}%
_{2}\right) $. Per ipotesi di invertibilit\`{a} esistono e sono unici $%
\mathbf{v}_{1}\mathbf{,v}_{2}\in V:L\left( \mathbf{v}_{1}\right) =\mathbf{w}%
_{1}$ e $L\left( \mathbf{v}_{2}\right) =\mathbf{w}_{2}$: $\mathbf{v}%
_{1}=L^{-1}\left( \mathbf{w}_{1}\right) $, $\mathbf{v}_{2}=L^{-1}\left( 
\mathbf{w}_{2}\right) $. Allora, usando il criterio nel verso meno consueto, 
$L^{-1}\left( \lambda _{1}L\left( \mathbf{v}_{1}\right) +\lambda _{2}L\left( 
\mathbf{v}_{2}\right) \right) =L^{-1}\left( L\left( \lambda _{1}\mathbf{v}%
_{1}+\lambda _{2}\mathbf{v}_{2}\right) \right) =\lambda _{1}\mathbf{v}%
_{1}+\lambda _{2}\mathbf{v}_{2}=\lambda _{1}L^{-1}\left( \mathbf{w}%
_{1}\right) +\lambda _{2}L^{-1}\left( \mathbf{w}_{2}\right) $. $\blacksquare 
$

\begin{enumerate}
\item $V=\mathbf{K}^{n}$, $W=\mathbf{K}^{m}$. Considero $A\in M_{\mathbf{K}%
}\left( m,n\right) $, $\tciLaplace _{A}:\mathbf{K}^{n}\rightarrow \mathbf{K}%
^{m}$. Quando esiste $\tciLaplace _{A}^{-1}$? Voglio $\tciLaplace _{A}\circ
\tciLaplace _{A}^{-1}=id_{\mathbf{K}^{m}}$ e $\tciLaplace _{A}^{-1}\circ
\tciLaplace _{A}=id_{\mathbf{K}^{n}}$. Se $m=n$ e $A$ \`{e} invertibile,
allora $\tciLaplace _{A}^{-1}=\tciLaplace _{A^{-1}}$. Infatti $\tciLaplace
_{A^{-1}}\left( \tciLaplace _{A}\left( \mathbf{v}\right) \right)
=\tciLaplace _{A^{-1}}\left( A\mathbf{v}\right) =A^{-1}\left( A\mathbf{v}%
\right) =Id_{n}\mathbf{v=v}$ e analogamente $\tciLaplace _{A}^{-1}\left(
\tciLaplace _{A^{-1}}\left( \mathbf{v}\right) \right) =\mathbf{v}$. Quindi $%
\tciLaplace _{A}^{-1}=\tciLaplace _{A^{-1}}$ e $\tciLaplace _{A}^{-1}$
esiste solo quando $m=n$ e $A$ \`{e} invertibile.
\end{enumerate}

Si fanno altri esempi generali di applicazioni lineari.

\begin{enumerate}
\item $V=C^{1}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $, $W=C^{0}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $, $L:V\rightarrow W$ che associa ad ogni funzione di $V$ la sua
derivata.

\item $V=C^{0}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $, $W=C^{1}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $, $\Sigma :V\rightarrow W$ che associa ad ogni funzione $g\left(
x\right) $ di $V$ la sua funzione integrale $\int_{0}^{x}g\left( t\right) dt$%
.

\item $V=C^{k}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $, $W=C^{0}\left( 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\right) $, $L:V\rightarrow W$ che associa ad ogni funzione di $V$ una
combinazione lineare delle sue derivate: $L\left( y\right) =y^{\left(
k\right) }+a_{1}y^{\left( k-1\right) }+...+a_{k-1}y^{\prime }+a_{k}y$, con $%
a_{i}\in \mathbf{K}$.
\end{enumerate}

Ci sono delle conseguenze notevoli delle propriet\`{a} della applicazioni
lineari.

\begin{description}
\item[-] $L\left( \mathbf{v}_{1}\mathbf{-v}_{2}\right) =L\left( \mathbf{v}%
_{1}+\left( -1\right) \mathbf{v}_{2}\right) =L\left( \mathbf{v}_{1}\right)
+L\left( \left( -1\right) \mathbf{v}_{2}\right) =L\left( \mathbf{v}%
_{1}\right) -L\left( \mathbf{v}_{2}\right) $ perch\'{e} $L$ \`{e}
un'applicazione lineare.

\item[-] $L\left( \mathbf{0}_{V}\right) =L\left( \mathbf{v-v}\right)
=L\left( \mathbf{v}\right) -L\left( \mathbf{v}\right) =\mathbf{0}_{W}$. Che $%
L\left( \mathbf{0}_{V}\right) =\mathbf{0}_{W}$ \`{e} necessario affinch\'{e} 
$L$ sia un'applicazione lineare: se $L\left( \mathbf{0}_{V}\right) \neq 
\mathbf{0}_{W}$, $L$ non \`{e} un'applicazione lineare.
\end{description}

% \begin{enumerate}
% \item $f:%
% %TCIMACRO{\U{211d} }%
% %BeginExpansion
% \mathbb{R}
% %EndExpansion
% \rightarrow 
% %TCIMACRO{\U{211d} }%
% %BeginExpansion
% \mathbb{R}
% %EndExpansion
% $, $f\left( x\right) =x+1$ non \`{e} un'applicazione lineare perch\'{e} $%
% f\left( 0\right) =1\neq 0$. Le applicazioni lineari non coincidono con le
% funzioni lineari (i polinomi di grado 1). In questo caso $f$ \`{e} composta
% da una parte lineare e una parte costante, ed \`{e} una trasformazione
% lineare affine (?).
% \end{enumerate}

Come si comporta un'applicazione lineare rispetto ai sottospazi di dominio e
codominio?

\textbf{Proposizione}%
\begin{eqnarray*}
\text{(i) Hp} &\text{: }&L:V\rightarrow W\text{ \`{e} un'applicazione
lineare; }H\subset V\text{ \`{e} un sottospazio vettoriale} \\
\text{Ts} &\text{: }&L\left( H\right) =\left\{ \mathbf{w}\in W:\exists \text{
}\mathbf{v}\in H:L\left( \mathbf{v}\right) =\mathbf{w}\right\} \text{ \`{e}
un sottospazio vettoriale di }W \\
\text{(ii) Hp} &\text{: }&L:V\rightarrow W\text{ \`{e} un'applicazione
lineare; }K\subset W\text{ \`{e} un sottospazio vettoriale} \\
\text{Ts} &\text{: }&L^{-1}\left( K\right) =\left\{ \mathbf{v}\in V:L\left( 
\mathbf{v}\right) \in \mathbf{K}\right\} \text{ \`{e} un sottospazio
vettoriale di }V
\end{eqnarray*}

Quindi un'applicazione lineare preserva le propriet\`{a} dello spazio
vettoriale. $L^{-1}\left( K\right) $ denota un insieme di vettori, non ha
niente a che fare con l'applicazione lineare inversa.

\textbf{Dim} (i) Devo verificare che $L\left( H\right) $ \`{e} chiuso
rispetto alle operazioni di somma e prodotto per scalare. Dati $\mathbf{w}%
_{1}\mathbf{,w}_{2}\in L\left( H\right) $, esistono $\mathbf{v}_{1}\mathbf{,v%
}_{2}\in H:L\left( \mathbf{v}_{1}\right) =\mathbf{w}_{1}$ e $L\left( \mathbf{%
v}_{2}\right) =\mathbf{w}_{2}$. Ma $\mathbf{w}_{1}\mathbf{+w}_{2}=L\left( 
\mathbf{v}_{1})\mathbf{+}L(\mathbf{v}_{2}\right) =L\left( \mathbf{v}_{1}%
\mathbf{+v}_{2}\right) $. Poich\'{e} $H$ \`{e} un sottospazio, $\mathbf{v}%
_{1}\mathbf{+v}_{2}\in H$, dunque $L\left( \mathbf{v}_{1}\mathbf{+v}%
_{2}\right) \in L\left( H\right) $. Dato $\mathbf{w}\in L\left( H\right) $, $%
\exists $ $\mathbf{v}\in H:L\left( \mathbf{v}\right) =\mathbf{w}$. Allora $%
\lambda \mathbf{w=}\lambda L\left( \mathbf{v}\right) =L\left( \lambda 
\mathbf{v}\right) $: poich\'{e} $H$ \`{e} un sottospazio, $\lambda \mathbf{v}%
\in H$ e $\lambda \mathbf{w}\in L\left( H\right) $.

(ii) Devo verificare che $L^{-1}\left( K\right) $ \`{e} chiuso rispetto alle
operazioni di somma e prodotto per scalare. Dati $\mathbf{v}_{1}\mathbf{,v}%
_{2}\in L^{-1}\left( K\right) $, $L\left( \mathbf{v}_{1}\right) \in K$ e $%
L\left( \mathbf{v}_{2}\right) \in K$. Ma $L\left( \mathbf{v}_{1})\mathbf{+}L(%
\mathbf{v}_{2}\right) =L\left( \mathbf{v}_{1}\mathbf{+v}_{2}\right) $, poich%
\'{e} $K$ \`{e} un sottospazio, appartiene a $K$, dunque $\mathbf{v}_{1}%
\mathbf{+v}_{2}\in L^{-1}\left( K\right) $. Dato $\mathbf{v}\in L^{-1}\left(
K\right) $, $L\left( \mathbf{v}\right) \in K$. Allora $\lambda L\left( 
\mathbf{v}\right) =L\left( \lambda \mathbf{v}\right) $, poich\'{e} $K$ \`{e}
un sottospazio, appartiene a $K$, dunque $\lambda \mathbf{v}\in L^{-1}\left(
K\right) $. $\blacksquare $

\begin{enumerate}
\item Considero $A=\left[ 
\begin{array}{ccc}
1 & 2 & 3 \\ 
4 & 5 & 6%
\end{array}%
\right] $, $\tciLaplace _{A}:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$. Considero $H=Span\left( \left[ 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right] ,\left[ 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right] \right) \subset 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$. Cos'\`{e} $\tciLaplace _{A}\left( H\right) $? Un generico elemento di 
$H$ \`{e} $\mathbf{v}=\alpha \left[ 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right] +\beta \left[ 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right] $. Un generico elemento di $\tciLaplace _{A}\left( H\right) $ \`{e} $%
\tciLaplace _{A}\left( \mathbf{v}\right) =\tciLaplace _{A}\left( \alpha %
\left[ 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right] +\beta \left[ 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right] \right) =\alpha \tciLaplace _{A}\left( \left[ 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right] \right) +\beta \tciLaplace _{A}\left( \left[ 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right] \right) =$ $\alpha A\left[ 
\begin{array}{c}
1 \\ 
0 \\ 
0%
\end{array}%
\right] +\beta A\left[ 
\begin{array}{c}
0 \\ 
1 \\ 
1%
\end{array}%
\right] =\alpha \left[ 
\begin{array}{c}
1 \\ 
4%
\end{array}%
\right] +\beta \left[ 
\begin{array}{c}
5 \\ 
11%
\end{array}%
\right] $. Dunque $\tciLaplace _{A}\left( H\right) =Span\left( \left[ 
\begin{array}{c}
1 \\ 
4%
\end{array}%
\right] ,\left[ 
\begin{array}{c}
5 \\ 
11%
\end{array}%
\right] \right) =%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$, perch\'{e} tali vettori sono due e sono linearmente indipendenti.

Considero $K=Span\left( \left[ 
\begin{array}{c}
0 \\ 
1%
\end{array}%
\right] \right) \subset 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$. $\tciLaplace _{A}^{-1}\left( K\right) =\left\{ \mathbf{x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{3}:A\mathbf{x}=\lambda \left[ 
\begin{array}{c}
0 \\ 
1%
\end{array}%
\right] \right\} =\left\{ \mathbf{x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{3}:A\mathbf{x}=\left[ 
\begin{array}{c}
0 \\ 
\lambda%
\end{array}%
\right] \right\} $ e risolvo il sistema lineare $\left[ 
\begin{array}{cccc}
1 & 2 & 3 & 0 \\ 
4 & 5 & 6 & \lambda%
\end{array}%
\right] $: la soluzione \`{e} un'espressione generica dell'elemento del
sottospazio.

\item Data $L:V\rightarrow W$ e $Z$ sottospazio di $W$, qual \`{e} $\dim
\left( L^{-1}\left( Z\right) \right) $? Non si pu\`{o} dire molto in
generale, perch\'{e}, anche conoscendo $Z$, $\dim \left( L^{-1}\left(
Z\right) \right) $ dipende dall'intersezione tra $\func{Im}\left( L\right) $
e $Z$. Se infatti l'unica intersezione tra $\func{Im}\left( L\right) $ e $Z$ 
\`{e} il vettore nullo (l'intersezione non pu\`{o} essere vuota perch\'{e}
sono entrambi sottospazi), allora $L^{-1}\left( Z\right) =\ker \left(
L\right) $ e $\dim \left( L^{-1}\left( Z\right) \right) =\dim \left(
V\right) -\dim \left( \func{Im}\left( L\right) \right) $. $\dim \left(
Z\right) $ pu\`{o} essere qualsiasi.

\item Se $L=A\mathbf{x}$ e cerco $L^{-1}\left( V\right) $, posso scrivere le
equazioni cartesiane di $V$, rappresentate da una matrice $B$: allora $%
\mathbf{y}\in V\Longleftrightarrow B\mathbf{y=0}$. Si ha quindi che $\mathbf{%
x}\in L^{-1}\left( V\right) \Longleftrightarrow L\left( \mathbf{x}\right)
\in V\Longleftrightarrow A\mathbf{x}\in V\Longleftrightarrow B\left( A%
\mathbf{x}\right) =0$. Trovare $L^{-1}\left( V\right) $ significa quindi
trovare $\ker \left( BA\right) $.
\end{enumerate}

Ci sono poi alcuni casi notevoli generali.

Se $H=V$ invece che $H\subset V$, $L\left( H\right) =L\left( V\right) =\func{%
Im}L$.

\textbf{Def} Si definisce $\func{Im}L=L\left( V\right) $ l'immagine
dell'applicazione lineare $L$.

Se $L$ \`{e} $\tciLaplace _{A}$, $\func{Im}L=\func{col}\left( A\right) $.
Quindi, essendo $\dim \left( \func{col}\left( A\right) \right) =r\left(
A\right) $, si ha $\dim \left( \func{col}\left( A\right) \right) \leq \min
\left\{ m,n\right\} $, in particolare $\dim \left( \func{col}\left( A\right)
\right) \leq n$, che \`{e} la dimensione del dominio.

Considerato l'insieme $\left\{ \mathbf{0}_{W}\right\} \subseteq W$, $%
L^{-1}\left( \left\{ \mathbf{0}_{W}\right\} \right) =L^{-1}\left( \mathbf{0}%
_{W}\right) =\ker L$.

\textbf{Def} Si definisce nucleo dell'applicazione lineare $L$ l'insieme $%
\ker L=L^{-1}\left( \left\{ \mathbf{0}_{W}\right\} \right) $.

Data $\tciLaplace _{A}:\mathbf{K}^{n}\mathbf{\rightarrow K}^{m}$, $A\in M_{%
\mathbf{K}}\left( m,n\right) $, $\ker \tciLaplace _{A}=\tciLaplace
_{A}^{-1}\left( \left\{ \mathbf{0}_{W}\right\} \right) =\left\{ \mathbf{x\in
K}^{n}:\tciLaplace _{A}\left( \mathbf{x}\right) =\mathbf{0}_{W}\right\}
=\left\{ \mathbf{x\in K}^{n}:A\mathbf{x}=\mathbf{0}\right\} =\ker A$.

In generale $\ker \tciLaplace _{A}=\ker A$.

Se $V,W$ sono spazi vettoriali su $\mathbf{K}$ e $L:V\rightarrow W$ \`{e}
un'applicazione lineare, dato $\mathbf{w}\in W$, $L^{-1}\left( \mathbf{w}%
\right) =\left\{ \mathbf{v}\in V:L\left( \mathbf{v}\right) =\mathbf{w}%
\right\} $. Nel caso di $\tciLaplace _{A}:\mathbf{K}^{n}\mathbf{\rightarrow K%
}^{m}$, $\tciLaplace _{A}\left( \mathbf{x}\right) =A\mathbf{x}$ con $A\in M_{%
\mathbf{K}}\left( m,n\right) $, $\mathbf{b\in K}^{m}$, $\tciLaplace
_{A}^{-1}\left( \mathbf{b}\right) =\left\{ \mathbf{v}\in \mathbf{K}%
^{n}:L\left( \mathbf{v}\right) =\mathbf{b}\right\} =\left\{ \mathbf{v}\in 
\mathbf{K}^{n}:A\mathbf{v}=\mathbf{b}\right\} $: trovare la controimmagine
di $\mathbf{b}$ attraverso $\tciLaplace _{A}$ significa risolvere il sistema
lineare $A\mathbf{v}=\mathbf{b}$.

\textbf{Teorema di fibra}%
\begin{eqnarray*}
\text{Hp}\text{: } &&L:V\rightarrow W\text{ \`{e} un'applicazione lineare, }%
\mathbf{w}\in W\text{, }\mathbf{v}_{0}\in L^{-1}\left( \mathbf{w}\right) \\
\text{Ts}\text{: } &&L^{-1}\left( \mathbf{w}\right) =\left\{ \mathbf{v}_{0}%
\mathbf{+v}_{h}\text{ al variare di }\mathbf{v}_{h}\in \ker \left( L\right)
\right\}
\end{eqnarray*}

cio\`{e} tutte e sole le controimmagini di $\mathbf{w}$ si ottengono
sommando a $\mathbf{v}_{0}$ un elemento del nucleo.

\textbf{Dim} E' analoga a quella del teorema di struttura. Mostro che se $%
\mathbf{v}$ \`{e} tale che $L\left( \mathbf{v}\right) =\mathbf{w}$, allora $%
\mathbf{v-v}_{0}\in \ker \left( L\right) $: infatti $L\left( \mathbf{v-v}%
_{0}\right) =L\left( \mathbf{v}\right) -L\left( \mathbf{v}_{0}\right) =%
\mathbf{w-w=0}$. Mostro che $\mathbf{v}_{0}\mathbf{+v}_{h}$ \`{e} tale che $%
L\left( \mathbf{v}_{0}\mathbf{+v}_{h}\right) =\mathbf{w}$: infatti $L\left( 
\mathbf{v}_{0}\mathbf{+v}_{h}\right) =L\left( \mathbf{v}_{0}\right) +L\left( 
\mathbf{v}_{h}\right) =\mathbf{w+0=w}$. $\blacksquare $

$L^{-1}\left( \mathbf{w}\right) $ si dice fibra di $L$ sopra $\mathbf{w}$.

\begin{enumerate}
\item Dato $F^{1}$ insieme delle funzioni derivabili su $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, $D:\tciFourier ^{1}\rightarrow \tciFourier $ \`{e} $D\left( f\right)
=f^{\prime }$. Allora, se $D\left( f\right) =g$, $D^{-1}\left( g\right) =G+c$%
, dove $G$ \`{e} una soluzione particolare (cio\`{e} \`{e} una funzione tale
che $D\left( G\right) =g$) e $c$ \`{e} il generico elemento di $\ker \left(
D\right) $, cio\`{e} l'insieme delle funzioni che hanno derivata nulla
(quindi le funzioni costanti).
\end{enumerate}

\subsubsection{Iniettivit\`{a}, suriettivit\`{a} e invertibilit\`{a}}

Una funzione (quindi anche un'applicazione lineare) \`{e} invertibile se e
solo se \`{e} iniettiva e suriettiva.

(i) \emph{Suriettivit\`{a}} Un'applicazione lineare $L:V\rightarrow W$ \`{e}
suriettiva se $\forall $ $\mathbf{w}\in W$ $\exists $ $\mathbf{v}\in
V:L\left( \mathbf{v}\right) \mathbf{=w}$, cio\`{e} se $\forall $ $\mathbf{w}%
\in W$ $L^{-1}\left( \mathbf{w}\right) \neq \mathbf{\varnothing }$, cio\`{e}
posso risolvere il sistema lineare $A\mathbf{x=b}$ $\forall $ $\mathbf{b}\in 
\mathbf{K}^{m}$. Questo significa che $\func{Im}\left( L\right) =W$.

\textbf{Proposizione}%
\begin{eqnarray*}
\text{Hp}\text{: } &&L:V\rightarrow W\text{ \`{e} un'applicazione lineare; }%
\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} \text{ \`{e} un insieme di
generatori di }V \\
\text{Ts}\text{: } &&\left\{ L\left( \mathbf{v}_{1}\right) ,...,L\left( 
\mathbf{v}_{n}\right) \right\} \text{ \`{e} un insieme di generatori di }%
\func{Im}L\text{, i. e. }Span\left\{ L\left( \mathbf{v}_{1}\right)
,...,L\left( \mathbf{v}_{n}\right) \right\} =\func{Im}L
\end{eqnarray*}

\textbf{Dim} $\forall $ $\mathbf{w}\in \func{Im}\left( L\right) $, $\exists $
$\mathbf{v}\in V:L\left( \mathbf{v}\right) =\mathbf{w}$. Per ipotesi $%
V=Span\left( \mathbf{v}_{1}\mathbf{,...,v}_{n}\right) $ e in particolare $%
\mathbf{v}=\lambda _{1}\mathbf{v}_{1}+....+\lambda _{n}\mathbf{v}_{n}$.
Applico $L$ a entrambi i lati: ottengo $L\left( \lambda _{1}\mathbf{v}%
_{1}+....+\lambda _{n}\mathbf{v}_{n}\right) =\lambda _{1}L\left( \mathbf{v}%
_{1}\right) +....+\lambda _{n}L\left( \mathbf{v}_{n}\right) =\mathbf{w}$,
perch\'{e} $L$ \`{e} un'applicazione lineare. Dunque $\mathbf{w}\in
Span\left\{ L\left( \mathbf{v}_{1}\right) ,...,L\left( \mathbf{v}_{n}\right)
\right\} $, cio\`{e} ogni elemento dell'immagine pu\`{o} essere scritto come
combinazione lineare di $L\left( \mathbf{v}_{1}\right) ,...,L\left( \mathbf{v%
}_{n}\right) $, e $\func{Im}L\subseteq Span\left\{ L\left( \mathbf{v}%
_{1}\right) ,...,L\left( \mathbf{v}_{n}\right) \right\} $. Va dimostrata
anche l'inclusione opposta, che \`{e} ovvia perch\'{e} lo span di un insieme
di vettori \`{e} un sottospazio. $\blacksquare $

Ci sono alcune conseguenze:

\begin{description}
\item[-] $L:V\rightarrow W$ \`{e} un'applicazione lineare suriettiva se e
solo $\func{Im}\left( L\right) =W$, cio\`{e}, applicando la proposizione, se
l'immagine di un insieme di generatori di $V$ \`{e} un insieme di generatori
di $W$;

\item[-] la suriettivit\`{a} conserva la propriet\`{a} di essere generatori
di tutto lo spazio
\end{description}

\begin{enumerate}
\item Considero $\tciLaplace _{A}:\mathbf{K}^{n}\mathbf{\rightarrow K}^{m}$,
con $A\in M_{\mathbf{K}}\left( m,n\right) $. $\tciLaplace _{A}$ \`{e}
suriettiva se e solo se $A\mathbf{x=b}$ \`{e} risolubile per ogni $\mathbf{%
b\in K}^{n}$. Per il teorema di Rouch\'{e}-Capelli questo \`{e} possibile se
e solo se $r\left( A\right) =m$ (infatti, per il teorema di nullit\`{a} pi%
\`{u} rango, $\dim \left( \func{Im}\left( \tciLaplace _{A}\right) \right) =r$
dev'essere $m$ affinch\'{e} sia suriettiva): in tal caso si ha per ogni $%
\mathbf{b}$ $r\left( A\right) =r\left( A|\mathbf{b}\right) $. Poich\'{e} $%
r\left( A\right) \leq \min \left\{ m,n\right\} $, affinch\'{e} $\tciLaplace
_{A}$ sia suriettiva occorre $m\leq n$. Se $n<m$ $\tciLaplace _{A}$ non \`{e}
suriettiva. Infatti, per nullit\`{a} pi\`{u} rango, $\dim \left( \func{Im}%
\left( \tciLaplace _{A}\right) \right) =n-\dim \left( \ker \left(
\tciLaplace _{A}\right) \right) \leq n$: se $m>n$ $\dim \left( \func{Im}%
\left( \tciLaplace _{A}\right) \right) $ non pu\`{o} essere $m$.

Prendendo la base canonica di $\mathbf{K}^{n}$ $B=\left\{ \mathbf{e}^{1}%
\mathbf{,...,e}^{n}\right\} $, si ha che $B$ \`{e} un insieme di generatori
di $\mathbf{K}^{n}$, quindi si applica la proposizione e $\func{Im}\left(
\tciLaplace _{A}\right) =Span\left\{ \tciLaplace _{A}\left( \mathbf{e}%
^{1}\right) ,...,\tciLaplace _{A}\left( \mathbf{e}^{n}\right) \right\}
=Span\left\{ C_{1}\left( A\right) ,...,C_{n}\left( A\right) \right\} =\func{%
col}\left( A\right) $. $\tciLaplace _{A}$ \`{e} suriettiva se e solo se $%
\func{col}\left( A\right) =\mathbf{K}^{m}$, cio\`{e} le colonne di $A$
generano $\mathbf{K}^{m}$.
\end{enumerate}

$L$, descritta dalla matrice $A$, \`{e} suriettiva se e solo se \`{e}
invertibile a destra. Infatti, se \`{e} invertibile a destra, $\exists $ $%
B:AB=Id_{m}$: questo significa che $\left[ 
\begin{array}{c}
R_{1}\left( A\right) \\ 
... \\ 
R_{m}\left( A\right)%
\end{array}%
\right] B=\left[ 
\begin{array}{c}
R_{1}\left( A\right) B \\ 
... \\ 
R_{m}\left( A\right) B%
\end{array}%
\right] $. Dal momento che le $m$ righe del risultato sono linearmente
indipendenti, anche le righe di $A$ sono indipendenti e $r\left( A\right) =m$%
.

(ii) \emph{Iniettivit\`{a}} Un'applicazione lineare $L:V\rightarrow W$ \`{e}
iniettiva se $L\left( \mathbf{v}_{1}\right) =L\left( \mathbf{v}_{2}\right) $
implica $\mathbf{v}_{1}\mathbf{=v}_{2}$, o equivalentemente se per ogni $%
\mathbf{v}_{1}\mathbf{,v}_{2}\in \mathbf{K}^{n}$, $\mathbf{v}_{1}\mathbf{%
\neq v}_{2}$, si ha $L\left( \mathbf{v}_{1}\right) \neq L\left( \mathbf{v}%
_{2}\right) $.

\textbf{Teorema: criterio di iniettivit\`{a}}%
\begin{eqnarray*}
\text{Hp}\text{: } &&L:V\rightarrow W\text{ \`{e} un'applicazione lineare} \\
\text{Ts}\text{: } &&L\text{ \`{e} iniettiva }\Longleftrightarrow \text{ }%
\ker \left( L\right) =\left\{ 0_{V}\right\}
\end{eqnarray*}

\textbf{Dim} Mostro che $L$ iniettiva implica $\ker \left( L\right) =\left\{
0_{V}\right\} $. Poich\'{e} $\ker \left( L\right) $ \`{e} un sottospazio
vettoriale, $0_{V}\in \ker \left( L\right) $. Ma $L$ \`{e} iniettiva, quindi 
$L^{-1}\left( \left\{ 0_{W}\right\} \right) $ contiene un solo elemento, che 
\`{e} pertanto $0_{V}$, e $\ker \left( L\right) =\left\{ 0_{V}\right\} $.

Mostro che $\ker \left( L\right) =\left\{ 0_{V}\right\} $ implica $L$
iniettiva: uso la prima definizione di iniettivit\`{a}. Considero $\mathbf{v}%
_{1}\mathbf{,v}_{2}\in V:L\left( \mathbf{v}_{1}\right) =L\left( \mathbf{v}%
_{2}\right) $. Equivalentemente $L\left( \mathbf{v}_{1}\right) -L\left( 
\mathbf{v}_{2}\right) =\mathbf{0}\Longleftrightarrow L\left( \mathbf{v}_{1}%
\mathbf{-v}_{2}\right) =\mathbf{0}$ perch\'{e} $L$ \`{e} lineare, ma allora $%
\mathbf{v}_{1}\mathbf{-v}_{2}\in \ker \left( L\right) $, che per\`{o}
contiene solo $\left\{ 0_{V}\right\} $ per ipotesi, dunque $\mathbf{v}_{1}%
\mathbf{-v}_{2}\mathbf{=0\Longleftrightarrow v}_{1}=\mathbf{v}_{2}$. $%
\blacksquare $

\begin{enumerate}
\item Considero $\tciLaplace _{A}:\mathbf{K}^{n}\mathbf{\rightarrow K}^{m}$,
con $A\in M_{\mathbf{K}}\left( m,n\right) $. $\tciLaplace _{A}$ \`{e}
iniettiva se e solo se $\ker \left( L\right) =\left\{ 0_{V}\right\} $. Per
il teorema di Rouch\'{e}-Capelli questo \`{e} possibile se e solo se $%
r\left( A\right) =n$ (il sistema omogeneo ha un'unica soluzione). Poich\'{e} 
$r\left( A\right) \leq \min \left\{ m,n\right\} $, affinch\'{e} $\tciLaplace
_{A}$ sia iniettiva occorre $n\leq m$. Se $m<n$ $\tciLaplace _{A}$ non \`{e}
iniettiva.
\end{enumerate}

Riepilogando, affinch\'{e} $\tciLaplace _{A}$ sia invertibile occorre che
sia suriettiva e iniettiva, per le quali propriet\`{a} \`{e} necessario
rispettivamente $m\leq n$ e $n\leq m$, quindi $m=n$; se $m\neq n$ $%
\tciLaplace _{A}$ non \`{e} invertibile.

\begin{enumerate}
\item Se $\dim V=\dim W=n$, $L$ \`{e} iniettiva se e solo se \`{e}
suriettiva: infatti $\dim \left( \ker \left( L\right) \right) =0$ se e solo
se $\dim \left( \func{Im}L\right) =n$, per il teorema di nullit\`{a} pi\`{u}
rango. Dunque \`{e} sufficiente verificare una delle due condizioni per
stabilire se $L$ \`{e} biunivoca.
\end{enumerate}

\textbf{Corollario}%
\begin{eqnarray*}
\text{Hp} &\text{: }&L:V\rightarrow W\text{ \`{e} un'applicazione lineare
iniettiva; }\mathbf{v}_{1}\mathbf{,...,v}_{n}\text{ sono vettori linearmente
indipendenti} \\
\text{Ts} &\text{: }&L\left( \mathbf{v}_{1}\right) ,...,L\left( \mathbf{v}%
_{n}\right) \text{ sono vettori linearmente indipendenti }
\end{eqnarray*}

\textbf{Dim} Mostro che $\lambda _{1}L\left( \mathbf{v}_{1}\right)
+...+\lambda _{n}L\left( \mathbf{v}_{n}\right) =\mathbf{0}_{W}$ implica $%
\lambda _{1}=...=\lambda _{n}=0$. Per linearit\`{a} tale equazione \`{e}
equivalente a $L\left( \lambda _{1}\mathbf{v}_{1}+...+\lambda _{n}\mathbf{v}%
_{n}\right) =0_{W}$, quindi $\lambda _{1}\mathbf{v}_{1}+...+\lambda _{n}%
\mathbf{v}_{n}\in \ker \left( L\right) $. Poich\'{e} $L$ \`{e} iniettiva, $%
\ker \left( L\right) =\left\{ 0_{V}\right\} $ e $\lambda _{1}\mathbf{v}%
_{1}+...+\lambda _{n}\mathbf{v}_{n}=0_{V}$, e poich\'{e} tali vettori sono
linearmente indipendenti $\lambda _{1}=...=\lambda _{n}=0$. $\blacksquare $

Quindi l'iniettivit\`{a} preserva l'indipendenza lineare.

Non vale l'implicazione in assenza di iniettivit\`{a}: si consideri
l'applicazione lineare $L\left( \mathbf{v}\right) =\mathbf{0}$.

\begin{enumerate}
\item Se $L\left( \mathbf{v}_{1}\right) ,...,L\left( \mathbf{v}_{n}\right) $
sono vettori linearmente indipendenti, allora $\mathbf{v}_{1}\mathbf{,...,v}%
_{n}$ sono vettori linearmente indipendenti, anche se $L$ non \`{e}
iniettiva. Infatti, se per assurdo $\mathbf{v}_{1}\mathbf{,...,v}_{n}$
fossero linearmente dipendenti, si avrebbe $x_{1}\mathbf{v}_{1}\mathbf{+...+}%
x_{n}\mathbf{v}_{n}=\mathbf{0}$ con i coefficienti non tutti nulli e $%
L\left( x_{1}\mathbf{v}_{1}\mathbf{+...+}x_{n}\mathbf{v}_{n}\right) =L\left( 
\mathbf{0}\right) $. Ma allora per linearit\`{a} si otterrebbe $x_{1}L\left( 
\mathbf{v}_{1}\right) \mathbf{+...+}x_{n}L\left( \mathbf{v}_{n}\right) =%
\mathbf{0}$ con coefficienti non tutti nulli, cio\`{e} $L\left( \mathbf{v}%
_{1}\right) ,...,L\left( \mathbf{v}_{n}\right) $ sarebbero linearmente
dipendenti,b b contro l'ipotesi.

\item Se $L:V\rightarrow W$ \`{e} un'applicazione lineare, considero una
base $B_{1}$ di $\ker L$ e la completo a una base di $V$, usando $B_{2}$
(che genera il sottospazio $U\subseteq V$) come completamento. Allora $U\cap
\ker L=\left\{ \mathbf{0}\right\} $ perch\'{e} $B_{1}\cup B_{2}$ \`{e} un
insieme di vettori linearmente indipendenti. Se per assurdo
nell'intersezione ci fosse altro, esisterebbe $\mathbf{v\neq 0}$ che si pu%
\`{o} scrivere come combinazione lineare dei vettori di entrambe le basi, e
portando dall'altro lato si avrebbe che si pu\`{o} ottenere il vettore nullo
con coefficienti non tutti nulli, per cui i vettori non sarebbero
linearmente indipendenti. Anche intuitivamente, se $U\cap \ker L$ non
contenesse solo il vettore nullo significherebbe che $B_{2}$ non \`{e} un
vero completamento, ma contiene dei vettori "inutili".

\item $L$, descritta dalla matrice $A$, \`{e} iniettiva se e solo se \`{e}
invertibile a sinistra. Infatti, se \`{e} invertibile a sinistra, $\exists $ 
$B:BA=Id_{n}$: questo significa che $B\left[ C_{1}\left( A\right)
|...|C_{n}\left( A\right) \right] =\left[ M\left( C_{1}\left( A\right)
\right) |...|M\left( C_{n}\left( A\right) \right) \right] $. Dal momento che
le $n$ colonne del risultato sono linearmente indipendenti, anche le colonne
di $A$ sono indipendenti e $r\left( A\right) =n$.
\end{enumerate}

\textbf{Corollario}%
\begin{eqnarray*}
\text{Hp}\text{: } &&L:V\rightarrow W\text{ \`{e} un'applicazione lineare
iniettiva; }H\subset V\text{ \`{e} un sottospazio vettoriale} \\
\text{Ts} &\text{:}&\text{ }\dim \left( L\left( H\right) \right) =\dim
\left( H\right)
\end{eqnarray*}

\textbf{Dim} $\dim \left( H\right) =h$, cio\`{e} $B=\left\{ \mathbf{v}_{1}%
\mathbf{,...,v}_{h}\right\} $ \`{e} una base di $H$. Mostro che $L\left(
B\right) =\left\{ L\left( \mathbf{v}_{1}\right) ,...,L\left( \mathbf{v}%
_{h}\right) \right\} $ \`{e} una base di $L\left( H\right) $. Per il
precedente corollario $L\left( \mathbf{v}_{1}\right) ,...,L\left( \mathbf{v}%
_{h}\right) $ sono vettori linearmente indipendenti. Per costruzione (per la
proposizione??) sono generatori di $L\left( H\right) $: $\forall $ $\mathbf{w%
}\in L\left( H\right) $, $\exists $ $\mathbf{v}\in H:L\left( \mathbf{v}%
\right) =\mathbf{w}$. Per ipotesi $H=Span\left( \mathbf{v}_{1}\mathbf{,...,v}%
_{h}\right) $ e in particolare $\mathbf{v}=\lambda _{1}\mathbf{v}%
_{1}+....+\lambda _{h}\mathbf{v}_{h}$. Applico $L$ a entrambi i lati:
ottengo $L\left( \lambda _{1}\mathbf{v}_{1}+....+\lambda _{h}\mathbf{v}%
_{h}\right) =\lambda _{1}L\left( \mathbf{v}_{1}\right) +....+\lambda
_{h}L\left( \mathbf{v}_{h}\right) =\mathbf{w}$, perch\'{e} $L$ \`{e}
un'applicazione lineare. Dunque $\mathbf{w}\in Span\left\{ L\left( \mathbf{v}%
_{1}\right) ,...,L\left( \mathbf{v}_{h}\right) \right\} $, cio\`{e} ogni
elemento di $L\left( H\right) $ pu\`{o} essere scritto come combinazione
lineare di $L\left( \mathbf{v}_{1}\right) ,...,L\left( \mathbf{v}_{h}\right) 
$, che costituiscono dunque una base. $\blacksquare $

Ci sono alcune conseguenze:

\begin{description}
\item[-] $H\subseteq V$ implica $\dim \left( H\right) \leq \dim \left(
V\right) $. Infatti, se considero una base di $H$ $B=\left\{ \mathbf{v}_{1}%
\mathbf{,...,v}_{h}\right\} $ e l'applicazione lineare identit\`{a} $%
id:H\rightarrow V$, che \`{e} iniettiva, per il primo corollario $\mathbf{v}%
_{1}=id\left( \mathbf{v}_{1}\right) $,..., $\mathbf{v}_{h}=id\left( \mathbf{v%
}_{h}\right) $ sono linearmente indipendenti in $V$.(corollario 2?) Allora $%
h=\dim \left( H\right) $ \`{e} minore o uguale del massimo numero di vettori
linearmente indipendenti in $V$, che \`{e} $\dim \left( V\right) $.

\item[-] Se $H\subseteq V$, $\dim \left( H\right) =\dim \left( V\right) $
implica $H=V$ (l'implicazione opposta \`{e} ovvia). Infatti si ha $\dim
\left( H\right) \leq \dim \left( V\right) $, ma se per assurdo la relazione
fosse stretta, si potrebbe completare una base di $H$ a una base di $V$, e
la dimensione cambierebbe. In alternativa si pu\`{o} considerare che, dato $%
\dim H=\dim V=n$, una base di $H$ \`{e} formata da $n$ elementi linearmente
indipendenti sia in $H$ che in $V$, essendo $H\subseteq V$, quindi \`{e}
anche una base di $V$, ma se due spazi vettoriali hanno una base in comune
coincidono.
\end{description}

\subsubsection{Rappresentazione di un'applicazione lineare}

Come definire un'applicazione lineare?

\textbf{Proposizione} 
\begin{gather*}
\text{Hp}\text{: }V,W\text{ sono spazi vettoriali; }B=\left\{ \mathbf{v}_{1}%
\mathbf{,...,v}_{n}\right\} \text{ \`{e} una base di }V\text{; } \\
S=\left\{ \mathbf{w}_{1}\mathbf{,...,w}_{n}\right\} \text{ \`{e} un insieme
di vettori qualsiasi di }W \\
\text{Ts}\text{: esiste un'unica applicazione lineare }L:V\rightarrow
W:L\left( \mathbf{v}_{1}\right) =\mathbf{w}_{1},...,L\left( \mathbf{v}%
_{n}\right) =\mathbf{w}_{n}
\end{gather*}

\textbf{Dim} Se esiste $L:V\rightarrow W$ con le propriet\`{a} richieste,
allora $\forall $ $\mathbf{v}\in V$ $\mathbf{v}=\lambda _{1}\mathbf{v}%
_{1}+...+\lambda _{n}\mathbf{v}_{n}$ e $L\left( \mathbf{v}\right) =\lambda
_{1}L\left( \mathbf{v}_{1}\right) +...+\lambda _{n}L\left( \mathbf{v}%
_{n}\right) =\lambda _{1}\mathbf{w}_{1}+...+\lambda _{n}\mathbf{w}_{n}\in W$%
. Rimane da verificare che una funzione definita in questo modo \`{e}
additiva e omogenea (??). $\blacksquare $

\begin{enumerate}
\item $V=\mathbf{K}^{n}$, $W=\mathbf{K}^{m}$. So gi\`{a} costruire
applicazioni lineari $L:\mathbf{K}^{n}\rightarrow \mathbf{K}^{m}$ usando il
prodotto matriciale ($L=\tciLaplace _{A}$). Posso costruirne altre? Uso la
proposizione, considerando base di $\mathbf{K}^{n}$ $B=\left\{ \mathbf{e}_{1}%
\mathbf{,...,e}_{n}\right\} $. Scelgo le immagini dei vettori di $B$ $%
S=\left\{ \mathbf{w}_{1}\mathbf{,...,w}_{n}\right\} $: $L\left( \mathbf{e}%
_{1}\right) =\mathbf{w}_{1},...,L\left( \mathbf{e}_{n}\right) =\mathbf{w}%
_{n} $. Quindi $L\left( \mathbf{e}_{1}\right) =L\left( 
\begin{array}{c}
1 \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right) =\left( 
\begin{array}{c}
w_{11} \\ 
... \\ 
w_{m1}%
\end{array}%
\right) $, ecc. $\forall $ $\mathbf{v}\in V$ $\mathbf{v}=v_{1}\mathbf{e}%
_{1}+...+v_{n}\mathbf{e}_{n}=\left( 
\begin{array}{c}
v_{1} \\ 
... \\ 
v_{n}%
\end{array}%
\right) $. Calcolo $L\left( \mathbf{v}\right) =L\left( v_{1}\mathbf{e}%
_{1}+...+v_{n}\mathbf{e}_{n}\right) =v_{1}L\left( \mathbf{e}_{1}\right)
+...+v_{n}L\left( \mathbf{e}_{n}\right) =v_{1}\left( 
\begin{array}{c}
w_{11} \\ 
... \\ 
w_{m1}%
\end{array}%
\right) +...+v_{n}\left( 
\begin{array}{c}
w_{1n} \\ 
... \\ 
w_{mn}%
\end{array}%
\right) $, che pu\`{o} essere riscritta usando il prodotto matriciale $\left[
\begin{array}{ccc}
w_{11} & ... & w_{1n} \\ 
... & ... & ... \\ 
w_{m1} & ... & w_{mn}%
\end{array}%
\right] \left[ 
\begin{array}{c}
v_{1} \\ 
... \\ 
v_{n}%
\end{array}%
\right] $. Quindi l'applicazione lineare costruita \`{e} in ogni caso
codificata col prodotto matriciale.

Tutte e sole le applicazioni lineari $L:\mathbf{K}^{n}\mathbf{\rightarrow K}%
^{m}$ sono del tipo $L=\tciLaplace _{A}$, con $A\in M_{\mathbf{K}}\left(
m,n\right) $. $M_{\mathbf{K}}\left( m,n\right) =\hom \left( \mathbf{K}^{n}%
\mathbf{,K}^{m}\right) $ indica un omomorfismo, cio\`{e} l'insieme delle
applicazioni lineari da $\mathbf{K}^{n}$\textbf{\ }in $\mathbf{K}^{m}$. In
generale $M_{\mathbf{K}}\left( m,n\right) =mor\left( \mathbf{K}^{n}\mathbf{,K%
}^{m}\right) $ indica un morfismo: nel caso dominio e codominio siano
strutture algebriche si parla di omomorfismo.
\end{enumerate}

\textbf{Teorema (isomorfismo con }$\mathbf{K}^{n}$)%
\begin{gather*}
\text{Hp}\text{: }V\text{ \`{e} uno spazio vettoriale su }\mathbf{K}\text{, }%
\dim \left( V\right) =n \\
\text{Ts}\text{: esiste un'applicazione lineare }L:V\rightarrow \mathbf{K}%
^{n}\text{ invertibile}
\end{gather*}

\textbf{Dim} Uso la proposizione precedente: $B=\left\{ \mathbf{v}_{1}%
\mathbf{,...,v}_{n}\right\} $ \`{e} una base di $V$ e definisco $%
L:V\rightarrow \mathbf{K}^{n}$ come l'unica applicazione lineare tale che $%
L\left( \mathbf{v}_{i}\right) =\mathbf{e}_{i}$ per ogni $i=1,...,n$. $L$ 
\`{e} invertibile perch\'{e} conserva sia la propriet\`{a} di essere
generatori che l'indipendenza lineare, essendo $\left\{ \mathbf{e}_{1}%
\mathbf{,...,e}_{n}\right\} $ la base canonica di $\mathbf{K}^{n}$. $%
\blacksquare $

Un'applicazione lineare invertibile si dice isomorfismo; se esiste $%
L:V\rightarrow \mathbf{K}^{n}$ invertibile, $V$ e $\mathbf{K}^{n}$ si dicono
isomorfi, quindi il teorema pu\`{o} essere espresso pi\`{u} sinteticamente
dicendo che ogni spazio vettoriale di dimensione $n$ \`{e} isomorfo a $%
\mathbf{K}^{n}$.

L'applicazione lineare usata nella dimostrazione riveste un ruolo
particolare.

\textbf{Def} Dato uno spazio vettoriale $V$ su un campo $\mathbf{K}$ e una
sua base $B=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} $, si dice
mappa delle coordinate l'applicazione lineare $X_{B}:V\rightarrow \mathbf{K}%
^{n}$ definita da $X_{B}\left( \mathbf{v}_{i}\right) =\mathbf{e}_{i}$ per $%
i=1,...,n$.

Questo significa che, se un generico vettore $\mathbf{v}$ si scrive rispetto
ai vettori di $B$ come $x_{1}\mathbf{v}_{1}+...+x_{n}\mathbf{v}_{n}$, $%
X_{B}\left( \mathbf{v}\right) =X_{B}\left( x_{1}\mathbf{v}_{1}+...+x_{n}%
\mathbf{v}_{n}\right) =x_{1}X_{B}\left( \mathbf{v}_{1}\right)
+...+x_{n}X_{B}\left( \mathbf{v}_{n}\right) =\left( 
\begin{array}{c}
x_{1} \\ 
... \\ 
x_{n}%
\end{array}%
\right) $. Quindi la mappa delle coordinate rispetto a $B$ di un vettore di $%
V$ associa a ogni $\mathbf{v}\in V$ il vettore dei coefficienti della
combinazione lineare dei vettori della base che permette di ottenere $%
\mathbf{v}$.

Tale applicazione lineare \`{e} un isomorfismo e l'applicazione lineare
inversa si dice mappa di parametrizzazione $P_{B}:\mathbf{K}^{n}\rightarrow
V $, definita da $P_{B}\left( \mathbf{e}_{i}\right) =\mathbf{v}_{i}$ per $%
i=1,...,n$.

Questo significa che, dato un vettore $\mathbf{w}$ delle coordinate rispetto
ai vettori di $B$, $P_{B}\left( \mathbf{w}\right) =P_{B}\left( x_{1}\mathbf{e%
}_{1}+...+x_{n}\mathbf{e}_{n}\right) =x_{1}P_{B}\left( \mathbf{e}_{1}\right)
+...+x_{n}P_{B}\left( \mathbf{e}_{n}\right) =x_{1}\mathbf{v}_{1}+...+x_{n}%
\mathbf{v}_{n}=\mathbf{v}$.

L'idea della mappa della coordinate \`{e} poter ricorrere a una "lingua
universale", cio\`{e} $\mathbf{K}^{n}$, per esprimere qualsiasi vettore di
uno spazio vettoriale e quindi comunicare tra spazi vettoriali diversi.

\begin{enumerate}
\item Considero $V=M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( 2,2\right) $ e la sua base canonica $B=\left\{
E_{11},E_{12},E_{21},E_{22}\right\} $. $X_{B}\left( E_{11}\right) =\mathbf{e}%
_{1}=\left( 
\begin{array}{c}
1 \\ 
0 \\ 
0 \\ 
0%
\end{array}%
\right) ,...,X_{B}\left( E_{22}\right) =\mathbf{e}_{4}=\left( 
\begin{array}{c}
0 \\ 
0 \\ 
0 \\ 
1%
\end{array}%
\right) $. Data $A=\left[ 
\begin{array}{cc}
a & b \\ 
c & d%
\end{array}%
\right] =aE_{11}+bE_{12}+cE_{21}+dE_{22}$, $X_{B}\left( A\right)
=aX_{B}\left( E_{11}\right) +bX_{B}\left( E_{12}\right) +cX_{B}\left(
E_{21}\right) +dX_{B}\left( E_{22}\right) =\left( 
\begin{array}{c}
a \\ 
b \\ 
c \\ 
d%
\end{array}%
\right) $: sono i coefficienti della combinazione lineare dei vettori della
base che permette di ottenere $A$.

\item Considero $V=\mathbf{K}\left[ t\right] _{\leq 2}$ e la sua base
canonica $B=\left\{ 1,t,t^{2}\right\} $. La mappa di parametrizzazione $%
P_{B}:\mathbf{K}^{3}\rightarrow \mathbf{K}\left[ t\right] _{\leq 2}$ associa
a ogni vettore $\mathbf{w=}\left( 
\begin{array}{c}
x \\ 
y \\ 
z%
\end{array}%
\right) $ di $\mathbf{K}^{3}$ il vettore di $V$ che si ottiene combinando
linearmente i vettori di $B$ con coefficienti $x,y,z$: $P_{B}\left( \mathbf{w%
}\right) =P_{B}\left( x_{1}\mathbf{e}_{1}+y\mathbf{e}_{2}+z\mathbf{e}%
_{3}\right) =x_{1}\cdot 1+y\cdot t+z\cdot t^{2}$. Al variare di $\mathbf{%
w\in K}^{3}$ si descrive ogni vettore di $\mathbf{K}\left[ t\right] _{\leq
2} $.
\end{enumerate}

\textbf{Teorema (isomorfismo tra spazi vettoriali con stessa dimensione)}%
\begin{eqnarray*}
\text{Hp}\text{: } &&V,W\text{ sono spazi vettoriali su }\mathbf{K} \\
\text{Ts}\text{: } &&V,W\text{ sono isomorfi }\Longleftrightarrow \dim
V=\dim W
\end{eqnarray*}

\textbf{Dim} (i) Se $V$ e $W$ sono isomorfi, esiste un'applicazione lineare $%
L:V\rightarrow W$ invertibile. Considero una base di $V$ $B=\left\{ \mathbf{v%
}_{1}\mathbf{,...,v}_{n}\right\} $. Quindi, essendo $L$ iniettiva, $L\left( 
\mathbf{v}_{1}\right) ,...,L\left( \mathbf{v}_{n}\right) $ sono vettori di $%
W $ linearmente indipendenti; essendo $L$ suriettiva, $L\left( \mathbf{v}%
_{1}\right) ,...,L\left( \mathbf{v}_{n}\right) $ sono generatori di $W$.
Dunque $\left\{ L\left( \mathbf{v}_{1}\right) ,...,L\left( \mathbf{v}%
_{n}\right) \right\} $ \`{e} una base di $W$ e $\dim W=\dim V=n$. In
generale, un'applicazione lineare invertibile preserva le basi.

(ii) Vale $\dim V=\dim W=n$: mostro che esiste un'applicazione lineare
invertibile da $V$ in $W$. Chiamo $B$ una base di $V$, $B^{\prime }$ una
base di $W$. Considero $X_{B}:V\rightarrow \mathbf{K}^{n}$, $P_{B}:\mathbf{K}%
^{n}\rightarrow V$, $X_{B^{\prime }}:W\rightarrow \mathbf{K}^{n}$, $%
P_{B^{\prime }}:\mathbf{K}^{n}\rightarrow W$. Allora $L=P_{B^{\prime }}\circ
X_{B}$, $L:V\rightarrow W$ \`{e} un isomorfismo, perch\'{e} composizione di
isomorfismi, dunque $V$ e $W$ sono isomorfi. $\blacksquare $

\textbf{Teorema di rappresentazione}

Si vuole ora utilizzare la "lingua universale", la traduzione nella quale 
\`{e} permessa dalla mappa delle coordinate, per rappresentare
un'applicazione lineare.

Sia $V$ uno spazio vettoriale su $\mathbf{K}$ di dimensione $n$ con base $%
B=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} $, sia $W$ uno spazio
vettoriale su $\mathbf{K}$ di dimensione $m$ con base $C=\left\{ \mathbf{w}%
_{1}\mathbf{,...,w}_{m}\right\} $. Considero un'applicazione lineare $%
L:V\rightarrow W$. Tutte le applicazione lineari possono essere
rappresentate con una matrice: qual \`{e} la matrice $A\in M_{\mathbf{K}%
}\left( m,n\right) :X_{C}\left( L\left( \mathbf{v}\right) \right)
=AX_{B}\left( \mathbf{v}\right) =\tciLaplace _{A}\left( X_{B}\left( \mathbf{v%
}\right) \right) $, cio\`{e} qual \`{e} $A:P_{C}\left( AX_{B}\left( \mathbf{v%
}\right) \right) =L\left( \mathbf{v}\right) $?%
\begin{gather*}
\text{Hp}\text{: }V,W\text{ sono spazi vettoriali su }\mathbf{K}\text{, }%
L:V\rightarrow W\text{ \`{e} un'applicazione lineare,} \\
B=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} \text{ \`{e} una base di 
}V\text{, }\dim V=n\text{, }C\text{ \`{e} una base di }W\text{, }\dim W=m \\
\text{Ts}\text{:}\text{ la matrice }A\in M_{\mathbf{K}}\left( m,n\right) 
\text{ definita come }a_{ij}=i-\text{esima coordinata rispetto alla base }C%
\text{ dell'immagine } \\
\text{mediante }L\text{ del }j-\text{esimo vettore di }B\text{ (}L\left( 
\mathbf{v}_{j}\right) \text{) \`{e} l'unica matrice tale che, }\forall \text{
}\mathbf{v}\in V\text{, }AX_{B}\left( \mathbf{v}\right) =X_{C}\left( L\left( 
\mathbf{v}\right) \right)
\end{gather*}

La tesi richiede che la matrice sia $A=\left[ X_{C}\left( L\left( \mathbf{v}%
_{1}\right) \right) |....|X_{C}\left( L\left( \mathbf{v}_{n}\right) \right) %
\right] $. Significa che ogni applicazione lineare pu\`{o} essere
rappresentata come un'applicazione lineare $\tciLaplace _{A}:\mathbf{K}^{n}%
\mathbf{\rightarrow K}^{m}$.

\textbf{Dim} $B=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} $, $%
C=\left\{ \mathbf{w}_{1}\mathbf{,...,w}_{m}\right\} $. Poich\'{e} $L\left( 
\mathbf{v}_{i}\right) \in W$, $L\left( \mathbf{v}_{i}\right) $ si pu\`{o}
scrivere come combinazione lineare dei vettori di $C$: $L\left( \mathbf{v}%
_{1}\right) =a_{11}\mathbf{w}_{1}+a_{21}\mathbf{w}_{2}+...+a_{m1}\mathbf{w}%
_{m}$ (in questo caso sto considerando il primo vettore della base $B$, $j=1$%
, perci\`{o} ogni coefficiente dev'essere del tipo $a_{i1}$),..., $L\left( 
\mathbf{v}_{n}\right) =a_{1n}\mathbf{w}_{1}+a_{2n}\mathbf{w}_{2}+...+a_{mn}%
\mathbf{w}_{m}$. Dunque%
\begin{equation*}
A=\left[ 
\begin{array}{cccc}
a_{11} & a_{12} & ... & a_{1n} \\ 
a_{21} & a_{22} & ... & a_{2n} \\ 
... & ... & ... & ... \\ 
a_{m1} & a_{m2} & ... & a_{mn}%
\end{array}%
\right]
\end{equation*}

dove la colonna $j$ \`{e} il vettore delle coordinate di $L\left( \mathbf{v}%
_{j}\right) $ rispetto a $C$, la riga $i$ \`{e} la sequenza dei coefficienti
di posto $i$ in ciascun vettore delle coordinate. Ora che \`{e} noto com'%
\`{e} costruita la matrice, si mostra che soddisfa le propriet\`{a}
richieste. Per ogni $\mathbf{v}\in V$ considero $\mathbf{v}=x_{1}\mathbf{v}%
_{1}+...+x_{n}\mathbf{v}_{n}$: $L\left( \mathbf{v}\right) =x_{1}L\left( 
\mathbf{v}_{1}\right) +...+x_{n}L\left( \mathbf{v}_{n}\right) $, perch\'{e} $%
L$ \`{e} lineare. Le coordinate di $L\left( \mathbf{v}\right) $ rispetto a $%
C $ sono $X_{C}\left( L\left( \mathbf{v}\right) \right) =x_{1}X_{C}\left(
L\left( \mathbf{v}_{1}\right) \right) +...+x_{n}X_{C}\left( L\left( \mathbf{v%
}_{n}\right) \right) $, poich\'{e} $X_{C}$ \`{e} lineare; inoltre $%
X_{C}\left( L\left( \mathbf{v}_{i}\right) \right) $ \`{e} noto per ogni $i$
perch\'{e} colonna di $A$. Quindi $X_{C}\left( L\left( \mathbf{v}\right)
\right) =x_{1}\left( 
\begin{array}{c}
a_{11} \\ 
... \\ 
a_{m1}%
\end{array}%
\right) +...+x_{n}\left( 
\begin{array}{c}
a_{1n} \\ 
... \\ 
a_{mn}%
\end{array}%
\right) $, e questo \`{e} uguale a $\left[ 
\begin{array}{cccc}
a_{11} & a_{12} & ... & a_{1n} \\ 
a_{21} & a_{22} & ... & a_{2n} \\ 
... & ... & ... & ... \\ 
a_{m1} & a_{m2} & ... & a_{mn}%
\end{array}%
\right] \left[ 
\begin{array}{c}
x_{1} \\ 
... \\ 
x_{n}%
\end{array}%
\right] =AX_{B}\left( \mathbf{v}\right) $. $%
\blacksquare $

\textbf{Def} La matrice $A$ introdotta nel teorema di rappresentazione si
dice matrice rappresentativa di $L$ rispetto alle basi $B$ e $C$ e si indica
con $A=M_{B}^{C}\left( L\right) $ (si pone a pedice la base del dominio, ad
apice la base del codominio).

\begin{enumerate}
\item $V=\mathbf{K}\left[ t\right] _{\leq 2}$, $\dim V=3$, $B=\left\{
1,t,t^{2}\right\} $, $W=Span\left( \mathbf{w}_{1}\mathbf{,w}_{2}\right) $, $%
C=\left\{ \mathbf{w}_{1}\mathbf{,w}_{2}\right\} $. $L:V\rightarrow W$ \`{e}
definita da $L\left( 1\right) =\mathbf{w}_{1}\mathbf{+w}_{2}$, $L\left(
t\right) =\mathbf{w}_{1}$, $L\left( t^{2}\right) =-\mathbf{w}_{1}+2\mathbf{w}%
_{2}$. $X_{C}\left( L\left( 1\right) \right) =\left( 
\begin{array}{c}
1 \\ 
1%
\end{array}%
\right) $, $X_{C}\left( L\left( t\right) \right) =\left( 
\begin{array}{c}
1 \\ 
0%
\end{array}%
\right) $, $X_{C}\left( L\left( t^{2}\right) \right) =\left( 
\begin{array}{c}
-1 \\ 
2%
\end{array}%
\right) $, quindi $M_{B}^{C}\left( L\right) =\left[ 
\begin{array}{ccc}
1 & 1 & -1 \\ 
1 & 0 & 2%
\end{array}%
\right] $.

\item $V=W=M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( 2,2\right) $, $B=\left\{ E_{11},E_{12},E_{21},E_{22}\right\} $, $%
L:V\rightarrow W$, $L\left( X\right) =AX$ con $A=\left[ 
\begin{array}{cc}
a & b \\ 
c & d%
\end{array}%
\right] $. La matrice rappresentativa di $L$ rispetto a $B$ si ottiene
calcolando $L\left( E_{11}\right) =AE_{11}=\left[ 
\begin{array}{cc}
a & 0 \\ 
c & 0%
\end{array}%
\right] =aE_{11}+cE_{21}$, $L\left( E_{12}\right) =AE_{12}=\left[ 
\begin{array}{cc}
0 & a \\ 
0 & c%
\end{array}%
\right] =aE_{12}+cE_{22}$, $L\left( E_{21}\right) =AE_{21}=\left[ 
\begin{array}{cc}
b & 0 \\ 
d & 0%
\end{array}%
\right] =bE_{11}+dE_{21}$, $L\left( E_{22}\right) =AE_{22}=\left[ 
\begin{array}{cc}
0 & b \\ 
0 & d%
\end{array}%
\right] =bE_{12}+dE_{22}$. Quindi $M_{B}^{B}\left( L\right) =\left[ 
\begin{array}{cccc}
a & 0 & b & 0 \\ 
0 & a & 0 & b \\ 
c & 0 & d & 0 \\ 
0 & c & 0 & d%
\end{array}%
\right] $.
\end{enumerate}

Il teorema di rappresentazione ha alcune conseguenze:

\begin{description}
\item[-] Se $L:V\rightarrow V$, $L=id$, $B=\left\{ \mathbf{v}_{1},...,%
\mathbf{v}_{n}\right\} $ \`{e} una base di $V$, $M_{B}^{B}\left( L\right)
=id $. Infatti $X_{B}\left( L\left( \mathbf{v}_{1}\right) \right)
=X_{B}\left( \mathbf{v}_{1}\right) =\left( 
\begin{array}{c}
1 \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right) $,..., $X_{B}\left( L\left( \mathbf{v}_{n}\right) \right)
=X_{B}\left( \mathbf{v}_{n}\right) =\left( 
\begin{array}{c}
0 \\ 
... \\ 
0 \\ 
1%
\end{array}%
\right) $, quindi $M_{B}^{B}\left( id\right) =Id$ ($id$ \`{e} l'applicazione
lineare identit\`{a}, $Id$ \`{e} la matrice identit\`{a}).

\item[-] $V,W,U$ sono spazi vettoriali su $\mathbf{K}$, $B,C,D$ sono basi di 
$V,W,U$ rispettivamente, $L_{1}:V\rightarrow W$, $L_{2}:W\rightarrow U$. La
matrice rappresentativa della composizione delle due applicazioni lineari 
\`{e} $M_{B}^{D}\left( L_{2}\circ L_{1}\right) =M_{C}^{D}\left( L_{2}\right)
\cdot M_{B}^{C}\left( L_{1}\right) $, cio\`{e} il prodotto delle due matrici
rappresentative.

\item[-] $V,W$ sono spazi vettoriali isomorfi su $\mathbf{K}$, $B,C$ sono
basi di $V,W$ rispettivamente (hanno la stessa dimensione per il teorema
visto), $L:V\rightarrow W$, $L^{-1}:W\rightarrow V$. La matrice
rappresentativa della composizione delle due applicazioni lineari \`{e} $%
M_{B}^{B}\left( L\circ L^{-1}\right) =M_{C}^{B}\left( L^{-1}\right) \cdot
M_{B}^{C}\left( L\right) $: poich\'{e} $M_{B}^{B}\left( L\circ L^{-1}\right)
=M_{B}^{B}\left( id\right) =Id$ per la prima conseguenza, $M_{C}^{B}\left(
L^{-1}\right) \cdot M_{B}^{C}\left( L\right) =Id_{n}$, cio\`{e} $%
M_{C}^{B}\left( L^{-1}\right) =M_{B}^{C}\left( L\right) ^{-1}$ (perch\'{e}
le due matrici sono quadrate, essendo $V,W$ isomorfi, e se esiste l'inversa
sinistra esiste anche l'inversa destra). La matrice rappresentativa
dell'inversa di $L$ \`{e} l'inversa della matrice rappresentativa di $L$.

\item[-] $V,W$ sono spazi vettoriali su $\mathbf{K}$, $B,C$ sono basi di $%
V,W $ rispettivamente, $L:V\rightarrow W$, $X_{B}:V\rightarrow \mathbf{K}%
^{n} $, $X_{C}:W\rightarrow \mathbf{K}^{m}$, $A=M_{B}^{C}\left( L\right) $, $%
\tciLaplace _{A}:\mathbf{K}^{n}\mathbf{\rightarrow K}^{m}$. E' noto che $%
\ker \left( \tciLaplace _{A}\right) =\ker \left( A\right) $ e $\func{Im}%
\left( \tciLaplace _{A}\right) =\func{col}\left( A\right) $, ma quali sono
le relazioni con $\ker \left( L\right) $e $\func{Im}\left( L\right) $,
rispettivamente? Vale $X_{B}\left( \ker \left( L\right) \right) =\ker \left(
A\right) $ (si applica all'indietro la linearit\`{a} di $L$) e $X_{C}\left( 
\func{Im}\left( L\right) \right) =\func{col}\left( A\right) $: coerentemente
con il teorema di rappresentazione, gli elementi del nucleo di $A$ sono i
rappresentanti mediante coordinate degli elementi del nucleo di $L$, e
analogamente per l'immagine. Si dimostra con il fatto che $X_{C},X_{B}$ sono
isomorfismi.
\end{description}

\subsubsection{Teorema di nullit\`{a} pi\`{u} rango}

\begin{eqnarray*}
\text{Hp}\text{: } &&V,W\text{ sono spazi vettoriali su }\mathbf{K}\text{ di
dimensione finita, }L:V\rightarrow W\text{ \`{e} un'applicazione lineare} \\
\text{Ts}\text{: } &&\dim V=\dim \left( \ker \left( L\right) \right) +\dim
\left( \func{Im}\left( L\right) \right)
\end{eqnarray*}

che \`{e} una reinterpretazione del gi\`{a} visto teorema del rango fondata
sul fatto che ogni applicazione lineare \`{e} rappresentata da una matrice. $%
\dim \left( \ker \left( L\right) \right) $ \`{e} la dimensione della parte
di informazione "buttata via".

\textbf{Dim} Chiamo $B$ una base di $V$, $\dim V=n$, $C$ una base di $W$, $%
\dim W=m$. Considero $\tciLaplace _{A}:\mathbf{K}^{n}\mathbf{\rightarrow K}%
^{m}$, $A=M_{B}^{C}\left( L\right) $; per la conseguenza 4 $X_{B}\left( \ker
\left( L\right) \right) =\ker \left( \tciLaplace _{A}\right) $: poich\'{e} $%
X_{B}$ \`{e} iniettiva (in quanto invertibile), $\dim \left( X_{B}\left(
\ker \left( L\right) \right) \right) =\dim \left( \ker \left( \tciLaplace
_{A}\right) \right) =\dim \left( \ker \left( A\right) \right) $ ($\ker
\left( L\right) $ e $\ker \left( \tciLaplace _{A}\right) $ sono isomorfi).
Analogamente, $X_{C}\left( \func{Im}\left( L\right) \right) =\func{col}%
\left( A\right) $: poich\'{e} $X_{C}$ \`{e} iniettiva, $\dim \left(
X_{C}\left( \func{Im}\left( L\right) \right) \right) =\dim \left( \func{col}%
\left( A\right) \right) $ ($\func{Im}\left( L\right) $ e $\func{col}\left(
A\right) $ sono isomorfi). Ma allora, per il teorema del rango, $\dim \left(
\ker \left( A\right) \right) +\dim \left( \func{col}\left( A\right) \right)
=n-r\left( A\right) +r\left( A\right) =n=\dim \mathbf{K}^{n}=\dim V$. $%
\blacksquare $

\textbf{Def} Data $L:V\rightarrow W$, si chiama rango di $L$ e si indica con 
$r\left( L\right) $ la dimensione dell'immagine di $L$.

\begin{enumerate}
\item Se $A\in M_{\mathbf{K}}\left( m,n\right) $, $\dim \left( \ker A\right)
=n\Longleftrightarrow \dim \left( \func{Im}A\right) =r\left( A\right) =0$,
cio\`{e} il nucleo ha dimensione massima (i. e. coincide con $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$) se e solo se $A$ \`{e} la matrice nulla.
\end{enumerate}

\textbf{Corollario}%
\begin{gather*}
\text{Hp}\text{: }V,W\text{ sono spazi vettoriali su }\mathbf{K}\text{, }%
\dim V=n\text{, }\dim W=m\text{, } \\
L:V\rightarrow W\text{ \`{e} un'applicazione lineare, }r=r\left( L\right) \\
\text{Ts}\text{:}\text{ (i) }L\text{ \`{e} iniettiva }\Longleftrightarrow
\dim V=\dim \left( \func{Im}\left( L\right) \right) \Longleftrightarrow n=r
\\
\text{(ii) }L\text{ \`{e} suriettiva }\Longleftrightarrow \func{Im}\left(
L\right) =W\Longleftrightarrow \dim \left( \func{Im}L\right) =\dim \left(
W\right) \Longleftrightarrow r=m \\
\text{(iii) }L\text{ \`{e} invertibile }\Longleftrightarrow r=m=n
\end{gather*}

Se ne possono dedurre conseguenze necessarie per iniettivit\`{a} e
suriettivit\`{a}: se $n>m$, $r\leq m$ e $L$ non pu\`{o} essere iniettiva ($%
\dim \left( \ker \left( L\right) \right) \geq n-m$); se $n<m$, $r\leq n$ e $%
L $ non pu\`{o} essere suriettiva ($\dim \left( \func{Im}\left( L\right)
\right) \leq n$).


\subsubsection{Cambiamenti di base}

Ha senso chiedersi come cambia la matrice rappresentativa di un'applicazione
lineare se si cambiano le basi con cui si costruisce.

\begin{enumerate}
\item $V=W=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$, $L:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$ \`{e} la simmetria assiale rispetto a una retta data. Tale retta deve
passare per l'origine, altrimenti la simmetria assiale non \`{e}
un'applicazione lineare. Voglio descrivere $L$ mediante una sua matrice
rappresentativa; fisso come base di $V=W$ la base canonica $B=\left\{ 
\mathbf{e}_{1}\mathbf{,e}_{2}\right\} $. $M_{B}^{B}\left( L\right) $?
Utilizzando la trigonometria si nota che $L\left( \mathbf{e}_{1}\right)
=\left( 
\begin{array}{c}
\cos 2\alpha \\ 
\sin 2\alpha%
\end{array}%
\right) $ e $L\left( \mathbf{e}_{2}\right) =\left( 
\begin{array}{c}
\cos \left( \frac{\pi }{2}-2\left( \frac{\pi }{2}-\alpha \right) \right) \\ 
\sin \left( \frac{\pi }{2}-2\left( \frac{\pi }{2}-\alpha \right) \right)%
\end{array}%
\right) =\left( 
\begin{array}{c}
\sin 2\alpha \\ 
-\cos 2\alpha%
\end{array}%
\right) $, dove $\alpha $ \`{e} l'angolo tra la retta e l'asse. Quindi si ha 
$M_{B}^{B}\left( L\right) =\left[ 
\begin{array}{cc}
\cos 2\alpha & \sin 2\alpha \\ 
\sin 2\alpha & -\cos 2\alpha%
\end{array}%
\right] $. Tuttavia questa scelta ignora la natura geometrica
dell'applicazione: considerandola ci si accorge che ci sono vettori la cui
simmetria assiale \`{e} particolarmente semplice, e. g. qualsiasi vettore $%
\mathbf{v}$ giaccia completamente sulla retta (l'asse di riflessione) \`{e}
tale che $L\left( \mathbf{v}\right) =\mathbf{v}$, e un vettore $\mathbf{w}$
ortogonale all'asse di riflessione \`{e} tale che $L\left( \mathbf{w}\right)
=\mathbf{-w}$. $\mathbf{v,w}$ sono linearmente indipendenti, quindi $%
C=\left\{ \mathbf{v,w}\right\} $ \`{e} una base di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$ (che dipende da $L$, cio\`{e} dalla retta). Adesso scrivere la matrice
rappresentativa \`{e} molto pi\`{u} semplice: $M_{C}^{C}\left( L\right) =%
\left[ 
\begin{array}{cc}
1 & 0 \\ 
0 & -1%
\end{array}%
\right] $ \`{e} una matrice diagonale (cio\`{e} se $i\neq j$ $a_{ij}=0$),
descritta da $n=2$ coefficienti, mentre $M_{B}^{B}\left( L\right) $ \`{e}
una matrice "normale" descrita da $n^{2}=4$ coefficienti. Se si avesse
genericamente $L:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, si starebbe confrontando una matrice con $n\cdot n$ coefficienti con
una descritta dagli $n$ coefficienti sulla diagonale.
\end{enumerate}

Ha senso scegliere, per rappresentare un'applicazione lineare $L:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, una matrice diagonale.

Qual \`{e} la relazione tra due matrici che rappresentano la stessa
applicazione lineare?

Siano $V,W$ spazi vettoriali tali che $\dim V=n,\dim W=m$, $B_{1},B_{2}$
sono basi di $V$, $C_{1},C_{2}$ sono basi di $W$, $L:V\rightarrow W$ \`{e}
un'applicazione lineare. Allora ci sono quattro possibili matrici
rappresentative di $L$: $M_{B_{1}}^{C_{1}}\left( L\right) $ (rosso)$%
,M_{B_{1}}^{C_{2}}\left( L\right) $ (verde)$,M_{B_{2}}^{C_{1}}\left(
L\right) $ (viola)$,M_{B_{2}}^{C_{2}}\left( L\right) $ (blu). Questo
significa che $X_{C_{1}}\left( L\left( \mathbf{v}\right) \right)
=M_{B_{1}}^{C_{1}}\left( L\right) X_{B_{1}}\left( \mathbf{v}\right) $, $%
X_{C_{1}}\left( L\left( \mathbf{v}\right) \right) =M_{B_{2}}^{C_{1}}\left(
L\right) X_{B_{2}}\left( \mathbf{v}\right) $ eccetera.\FRAME{dtbpF}{3.7137in%
}{2.134in}{0pt}{}{}{cambio di base.png}{\special{language "Scientific
Word";type "GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file
"F";width 3.7137in;height 2.134in;depth 0pt;original-width
7.579in;original-height 4.3389in;cropleft "0";croptop "1";cropright
"1";cropbottom "0";filename '../figure/cambio di base.PNG';file-properties
"XNPEU";}}

Tuttavia, se si conoscesse una matrice $S:X_{B_{2}}\left( \mathbf{v}\right)
=SX_{B_{1}}\left( \mathbf{v}\right) $, si potrebbe scrivere anche $%
X_{C_{1}}\left( L\left( \mathbf{v}\right) \right) =M_{B_{2}}^{C_{1}}\left(
L\right) X_{B_{2}}\left( \mathbf{v}\right) =M_{B_{2}}^{C_{1}}\left( L\right)
SX_{B_{1}}\left( \mathbf{v}\right) $, cio\`{e}, se fosse nota la matrice per
passare dalla rappresentazione di $\mathbf{v}$ rispetto a una base alla
rappresentazione di $\mathbf{v}$ rispetto a un'altra base, ci si potrebbe
muovere liberamente nel diagramma. Questo \`{e} un problema indipendente
dall'applicazione lineare con cui si ha a che fare. Qual \`{e} $%
S:X_{B_{2}}\left( \mathbf{v}\right) =SX_{B_{1}}\left( \mathbf{v}\right) $?
Questo \`{e} un caso particolare della ricerca di $A:AX_{B}\left( \mathbf{v}%
\right) =X_{C}\left( L\left( \mathbf{v}\right) \right) $, in cui $%
L:V\rightarrow V$ \`{e} $L\left( \mathbf{v}\right) =\mathbf{v}$, $B=B_{1}$ e 
$C=B_{2}$. La matrice cercata \`{e} quindi $M_{B_{1}}^{B_{2}}\left(
id\right) $. Il ruolo di $B_{1}$ e $B_{2}$ \`{e} interscambiabile: $%
M_{B_{1}}^{B_{2}}\left( id\right) =\left( M_{B_{2}}^{B_{1}}\left( id\right)
\right) ^{-1}$.

\textbf{Def} Date due basi $B_{1},B_{2}$ di uno spazio vettoriale $V$, la
matrice $M_{B_{2}}^{B_{1}}\left( id\right) $ si dice matrice del cambio di
base da $B_{2}$ a $B_{1}$.

Chiamo $P=M_{B_{2}}^{B_{1}}\left( id\right) $, $Q=M_{C_{2}}^{C_{1}}\left(
id\right) $. Allora $M_{B_{1}}^{C_{1}}\left( L\right)
=M_{C_{2}}^{C_{1}}\left( id\right) M_{B_{1}}^{C_{2}}\left( L\right)
=QM_{B_{1}}^{C_{2}}\left( L\right) $ per la conseguenza due del teorema di
rappresentazione; $M_{B_{1}}^{C_{1}}\left( L\right)
=QM_{B_{1}}^{C_{2}}\left( L\right) =QM_{B_{2}}^{C_{2}}\left( L\right) P^{-1}$%
, perch\'{e} $M_{B_{1}}^{C_{2}}\left( L\right) =M_{B_{2}}^{C_{2}}\left(
L\right) M_{B_{1}}^{B_{2}}\left( id\right) $ sempre per la conseguenza due; $%
M_{B_{1}}^{C_{1}}\left( L\right) =M_{B_{2}}^{C_{1}}\left( L\right)
P^{-1}=M_{B_{2}}^{C_{1}}\left( L\right) M_{B_{1}}^{B_{2}}\left( id\right) $
sempre per la conseguenza due.

\begin{enumerate}
\item Nell'esempio precedente, scegliendo $B=\left\{ \mathbf{e}_{1}\mathbf{,e%
}_{2}\right\} $ si ha come matrice rappresentativa $A=M_{B}^{B}\left(
L\right) =\left[ 
\begin{array}{cc}
\cos 2\alpha & \sin 2\alpha \\ 
\sin 2\alpha & -\cos 2\alpha%
\end{array}%
\right] $, con $C=\left\{ \mathbf{v,w}\right\} $ si ha $B=M_{C}^{C}\left(
L\right) =\left[ 
\begin{array}{cc}
1 & 0 \\ 
0 & -1%
\end{array}%
\right] $. Scrivo una matrice del cambio di base: la pi\`{u} facile da
scrivere \`{e} la $P=M_{C}^{B}\left( id\right) =\left[ 
\begin{array}{cc}
\cos \alpha & \cos \left( \frac{\pi }{2}+\alpha \right) \\ 
\sin \alpha & \sin \left( \frac{\pi }{2}+\alpha \right)%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
\cos \alpha & -\sin \alpha \\ 
\sin \alpha & \cos \alpha%
\end{array}%
\right] $, dove $\mathbf{v}=\cos \alpha \mathbf{e}_{1}+\sin \alpha \mathbf{e}%
_{2}$ e $\mathbf{w}=-\sin \alpha \mathbf{e}_{1}+\cos \alpha \mathbf{e}_{2}$.
Quindi vale $M_{B}^{B}\left( L\right) =M_{C}^{B}\left( id\right)
M_{C}^{C}\left( L\right) M_{B}^{C}\left( id\right) \Longleftrightarrow
A=PBP^{-1}\Longleftrightarrow B=P^{-1}AP$.
\end{enumerate}

\textbf{Def} Due matrici $A,B\in M_{\mathbf{K}}\left( n,n\right) $ si dicono
simili se esiste una matrice invertibile $P\in M_{\mathbf{K}}\left(
n,n\right) :B=P^{-1}AP$.

Se due matrici $A,B$ rappresentano la stessa applicazione lineare, allora
sono simili (implicazione inversa?): se $B=P^{-1}AP$, $P$ \`{e} la matrice
del cambio di base dalla base rispetto a cui $B$ rappresenta l'applicazione
lineare alla base canonica (rispetto cui $A$ rappresenta l'endomorfismo), $B$
rappresenta l'endomorfismo rispetto alla base formata dalle colonne di $P$.

Due matrici $A,B$ simili hanno lo stesso determinante: infatti $\det B=\det
P^{-1}AP=\det P^{-1}\det A\det P=\det A\det P^{-1}\det P=\det A\det Id=\det
A $ usando due volte il teorema di Binet e la commutativit\`{a} del prodotto
su $\mathbf{K}$.

La relazione di similitudine tra matrici \`{e} una relazione di equivalenza.
Infatti ha le propriet\`{a}:

\begin{enumerate}
\item Riflessiva: ogni matrice $A$ \`{e} simile a se stessa. $%
A=Id_{n}^{-1}AId_{n}=Id_{n}AId_{n}$

\item Simmetrica: se $A$ \`{e} simile a $B$ allora $B$ \`{e} simile ad $A$.
Infatti se $B=P^{-1}AP$, si ha $A=PBP^{-1}=Q^{-1}BQ$, ponendo $Q=P^{-1}$.

\item Transitiva: se $A$ \`{e} simile a $B$ e $B$ \`{e} simile a $C$, allora 
$A$ \`{e} simile a $C$. Infatti se $B=P^{-1}AP$, $C=Q^{-1}BQ$, si ha $%
C=Q^{-1}BQ=Q^{-1}\left( P^{-1}AP\right) Q=\left( Q^{-1}P^{-1}\right) A\left(
PQ\right) =\left( PQ\right) ^{-1}A\left( PQ\right) =R^{-1}AR$, ponendo $R=PQ$%
.
\end{enumerate}

Ogni relazione di equivalenza in un insieme induce una partizione
dell'insieme in classi di equivalenza (come avviene anche e. g. per le
frazioni): quindi si pu\`{o} dividere $M_{\mathbf{K}}\left( n,n\right) $ in
classi di equivalenza di matrici simili, prive di intersezioni. Una classe
di equivalenza di matrici simili, rappresentata dalla matrice $A$, si indica
con $\left[ A\right] $.

Come scegliere una opportuna matrice per rappresentare ogni classe di
equivalenza (cio\`{e} ogni applicazione lineare con dominio e codominio
isomorfi)?

\subsection{Diagonalizzabilit\`{a}, autovettori, autovalori, similitudine}

Tutto quello che segue riguarda matrici quadrate.

\textbf{Def} Una matrice $A\in M_{\mathbf{K}}\left( n,n\right) $ si dice
diagonalizzabile su $\mathbf{K}$ se esiste una matrice diagonale $D$ a cui $%
A $ \`{e} simile, i. e. se $\exists $ $P\in M_{\mathbf{K}}\left( n,n\right) $
invertibile, $\exists $ $D\in M_{\mathbf{K}}\left( n,n\right) :P^{-1}AP=D$.

Analogamente, dato uno spazio vettoriale $V$ su un campo $\mathbf{K}$, sia $%
L:V\rightarrow V$ un'applicazione lineare. $L$ si dice diagonalizzabile se
esiste una base $B$ di $V$ tale che la matrice rappresentativa di $L$
rispetto a $B$ \`{e} diagonale.

Il campo $\mathbf{K}$ ha una ruolo rilevante nella definizione: data $A=%
\left[ 
\begin{array}{cc}
0 & -1 \\ 
1 & 0%
\end{array}%
\right] $, se si considera $A\in M_{\mathbf{%
%TCIMACRO{\U{211a} }%
%BeginExpansion
\mathbb{Q}
%EndExpansion
}}\left( 2,2\right) $ o $M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( 2,2\right) $ $A$ non \`{e} diagonalizzabile, ma lo \`{e} su $%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
$ (si trova una matrice diagonale a coefficienti complessi cui $A$ \`{e}
simile).

Se $A$ \`{e} diagonalizzabile, il rappresentante della classe di equivalenza
di matrici simili cui $A$ appartiene \`{e} la matrice diagonale cui $A$ \`{e}
simile.

\textbf{Def} Sia $V$ uno spazio vettoriale, $L:V\rightarrow V$
un'applicazione lineare. Un vettore $\mathbf{v}\in V$, $\mathbf{v\neq 0}$,
si dice autovettore di $L$ se $\exists $ $\lambda \in \mathbf{K}$, detto
autovalore di $\mathbf{v}$, tale che $L\left( \mathbf{v}\right) =\lambda 
\mathbf{v}$.

Un autovettore di $A\in M_{\mathbf{K}}\left( n,n\right) $ \`{e} un
autovettore dell'applicazione lineare $\tciLaplace _{A}$, quindi $\mathbf{%
v\neq 0}:\exists $ $\lambda \in \mathbf{K}:\tciLaplace _{A}\left( \mathbf{v}%
\right) =A\mathbf{v}=\lambda \mathbf{v}$.

\begin{enumerate}
\item Nell'esempio della simmetria assiale, $\mathbf{v}$ giacente sulla
retta \`{e} un autovettore di $L$ con autovalore $1$, $\mathbf{w}$
ortogonale alla retta \`{e} un autovettore di $L$ con autovalore $-1$ (gli
autovalori sono gli elementi sulla diagonale). Si \`{e} inoltre notato che
esiste una matrice rappresentativa di $L$ diagonale, quindi la matrice $%
\left[ 
\begin{array}{cc}
\cos 2\alpha & \sin 2\alpha \\ 
\sin 2\alpha & -\cos 2\alpha%
\end{array}%
\right] $ \`{e} diagonalizzabile su $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$. La diagonalizzabilit\`{a} \`{e} legata al numero di autovettori.
\end{enumerate}

\textbf{Primo criterio di diagonalizzabilit\`{a}} 
\begin{gather*}
\text{Hp}\text{: }A\in M_{\mathbf{K}}\left( n,n\right) \\
\text{Ts}\text{: }A\text{ \`{e} diagonalizzabile su }\mathbf{K}%
\Longleftrightarrow \text{esiste una base di }\mathbf{K}^{n}\text{ formata
da autovettori di }A
\end{gather*}

e in tal caso ogni matrice che rappresenti $\tciLaplace _{A}$ rispetto a una
base di autovettori di $A$ \`{e} diagonale, con gli autovalori sulla
diagonale: ad essa $A$ \`{e} simile, e la matrice del cambio di base da una
base di autovettori alla base canonica \`{e} formata proprio da tali
autovettori.

\textbf{Dim} (i) Se $A$ \`{e} diagonalizzabile, allora esiste una matrice
diagonale $D=\left[ 
\begin{array}{ccc}
d_{11} & ... & 0 \\ 
... & ... & ... \\ 
0 & ... & d_{33}%
\end{array}%
\right] $ cui $A$ \`{e} simile: due matrici simili rappresentano la stessa
applicazione lineare $\tciLaplace _{A}:\mathbf{K}^{n}\mathbf{\rightarrow K}%
^{n}$, quindi esiste una base $B=\left\{ \mathbf{v}_{1},...,\mathbf{v}%
_{n}\right\} $ di $\mathbf{K}^{n}$ tale che $M_{B}^{B}\left( \tciLaplace
_{A}\right) =D$. Determino l'immagine attraverso $\tciLaplace _{A}$ dei
vettori di $B$: $X_{B}\left( \mathbf{v}_{1}\right) =\left( 
\begin{array}{c}
1 \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right) $, $X_{B}\left( \tciLaplace _{A}\left( \mathbf{v}_{1}\right) \right)
=\left[ 
\begin{array}{ccc}
d_{11} & ... & 0 \\ 
... & ... & ... \\ 
0 & ... & d_{nn}%
\end{array}%
\right] \left( 
\begin{array}{c}
1 \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right) =\left( 
\begin{array}{c}
d_{11} \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right) =d_{11}X_{B}\left( \mathbf{v}_{1}\right) $, quindi $\mathbf{v}_{1}$ 
\`{e} un autovettore di $A$ con autovalore $d_{11}$. Lo stesso vale per ogni
altro vettore della base: per ogni $i=1,...,n$, $\mathbf{v}_{i}$ \`{e} un
autovettore di $A$ con autovalore $d_{ii}$. Quindi esiste una base di $%
\mathbf{K}^{n}$ formata da autovettori di $A$: \`{e} proprio la base $B$
rispetto a cui la matrice diagonale rappresenta $\tciLaplace _{A}$.

(ii) Esiste per ipotesi una base di $\mathbf{K}^{n}$ formata da autovettori
di $A$: la chiamo $B=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} $, e
per ogni $i=1,...,n$ $\tciLaplace _{A}\left( \mathbf{v}_{i}\right) =\lambda
_{i}\mathbf{v}_{i}$. Mi chiedo com'\`{e} fatta la matrice rappresentativa di 
$\tciLaplace _{A}$ rispetto a tale base $M_{B}^{B}\left( \tciLaplace
_{A}\right) $. 
\begin{equation*}
M_{B}^{B}\left( \tciLaplace _{A}\right) =\left[ 
\begin{array}{cccc}
\lambda _{1} & 0 & ... & 0 \\ 
0 & \lambda _{2} & ... & 0 \\ 
... & ... & ... & ... \\ 
0 & 0 & 0 & \lambda _{n}%
\end{array}%
\right]
\end{equation*}

perch\'{e} $\tciLaplace _{A}\left( \mathbf{v}_{i}\right) =\lambda _{i}%
\mathbf{v}_{i}$, quindi $X_{B}\left( \tciLaplace _{A}\left( \mathbf{v}%
_{1}\right) \right) =\left( 
\begin{array}{c}
\lambda _{1} \\ 
0 \\ 
... \\ 
0%
\end{array}%
\right) $. Poich\'{e} esiste una matrice rappresentativa diagonale di $%
\tciLaplace _{A}$, esiste una matrice diagonale cui $A$ \`{e} simile: $%
\exists $ $P$ invertibile tale che $P^{-1}AP=D$, perci\`{o} $A$ \`{e}
diagonalizzabile. $\blacksquare $

Da questo teorema si evince che per studiare la diagonalizzabilit\`{a} di
una matrice si devono studiare i suoi autovettori.

\begin{enumerate}
\item Considero $L:\mathbf{K}\left[ t\right] _{\leq 3}\rightarrow \mathbf{K}%
\left[ t\right] _{\leq 3}$, $L\left( p\left( t\right) \right) =tp^{\prime
\prime }\left( t\right) -p^{\prime }\left( t\right) $ e la base canonica $%
B=\left\{ 1,t,t^{2},t^{3}\right\} $.

Scrivo $M_{B}^{B}\left( L\right) $:%
\begin{equation*}
M_{B}^{B}\left( L\right) =A=\left[ 
\begin{array}{cccc}
0 & -1 & 0 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 3 \\ 
0 & 0 & 0 & 0%
\end{array}%
\right]
\end{equation*}

$A$ non \`{e} diagonale; \`{e} diagonalizzabile? Per il primo criterio, la
diagonalizzabilit\`{a} \`{e} equivalente all'esistenza di una base di $%
\mathbf{K}\left[ t\right] _{\leq 3}$ (o $\mathbf{K}^{4}$) formata da
autovettori di $A$.

Se ragiono su $\mathbf{K}\left[ t\right] _{\leq 3}$ per trovare gli
autovettori, significa che cerco $\lambda \in \mathbf{K}$ e $p\left(
t\right) =a_{0}+a_{1}t+a_{2}t^{2}+a_{3}t^{3}:L\left( p\left( t\right)
\right) =\lambda p\left( t\right) \Longleftrightarrow tp^{\prime \prime
}\left( t\right) -p^{\prime }\left( t\right) =\lambda p\left( t\right) $.
Sto cercando simultaneamente autovalori e autovettori: ho parametri che
ricoprono una funzione diversa.

Se invece ragiono su $\mathbf{K}^{4}$, sto cercando l'autovalore $\lambda
\in \mathbf{K}$ e l'autovettore $\left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] \in \mathbf{K}^{4}:A\left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] =\lambda \left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] $. Voglio scrivere questa uguaglianza nella forma consueta di
rappresentazione di un sistema lineare: allora scrivo%
\begin{eqnarray*}
A\left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] &=&\lambda \left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] \Longleftrightarrow A\left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] =\lambda \left( Id_{4}\left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] \right) \\
A\left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] &=&\left( \lambda Id_{4}\right) \left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] \Longleftrightarrow A\left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] -\left( \lambda Id_{4}\right) \left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] =0 \\
\left( A-\lambda Id_{4}\right) \left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] &=&0
\end{eqnarray*}

Quindi ho un sistema lineare omogeneo con matrice dei coefficienti $%
A-\lambda Id_{4}$: trovare autovalori e autovettori di $A$ si riduce a
studiare le soluzioni di tale sistema. Ricordo che affinch\'{e} $\mathbf{v}=%
\left[ 
\begin{array}{c}
a_{0} \\ 
a_{1} \\ 
a_{2} \\ 
a_{3}%
\end{array}%
\right] $ sia un autovettore dev'essere $\mathbf{v\neq 0}$: il sistema
omogeneo non ammette solo la soluzione nulla (cio\`{e} esiste un
autovettore) se e solo se $r\left( A-\lambda Id_{4}\right)
<n\Longleftrightarrow \det \left( A-\lambda Id_{4}\right) =0$.
\end{enumerate}

In generale, $\lambda $ \`{e} autovalore di $A\Longleftrightarrow r\left(
A-\lambda Id\right) <n\Longleftrightarrow \det \left( A-\lambda Id\right) =0$
$\Longleftrightarrow $ $A-\lambda Id$ non \`{e} invertibile.

\textbf{Def} Data una matrice $A\in M_{\mathbf{K}}\left( n,n\right) $, il
polinomio $p_{A}\left( x\right) =\det \left( A-xId_{n}\right) $ si dice
polinomio caratteristico di $A$ e l'equazione $p_{A}\left( x\right) =0$ si
dice equazione caratteristica di $A$.

Il polinomio caratteristico \`{e} di grado al pi\`{u} $n$.

\textbf{Proposizione}%
\begin{eqnarray*}
\text{Hp} &\text{: }&A\in M_{\mathbf{K}}\left( n,n\right) \\
\text{Ts} &\text{: }&\lambda \in \mathbf{K}\text{ \`{e} un autovalore di }%
A\Longleftrightarrow p_{A}\left( \lambda \right) =0
\end{eqnarray*}

cio\`{e} $\lambda $ \`{e} un autovalore di $A$ se e solo se \`{e} soluzione
dell'equazione caratteristica.

Quindi la sequenza di passaggi da fare per trovare gli autovettori e gli
autovalori di un'applicazione lineare \`{e}:

\begin{description}
\item[-] scriverne una matrice rappresentativa

\item[-] determinare il polinomio caratteristico e le sue radici

\item[-] calcolare le soluzioni del sistema omogeneo sostituendo a $\lambda $
i valori trovati al punto precedente
\end{description}

\begin{enumerate}
\item Riprendo l'esempio di prima: $p_{A}\left( x\right) =\det \left(
A-\lambda Id_{4}\right) =\det \left[ 
\begin{array}{cccc}
-x & -1 & 0 & 0 \\ 
0 & -x & 0 & 0 \\ 
0 & 0 & -x & 3 \\ 
0 & 0 & 0 & -x%
\end{array}%
\right] =x^{4}$ perch\'{e} $A-\lambda Id_{4}$ \`{e} triangolare superiore. C'%
\`{e} un'unica soluzione dell'equazione caratteristica: $\lambda =0$.
Determino gli autovettori risolvendo il sistema $A\mathbf{x=0}$: se un
autovalore \`{e} nullo, gli elementi del nucleo di $A$ sono autovettori
rispetto all'autovalore nullo: quindi esiste l'autovalore $0$ se e solo se
il nucleo non contine solo il vettore nullo, cio\`{e} se e solo se $r\left(
A\right) <n$. Trovo che $\ker \left( A\right) =Span\left( \left[ 
\begin{array}{c}
1 \\ 
0 \\ 
0 \\ 
0%
\end{array}%
\right] ,\left[ 
\begin{array}{c}
0 \\ 
0 \\ 
1 \\ 
0%
\end{array}%
\right] \right) $ \`{e} l'insieme degli autovettori di $A$. Esistono inoltre
due autovettori di $A$ linearmente indipendenti, che \`{e} il massimo numero
di autovettori linearmente indipendenti di $A$, essendo $\dim \left( \ker
\left( A\right) \right) =2$. Questo significa che non esiste una base di $%
\mathbf{K}^{4}$ formata da autovettori di $A$, perch\'{e} non si possono
trovare quattro vettori di $A$ linearmente indipendenti: quindi $A$ non \`{e}
diagonalizzabile.

\item Data $A\in M_{\mathbf{K}}\left( n,n\right) $, $A^{T}$ ha lo stesso
polinomio caratteristico di $A$: $\det \left( A^{T}-\lambda Id\right) =\det
\left( A-\lambda Id\right) $. Infatti, posto $B=A-\lambda Id$, $\det B=\det
\left( A-\lambda Id\right) =\det B^{T}$, e $\det B^{T}=\det \left( A-\lambda
Id\right) ^{T}=\det \left( A^{T}-\lambda Id^{T}\right) =\det \left(
A^{T}-\lambda Id\right) $ per linearit\`{a} della trasposizione.
\end{enumerate}

\textbf{Def} Sia $A\in M_{\mathbf{K}}\left( n,n\right) $, sia $\lambda $ un
autovalore di $A$. Si definisce molteplicit\`{a} algebrica di $\lambda $ $%
\max \left\{ k\in 
%TCIMACRO{\U{2115} }%
%BeginExpansion
\mathbb{N}
%EndExpansion
:\left( x-\lambda \right) ^{k}\text{ divide }p_{A}\left( x\right) \right\} $
e si indica con $a_{\lambda }$ o $a\left( \lambda \right) $. Si definisce
autospazio associato a $\lambda $ il sottospazio vettoriale degli
autovettori con autovalore $\lambda $, cio\`{e} $\left\{ \mathbf{v}\in 
\mathbf{K}^{n}:A\mathbf{v}=\lambda \mathbf{v}\right\} =\left\{ \mathbf{v}\in 
\mathbf{K}^{n}:\left( A-\lambda Id_{n}\right) \mathbf{v}=0\right\} =\ker
\left( A-\lambda Id_{n}\right) $, e si indica con $V_{\lambda }$. Si
definisce molteplicit\`{a} geometrica di $\lambda $ la dimensione
dell'autospazio associato a $\lambda $ $\dim V_{\lambda }$ e si indica con $%
g_{\lambda }$ o $g\left( \lambda \right) $.

Vale $g\left( \lambda \right) =\dim V_{\lambda }=\dim \left( \ker \left(
A-\lambda Id_{n}\right) \right) =n-r\left( A-\lambda Id_{n}\right) $,
quindi, essendo $r\left( A-\lambda Id_{n}\right) <n$ (altrimenti
l'autospazio $V_{\lambda }$ conterrebbe solo il vettore nullo), $g\left(
\lambda \right) \geq 1$. Inoltre $1\leq g\left( \lambda \right) \leq a\left(
\lambda \right) $; se $a\left( \lambda \right) =1$, $\lambda $ si dice
autovalore semplice e $g\left( \lambda \right) =1$. Se $a\left( \lambda
\right) =g\left( \lambda \right) $, $\lambda $ si dice autovalore regolare.
Se un autovalore \`{e} semplice, \`{e} anche regolare.

L'autospazio associato a $\lambda $ \`{e} l'insieme degli autovettori con
autovalore $\lambda $, pi\`{u} il vettore nullo. L'insieme degli autovettori
con autovalore $\lambda $ \`{e} un sottospazio vettoriale perch\'{e}, se $%
\mathbf{v}_{1}\mathbf{,v}_{2}$ sono autovettori con autovalore $\lambda $,
anche $\mathbf{v}_{1}\mathbf{+v}_{2}$ \`{e} autovettore con autovalore $%
\lambda $: $A\left( \mathbf{v}_{1}\mathbf{+v}_{2}\right) =A\mathbf{v}_{1}+A%
\mathbf{v}_{2}=\lambda \mathbf{v}_{1}+\lambda \mathbf{v}_{2}=\lambda \left( 
\mathbf{v}_{1}\mathbf{+v}_{2}\right) $; se $\mathbf{v}_{1}$ \`{e}
autovettore con autovalore $\lambda $, anche $t\mathbf{v}_{1}$ \`{e}
autovettore con autovalore $\lambda $: $A\left( t\mathbf{v}_{1}\right) =tA%
\mathbf{v}_{1}=t\lambda \mathbf{v}_{1}=\lambda \left( t\mathbf{v}_{1}\right) 
$. In effetti \`{e} ovvio perch\'{e} $V_{\lambda }=\ker \left( A-\lambda
Id_{n}\right) $.

\begin{enumerate}
\item Nell'esempio di prima, la molteplicit\`{a} algebrica dell'autovalore $%
\lambda =0$ \`{e} $a_{0}=4$ e la molteplicit\`{a} geometrica dell'autospazio
associato a $\lambda =0$ \`{e} $\dim \left( V_{0}\right) =g_{0}=2$.

\item Data una matrice $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}(3,3)$ il cui polinomio caratteristico \`{e} $p_{A}(x)=-x^{3}+x$, allora si
pu\`{o} dire che la matrice $A+kId$ \`{e} invertibile per ogni $k\neq 0,1,-1$%
. Infatti, $A+kId$ \`{e} invertibile $\Longleftrightarrow $ $r\left(
A+kId\right) =3\Longleftrightarrow \dim \left( \ker \left( A+kId\right)
\right) =0$, cio\`{e} l'autospazio relativo all'autovalore $-k$ contiene
solo il vettore nullo (quindi $-k$ non \`{e} un autovalore di $A$). Questo
accade quando $k\neq 0,1,-1$, gli opposti degli autovalori di $A$.
\end{enumerate}

In generale, $\lambda $ \`{e} autovalore di $A\Longleftrightarrow r\left(
A-\lambda Id\right) <n\Longleftrightarrow \det \left( A-\lambda Id\right) =0$
$\Longleftrightarrow $ $A-\lambda Id$ non \`{e} invertibile.

Se $\tciLaplace _{A}:V\rightarrow V$ \`{e} un'applicazione lineare con $\dim
V\geq 1$, $A$ ha almeno un autovettore non nullo.

Una matrice triangolare $A$ con tutti zeri sulla diagonale e almeno un
elemento non nullo non \`{e} diagonalizzabile, perch\'{e} ha come radice del
polinomio caratteristico $0:a_{0}=n$, ma $r\left( A\right) \geq 1$.

\textbf{Secondo criterio di diagonalizzabilit\`{a}}%
\begin{gather*}
A\in M_{\mathbf{K}}\left( n,n\right) \text{ \`{e} diagonalizzabile su }%
\mathbf{K}\text{ }\Longleftrightarrow \\
\text{(i) }p_{A}\left( x\right) =\det \left( A-\lambda Id_{n}\right) \text{
ha }n\text{ radici in }\mathbf{K}\text{ contate con molteplicit\`{a}, i. e. }%
a\left( \lambda _{1}\right) +a\left( \lambda _{2}\right) +...=n \\
\text{(ii) per ogni autovalore molteplicit\`{a} algebrica e geometrica } \\
\text{coincidono, i. e. }a\left( \lambda _{i}\right) =g\left( \lambda
_{i}\right) \text{ per ogni }i
\end{gather*}

La seconda tesi si pu\`{o} esprimere dicendo che ogni autovalore \`{e}
regolare. Per il teorema fondamentale dell'algebra, che afferma che ogni
polinomio di grado $n$ a coefficienti in $%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
$ ha esattamente $n$ radici contate con molteplicit\`{a}, la prima
condizione \`{e} sempre soddisfatta se $\mathbf{K}=%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
$.

\textbf{Dim} Si dimostra solo l'implicazione da destra a sinistra: la
dimostrazione \`{e} divisa in quattro parti.

1) Per ogni autovalore $\lambda $, vale $1\leq g\left( \lambda \right) \leq
a\left( \lambda \right) $ (la seconda disuguaglianza \`{e} data per
dimostrata). Se $g\left( \lambda _{i}\right) \leq a\left( \lambda
_{i}\right) $ per ogni $i=1,...,s$ (per ogni autovalore $\lambda _{i}$),
supponendo di avere $s$ autovalori, vale $\sum_{i=1}^{s}g\left( \lambda
_{i}\right) \leq \sum_{i=1}^{s}a\left( \lambda _{i}\right) =a\left( \lambda
_{1}\right) +a\left( \lambda _{2}\right) +...+a\left( \lambda _{s}\right)
\leq n$ perch\'{e} il polinomio caratteristico non pu\`{o} avere pi\`{u} di $%
n$ radici; questo \`{e} vero in generale. Ma poich\'{e} per ipotesi $a\left(
\lambda _{i}\right) =g\left( \lambda _{i}\right) $ per ogni $i$ e $a\left(
\lambda _{1}\right) +a\left( \lambda _{2}\right) +...=n$, le due
disuguaglianze diventano due uguaglianze: $g\left( \lambda _{1}\right)
+...+g\left( \lambda _{s}\right) =a\left( \lambda _{1}\right) +...+a\left(
\lambda _{s}\right) =n$, cio\`{e} la somma delle dimensioni di ciascun
autospazio \`{e} $n$.

2) Dimostro la proposizione%
\begin{eqnarray*}
\text{Hp}\text{: } &&\lambda _{1},\lambda _{2}\text{ sono due autovalori di }%
A\text{ tali che }\lambda _{1}\neq \lambda _{2} \\
\text{Ts}\text{: } &&V_{\lambda _{1}}\cap V_{\lambda _{2}}=\left\{ \mathbf{0}%
\right\}
\end{eqnarray*}

cio\`{e} se $\lambda _{1}$ e $\lambda _{2}$ sono due autovalori distinti,
allora $V_{\lambda _{1}}\cap V_{\lambda _{2}}=\left\{ \mathbf{0}\right\} $. $%
\mathbf{v}\in \left( V_{\lambda _{1}}\cap V_{\lambda _{2}}\right)
\Longleftrightarrow \mathbf{v}\in V_{\lambda _{1}}$, $\mathbf{v}\in
V_{\lambda _{2}}$: significa che $\mathbf{v}$ \`{e} un autovettore rispetto
a due autovalori diversi, cio\`{e} $A\mathbf{v}=\lambda _{1}\mathbf{v}$ e $A%
\mathbf{v}=\lambda _{2}\mathbf{v}$. Quindi vale $\lambda _{1}\mathbf{v}%
=\lambda _{2}\mathbf{v}\Longleftrightarrow \left( \lambda _{1}-\lambda
_{2}\right) \mathbf{v=0}$: per la legge di annullamento del prodotto,
essendo $\lambda _{1}-\lambda _{2}\neq 0$, vale $\mathbf{v=0}$. Poich\'{e} $%
V_{\lambda _{1}}\cap V_{\lambda _{2}}=\left\{ \mathbf{0}\right\} $, $%
V_{\lambda _{1}}+V_{\lambda _{2}}=V_{\lambda _{1}}\oplus V_{\lambda _{2}}$,
quindi $\dim \left( V_{\lambda _{1}}+V_{\lambda _{2}}\right) =\dim
V_{\lambda _{1}}+\dim V_{\lambda _{2}}=g\left( \lambda _{1}\right) +g\left(
\lambda _{2}\right) $.

3) Dimostro per induzione la proposizione 
\begin{eqnarray*}
\text{Hp}\text{: } &&\lambda _{1},\lambda _{2},...,\lambda _{s}\text{ sono
autovalori distinti di }A\text{, }\mathbf{v}_{1}\in V_{\lambda _{1}},...,%
\mathbf{v}_{s}\in V_{\lambda _{s}} \\
\text{Ts}\text{: } &&\left\{ \mathbf{v}_{1}\in V_{\lambda _{1}},...,\mathbf{v%
}_{s}\in V_{\lambda _{s}}\right\} \text{ \`{e} un insieme di vettori
linearmente indipendenti}
\end{eqnarray*}

cio\`{e} se $\lambda _{1},\lambda _{2},...,\lambda _{s}$ sono autovalori
distinti e se considero un autovettore per ogni autospazio, allora tali
autovettori sono linearmente indipendenti. Inizio con il caso base,
mostrando che, se $\lambda _{1}\neq \lambda _{2}$, $\left\{ \mathbf{v}%
_{1}\in V_{\lambda _{1}},\mathbf{v}_{2}\in V_{\lambda _{2}}\right\} $ \`{e}
un insieme di vettori linearmente indipendenti: voglio mostrare che $t_{1}%
\mathbf{v}_{1}+t_{2}\mathbf{v}_{2}\mathbf{=0}$ implica $t_{1}=t_{2}=0$. [In
realt\`{a} l'indipendenza lineare di $\mathbf{v}_{1}\in V_{\lambda _{1}},%
\mathbf{v}_{2}\in V_{\lambda _{2}}$ segue dal fatto che $V_{\lambda
_{1}}\cap V_{\lambda _{2}}=\left\{ \mathbf{0}\right\} $: se per assurdo
fosse $\mathbf{v}_{1}=t\mathbf{v}_{2}$, allora ciascuno dei due vettori
sarebbe autovettore sia rispetto a $\lambda _{1}$ che rispetto a $\lambda
_{2}$ e l'intersezione conterrebbe anche loro due: $L\left( \mathbf{v}%
_{1}\right) =\lambda _{1}\mathbf{v}_{1}$ e $L\left( \mathbf{v}_{1}\right)
=L\left( t\mathbf{v}_{2}\right) =tL\left( \mathbf{v}_{2}\right) =t\left(
\lambda _{2}\mathbf{v}_{2}\right) =\lambda _{2}\left( t\mathbf{v}_{2}\right)
=\lambda _{2}\mathbf{v}_{1}$] Per sfruttare il fatto che $\mathbf{v}_{1}%
\mathbf{,v}_{2}$ sono autovettori, applico $\tciLaplace _{A}$ a lato
sinistro e destro: $A\left( t_{1}\mathbf{v}_{1}+t_{2}\mathbf{v}_{2}\right) =A%
\mathbf{0}\Longleftrightarrow t_{1}\lambda _{1}\mathbf{v}_{1}+t_{2}\lambda
_{2}\mathbf{v}_{2}=\mathbf{0}$. Voglio avere $\lambda _{1}$ e $\lambda _{2}$
nello stesso coefficiente, quindi riscrivo l'equazione come $t_{1}\lambda
_{1}\mathbf{v}_{1}+t_{2}\lambda _{2}\mathbf{v}_{2}-\lambda _{2}\mathbf{0}=%
\mathbf{0}$ e uso l'ipotesi $t_{1}\mathbf{v}_{1}+t_{2}\mathbf{v}_{2}\mathbf{%
=0}$: sostituendo ottengo $t_{1}\lambda _{1}\mathbf{v}_{1}+t_{2}\lambda _{2}%
\mathbf{v}_{2}-\lambda _{2}\left( t_{1}\mathbf{v}_{1}+t_{2}\mathbf{v}%
_{2}\right) =\mathbf{0\Longleftrightarrow }\left( \lambda _{1}-\lambda
_{2}\right) t_{1}\mathbf{v}_{1}=\mathbf{0}$. $\lambda _{1}\neq \lambda _{2}$
per ipotesi, $\mathbf{v}_{1}\neq \mathbf{0}$ perch\'{e} \`{e} un
autovettore, quindi pu\`{o} essere solo $t_{1}=0$, per la legge di
annullamento del prodotto. Allora, se $t_{1}\mathbf{v}_{1}+t_{2}\mathbf{v}%
_{2}\mathbf{=0}$ e $t_{1}=0$, di nuovo per la legge di annullamento del
prodotto, essendo $\mathbf{v}_{2}\neq \mathbf{0}$, $t_{2}=0$.

Allora, prendendo come ipotesi induttiva l'indipendenza lineare in $\left\{ 
\mathbf{v}_{1}\mathbf{,...,v}_{k}\right\} $, si mostra l'indipendenza
lineare dei vettori di $\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{k},\mathbf{v}%
_{k+1}\right\} $: voglio mostrare che $t_{1}\mathbf{v}_{1}+...+t_{k}\mathbf{v%
}_{k}+t_{k+1}\mathbf{v}_{k+1}\mathbf{=0}$ implica $t_{1}=...=t_{k}=t_{k+1}=0$%
. Per sfruttare il fatto che $\mathbf{v}_{1}\mathbf{,...,v}_{k+1}$ sono
autovettori, applico $\tciLaplace _{A}$ a lato sinistro e destro: $A\left(
t_{1}\mathbf{v}_{1}+...+t_{k}\mathbf{v}_{k}+t_{k+1}\mathbf{v}_{k+1}\right) =A%
\mathbf{0}\Longleftrightarrow t_{1}\lambda _{1}\mathbf{v}_{1}+...+t_{k}%
\lambda _{k}\mathbf{v}_{k}+t_{k+1}\lambda _{k+1}\mathbf{v}_{k+1}=\mathbf{0}$%
. Come prima, riscrivo l'equazione come $t_{1}\lambda _{1}\mathbf{v}%
_{1}+...+t_{k}\lambda _{k}\mathbf{v}_{k}+t_{k+1}\lambda _{k+1}\mathbf{v}%
_{k+1}-\lambda _{k+1}\mathbf{0}=\mathbf{0}$ e uso l'ipotesi $t_{1}\mathbf{v}%
_{1}+...+t_{k}\mathbf{v}_{k}+t_{k+1}\mathbf{v}_{k+1}\mathbf{=0}$:
sostituendo ottengo $t_{1}\lambda _{1}\mathbf{v}_{1}+...+t_{k}\lambda _{k}%
\mathbf{v}_{k}+t_{k+1}\lambda _{k+1}\mathbf{v}_{k+1}-\lambda _{k+1}\left(
t_{1}\mathbf{v}_{1}+...+t_{k}\mathbf{v}_{k}+t_{k+1}\mathbf{v}_{k+1}\right) =%
\mathbf{0\Longleftrightarrow }\left( \lambda _{1}-\lambda _{k+1}\right) t_{1}%
\mathbf{v}_{1}+...+\left( \lambda _{k}-\lambda _{k+1}\right) t_{k}\mathbf{v}%
_{k}=\mathbf{0}$. Per ipotesi induttiva $\mathbf{v}_{1}\mathbf{,...,v}_{k}$
sono indipendenti, quindi $\left( \lambda _{1}-\lambda _{k+1}\right)
t_{1}=...=\left( \lambda _{k}-\lambda _{k+1}\right) t_{k}=0$, ma gli
autovalori sono tutti distinti, quindi pu\`{o} essere solo $%
t_{1}=...=t_{k}=0 $. Allora, se $t_{1}\mathbf{v}_{1}+...+t_{k}\mathbf{v}%
_{k}+t_{k+1}\mathbf{v}_{k+1}\mathbf{=0}$ e $t_{1}=...=t_{k}=0$, per la legge
di annullamento del prodotto, essendo $\mathbf{v}_{k+1}\mathbf{\neq 0}$, $%
t_{k+1}=0$.

4) Dimostro la proposizione%
\begin{eqnarray*}
\text{Hp}\text{: } &&B_{1}\text{ \`{e} una base dell'autospazio }V_{1}\text{%
,..., }B_{s}\text{ \`{e} una base di }V_{s} \\
\text{Ts}\text{: } &&B_{1}\cup ...\cup B_{s}\text{ \`{e} un insieme di
vettori di }A\text{ linearmente indipendenti}
\end{eqnarray*}

cio\`{e} che posso costruire un insieme di autovettori di $A$ linearmente
indipendenti con cardinalit\`{a} $g\left( \lambda _{1}\right) +g\left(
\lambda _{2}\right) +...+g\left( \lambda _{s}\right) $. Questo si pu\`{o}
fare sempre; in questo caso significher\`{a} che si pu\`{o} costruire un
insieme di $n$ autovettori linearmente indipendenti, che costituiranno la
base di $\mathbf{K}^{n}$ cercata. Considero $V_{1}$ e la sua base di
cardinalit\`{a} $g\left( \lambda _{1}\right) $ $B_{1}=\left\{ \mathbf{w}_{11}%
\mathbf{,...,w}_{1g\left( \lambda _{1}\right) }\right\} $,..., $V_{s}$ e la
sua base di cardinalit\`{a} $g\left( \lambda _{s}\right) $ $B_{s}=\left\{ 
\mathbf{w}_{s1}\mathbf{,...,w}_{sg\left( \lambda _{s}\right) }\right\} $.
Voglio mostrare che $B_{1}\cup ...\cup B_{s}$ \`{e} un insieme di vettori
linearmente indipendenti, cio\`{e} che $t_{11}\mathbf{w}_{11}+\mathbf{...}%
+t_{1g\left( \lambda _{1}\right) }\mathbf{w}_{1g\left( \lambda _{1}\right)
}+...+t_{s1}\mathbf{w}_{s1}+\mathbf{...}+t_{sg\left( \lambda _{s}\right) }%
\mathbf{w}_{sg\left( \lambda _{s}\right) }=\mathbf{0}$ implica $%
t_{11}=...=t_{1g\left( \lambda _{1}\right) }=...=t_{s1}=...=t_{sg\left(
\lambda _{s}\right) }=0$. Pongo $\mathbf{v}_{i}=$ $t_{i1}\mathbf{w}_{i1}+%
\mathbf{...}+t_{ig\left( \lambda _{i}\right) }\mathbf{w}_{ig\left( \lambda
_{i}\right) }$, che \`{e} una generica combinazione lineare dei vettori
della base di $V_{i}$, quindi un generico vettore dell'autospazio $V_{i}$.
L'equazione precedente diventa $\mathbf{v}_{1}\mathbf{+...+v}_{s}=\mathbf{0}$%
. Ma $\mathbf{v}_{1}\mathbf{,...,v}_{s}$ sono $s$ vettori provenienti da $s$
autospazi diversi, quindi sono linearmente indipendenti per quanto visto al
punto 3. Dunque $\mathbf{v}_{1}\mathbf{+...+v}_{s}=\mathbf{0}$ pu\`{o}
essere vera se e solo se $\mathbf{v}_{1}\mathbf{,...,v}_{s}$ sono tutti
nulli (altrimenti si potrebbe scrivere una relazione di dipendenza lineare
tra di loro), perci\`{o} $\mathbf{v}_{1}\mathbf{=...=v}_{s}\mathbf{=0}$.
Perci\`{o} per ogni $i$ $\mathbf{v}_{i}=$ $t_{i1}\mathbf{w}_{i1}+\mathbf{...}%
+t_{ig\left( \lambda _{i}\right) }\mathbf{w}_{ig\left( \lambda _{i}\right) }=%
\mathbf{0}$: ma $\left\{ \mathbf{w}_{i1},...,\mathbf{w}_{ig\left( \lambda
_{i}\right) }\right\} $ costituiscono una base di $V_{i}$, quindi sono
linearmente indipendenti e $t_{i1}\mathbf{=...}=t_{ig\left( \lambda
_{i}\right) }=0$ per ogni $i=1,...,s$.

Si \`{e} quindi mostrato che se per ogni autospazio costruisco una base e
metto insieme tutti i vettori di tutte le basi, questi vettori sono
linearmente indipendenti.

In conclusione, poich\'{e} posso costruire un insieme di $n$ autovettori di $%
A$ linearmente indipendenti, questi costituiscono una base di $\mathbf{K}%
^{n} $ e $A$ \`{e} diagonalizzabile per il primo criterio di
diagonalizzabilit\`{a}. $\blacksquare $

Nel punto (ii) si \`{e} visto che $\lambda _{1},\lambda _{2}$ sono
autovalori distinti allora $V_{\lambda _{1}}\cap V_{\lambda _{2}}=\left\{ 
\mathbf{0}\right\} $ e $V_{\lambda _{1}}+V_{\lambda _{2}}=V_{\lambda
_{1}}\oplus V_{\lambda _{2}}$. La somma \`{e} diretta qualunque numero di
autospazi si consideri. Infatti, dati $\lambda _{1},\lambda _{2},\lambda
_{3} $ tutti distinti, $V_{\lambda _{1}}+V_{\lambda _{2}}+V_{\lambda
_{3}}=\left( V_{\lambda _{1}}+V_{\lambda _{2}}\right) +V_{\lambda
_{3}}=\left( V_{\lambda _{1}}\oplus V_{\lambda _{2}}\right) +V_{\lambda
_{3}} $. Per quanto visto in (iv) $\dim \left( V_{\lambda _{1}}+V_{\lambda
_{2}}+V_{\lambda _{3}}\right) =g_{\lambda _{1}}+g_{\lambda _{2}}+g_{\lambda
_{3}}$; $\dim \left( V_{\lambda _{1}}\oplus V_{\lambda _{2}}\right)
=g_{\lambda _{1}}+g_{\lambda _{2}}$. Per la formula di Grassman,
considerando $A=V_{\lambda _{1}}\oplus V_{\lambda _{2}}$ e $B=V_{\lambda
_{3}}$, $\dim \left( \left( V_{\lambda _{1}}\oplus V_{\lambda _{2}}\right)
\cap V_{\lambda _{3}}\right) =g_{\lambda _{1}}+g_{\lambda _{2}}+g_{\lambda
_{3}}-\left( g_{\lambda _{1}}+g_{\lambda _{2}}\right) -g_{\lambda _{3}}=0$,
quindi $V_{\lambda _{1}}+V_{\lambda _{2}}+V_{\lambda _{3}}=V_{\lambda
_{1}}\oplus V_{\lambda _{2}}\oplus V_{\lambda _{3}}$.

Dunque, se $A$ \`{e} diagonalizzabile su $\mathbf{K}$, $\dim \left(
V_{\lambda _{1}}+V_{\lambda _{2}}+...+V_{\lambda _{s}}\right) =\dim \left(
V_{\lambda _{1}}\oplus V_{\lambda _{2}}\oplus ...\oplus V_{\lambda
_{s}}\right) =n$ e $\mathbf{K}^{n}$ si pu\`{o} decomporre in sottospazi
invarianti, cio\`{e} si pu\`{o} decomporre in sottospazi%
\begin{equation*}
\mathbf{K}^{n}=V_{\lambda _{1}}\oplus V_{\lambda _{2}}\oplus ...\oplus
V_{\lambda _{s}}
\end{equation*}

che sono invarianti per $\tciLaplace _{A}$ perch\'{e} per ogni sottospazio $%
V_{\lambda _{i}}$, per ogni $\mathbf{v}\in V_{\lambda _{i}}$, vale $A\mathbf{%
v}=\lambda _{i}\mathbf{v}\in V_{\lambda _{i}}$: $V_{\lambda _{i}}$ \`{e}
invariante rispetto all'azione dell'applicazione lineare $\tciLaplace _{A}$.

Quindi per applicazioni lineari la ricerca di autovalori e autovettori
coincide con la ricerca dei sottospazi invarianti.

\begin{enumerate}
\item Se $A=P^{-1}DP$ con $D$ diagonale, si dice che $P$ diagonalizza $A$.
Esistono infinite matrici che diagonalizzano $A$, perch\'{e} ogni matrice le
cui colonne siano una base di autovettori di $A$ la diagonalizzano (una
volta trovata una base, posso prenderne qualsiasi multiplo).

\item Se $A$ \`{e} una matrice tale che la somma degli elementi su ogni
colonna $i$ \`{e} uguale ad $a$, allora $a$ \`{e} autovalore di $A$. Infatti
in tal caso $A^{T}$ \`{e} tale che la somma degli elementi su ciascuna riga 
\`{e} $a$: allora $A^{T}\left( 
\begin{array}{c}
1 \\ 
... \\ 
1%
\end{array}%
\right) =C_{1}\left( A\right) +...+C_{n}\left( A\right) =\left( 
\begin{array}{c}
a \\ 
... \\ 
a%
\end{array}%
\right) $. Poich\'{e} $A$ e $A^{T}$ hanno lo stesso polinomio
caratteristico, $a$ \`{e} autovalore di $A$.
\end{enumerate}

\textbf{Teorema (propriet\`{a} invarianti per
similitudine)}%
\begin{gather*}
\text{Hp}\text{: }A,B\text{ sono matrici simili} \\
\text{Ts}\text{: (i) }A,B\text{ hanno lo stesso polinomio caratteristico: }%
p_{A}\left( x\right) =p_{B}\left( x\right) \\
\text{(ii) }A,B\text{ hanno la stessa traccia e lo stesso determinante: }%
\det A=\det B\text{, }tr\left( A\right) =tr\left( B\right) \\
\text{(iii) }A,B\text{ hanno gli stessi autovalori con le stesse molteplicit%
\`{a} algebriche e geometriche} \\
\text{(iv) }A,B\text{ hanno lo stesso rango}
\end{gather*}

\textbf{Dim} Per ipotesi $\exists $ $P$ invertibile tale che $B=P^{-1}AP$.

(i) $p_{B}\left( x\right) =\det \left( B-xId\right) $. Scrivo $%
B-xId=P^{-1}AP-x\left( P^{-1}P\right) =P^{-1}AP-P^{-1}\left( xId\right)
P=P^{-1}\left( A-xId\right) P$. Allora $\det \left( B-xId\right) =\det
\left( P^{-1}\left( A-xId\right) P\right) =\det P^{-1}\det \left(
A-xId\right) \det P=\det P^{-1}P\det \left( A-xId\right) =\det \left(
A-xId\right) =p_{A}\left( x\right) $, usando due volte il teorema di Binet,
quindi i due polinomi caratteristici coincidono.

(ii) $\det B=\det \left( P^{-1}AP\right) =\det \left( P^{-1}P\right) \det
A=\det A$, usando due volte il teorema di Binet. Si pu\`{o} dimostrare anche
sfruttando l'uguaglianza dei polinomi caratteristici: $\det B=p_{B}\left(
0\right) =p_{A}\left( 0\right) =\det A$.

$tr\left( B\right) $ \`{e} il coefficiente del termine di grado $n-1$ di $%
p_{B}\left( x\right) $: $p_{B}\left( x\right) =\left( -1\right)
^{n}x^{n}+\left( -1\right) ^{n-1}tr\left( B\right) x^{n-1}+...+\det B$.
Analogamente $p_{A}\left( x\right) =\left( -1\right) ^{n}x^{n}+\left(
-1\right) ^{n-1}tr\left( A\right) x^{n-1}+...+c_{1}x+\det A$. Poich\'{e} $%
p_{A}\left( x\right) =p_{B}\left( x\right) $, i coefficienti devono
coincidere uno a uno, e in particolare $tr\left( A\right) =tr\left( B\right) 
$.

(iii) Poich\'{e} $p_{A}\left( x\right) =p_{B}\left( x\right) $, i due
polinomi hanno le stesse radici (e quindi autovalori) con la stessa
molteplicit\`{a} algebrica. Mostro che ogni autovalore ha la stessa
molteplicit\`{a} geometrica sia come autovalore di $A$ che come autovalore
di $B$. Se $\mathbf{v}$ \`{e} autovettore di $A$ con autovalore $\lambda $,
allora $P^{-1}\mathbf{v}$ \`{e} autovettore di $B$ con autovalore $\lambda $
($P^{-1}$ \`{e} la matrice del cambio di base da $A$ a $B$: $%
P^{-1}=M_{A}^{B}\left( id\right) $): infatti $B\left( P^{-1}\mathbf{v}%
\right) =\left( BP^{-1}\right) \mathbf{v=}\left( P^{-1}A\right) \mathbf{v=}%
P^{-1}\left( A\mathbf{v}\right) =P^{-1}\lambda \mathbf{v=}\lambda \left(
P^{-1}\mathbf{v}\right) $, sfruttando il fatto che $B=P^{-1}AP%
\Longleftrightarrow BP^{-1}=P^{-1}A$. $B$ non pu\`{o} avere autovettori che $%
A$ non ha, perch\'{e} se $\mathbf{v}$ \`{e} autovettore di $B$, $P\mathbf{v}$
\`{e} autovettore di $A$. Quindi, se $\left\{ \mathbf{v}_{1}\mathbf{,...,v}%
_{g_{\lambda }}\right\} $ \`{e} una base dell'autospazio $V_{\lambda }$ di $%
A $, $\left\{ P^{-1}\mathbf{v}_{1}\mathbf{,...,}P^{-1}\mathbf{v}_{g_{\lambda
}}\right\} $ \`{e} una base dell'autospazio $V_{\lambda }$ di $B$ (hanno
entrambi la stessa dimensione): $P^{-1}\mathbf{v}_{1}\mathbf{,...,}P^{-1}%
\mathbf{v}_{g_{\lambda }}$ sono linearmente indipendenti perch\'{e} $P^{-1}%
\mathbf{x}$ \`{e} un'applicazione lineare iniettiva (\`{e} la matrice del
cambio di base da $A$ a $B$), quindi associa vettori linearmente
indipendenti a vettori linearmente indipendenti. Dunque la molteplicit\`{a}
geometrica dell'autovalore $\lambda $ \`{e} la stessa sia come autovalore di 
$A$ che di $B$, e questo vale per ogni autovalore.

(iv) Due matrici simili rappresentano la stessa applicazione lineare. $%
r\left( B\right) =\dim \left( \func{Im}\left( \tciLaplace _{B}\right)
\right) $ e $r\left( A\right) =\dim \left( \func{Im}\left( \tciLaplace
_{A}\right) \right) $, ma essendo l'applicazione lineare la stessa, la
dimensione dell'immagine dev'essere uguale, cio\`{e} $\dim \left( \func{Im}%
\left( \tciLaplace _{B}\right) \right) $ $=\dim \left( \func{Im}\left(
\tciLaplace _{A}\right) \right) $, quindi $r\left( A\right) =r\left(
B\right) $. $\blacksquare $

Questo teorema permette di capire che si possono avere informazioni sugli
autovalori di una matrice anche senza calcolarli direttamente. Data $A\in M_{%
\mathbf{K}}\left( n,n\right) $, $A$ ha $n$ autovalori complessi $\lambda
_{1},\lambda _{2},...,\lambda _{n}$, e - consderando gli autovalori con
molteplicit\`{a} - vale $\det A=\lambda _{1}\lambda _{2}...\lambda _{n}$ e $%
tr\left( A\right) =\lambda _{1}+\lambda _{2}+...+\lambda _{n}$. Si dimostra
facilmente se $A$ \`{e} diagonalizzabile: considero la sua matrice diagonale 
$D$ ($A,D$ sono simili), $\det D=\lambda _{1}\lambda _{2}...\lambda _{n}$;
ma $\det D=\det A$, quindi $\det A=\lambda _{1}\lambda _{2}...\lambda _{n}$;
analogamente $tr\left( D\right) =\lambda _{1}+\lambda _{2}+...+\lambda _{n}$%
, ma $tr\left( A\right) =tr\left( D\right) $, quindi $tr\left( A\right)
=\lambda _{1}+\lambda _{2}+...+\lambda _{n}$. Perci\`{o} posso ricavare
informazioni direttamente da $A$ sui suoi autovalori anche se $A$ non \`{e}
diagonalizzabile.

\begin{enumerate}
\item $A\in M_{\mathbf{K}}\left( 2,2\right) $. $p_{A}\left( x\right)
=x^{2}-tr\left( A\right) x+\det A$ ha due radici, eventualmente complesse. $%
\lambda _{1},\lambda _{2}$ sono le soluzioni, e coerentemente con le
relazioni note tra le soluzioni di un'equazione di secondo grado ($%
x_{1}+x_{2}=\frac{-b}{a}$, $x_{1}x_{2}=\frac{c}{a}$), $tr\left( A\right)
=\lambda _{1}+\lambda _{2}$, $\det A=\lambda _{1}\lambda _{2}$. Infatti, se $%
A=\left[ 
\begin{array}{cc}
a & b \\ 
c & d%
\end{array}%
\right] $, $p_{A}\left( x\right) =\det \left( A-xId\right) =\det \left[ 
\begin{array}{cc}
a-x & b \\ 
c & d-x%
\end{array}%
\right] =\left( a-x\right) \left( d-x\right) -bc=$ $x^{2}-\left( a+d\right)
x+ad-bc=\left( -1\right) ^{2}x^{2}+\left( -1\right) ^{1}tr\left( A\right)
x+\det A$.

\item $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ ha ogni elemento uguale a $1$. Per cercarne gli
autovalori \`{e} in questo caso poco comodo usare il polinomio
caratteristico, essendo $n$ generico. Osservo che $r\left( A\right) =1$,
quindi $\dim \left( \ker A\right) =n-1$. $\ker \left( A\right) $ \`{e}
l'autospazio relativo all'autovalore $\lambda =0$, quindi l'autovalore nullo
ha molteplicit\`{a} geometrica $g_{0}=n-1$. Essendo $a_{0}\geq g_{0}=n-1$,
sappiamo che $0$ \`{e} una radice del polinomio caratteristico con
molteplicit\`{a} algebrica almeno pari a $n-1$. L'n-esima radice potrebbe
essere $0$ oppure no (comunque \`{e} reale perch\'{e} le radici complesse
sono sempre a coppie). Per cercare l'ultima radice, che \`{e} un autovalore,
uso il fatto che $tr\left( A\right) =n=\lambda _{1}+...+\lambda
_{n}=0+\lambda _{n}$, quindi $\lambda _{n}=n$; $a_{\lambda
_{n}}=1=g_{\lambda _{n}}$, autovalore semplice e regolare. Anche $0$ \`{e}
un autovalore regolare. Dunque $A$ \`{e} diagonalizzabile \`{e} la matrice
diagonale \`{e} $D=\left[ 
\begin{array}{cccc}
n & 0 & ... & 0 \\ 
0 & 0 & ... & 0 \\ 
... & ... & ... & ... \\ 
0 & 0 & ... & 0%
\end{array}%
\right] $. In questo caso l'autovalore $n$ ha come autospazio l'immagine
dell'applicazione lineare rappresentata da $A$: se $\mathbf{x}$ \`{e} il
generico vettore dell'immagine, $\mathbf{x}=x_{1}\left( 
\begin{array}{c}
1 \\ 
... \\ 
1%
\end{array}%
\right) +...+x_{n}\left( 
\begin{array}{c}
1 \\ 
... \\ 
1%
\end{array}%
\right) =\left( 
\begin{array}{c}
x_{1}+...+x_{n} \\ 
... \\ 
x_{1}+...+x_{n}%
\end{array}%
\right) $ e $A\left( 
\begin{array}{c}
x_{1}+...+x_{n} \\ 
... \\ 
x_{1}+...+x_{n}%
\end{array}%
\right) =\left( 
\begin{array}{c}
n\left( x_{1}+...+x_{n}\right) \\ 
... \\ 
n\left( x_{1}+...+x_{n}\right)%
\end{array}%
\right) =n\mathbf{x}$.
\end{enumerate}

Se $A$ \`{e} diagonalizzabile, per (iv) $r\left( A\right) $ \`{e} il numero
di autovalori non nulli di $A$. Infatti $A,D$ hanno lo stesso rango, e $%
r\left( D\right) $, dato che $D$ ha gli autovalori sulla diagonale, \`{e} il
numero di autovalori non nulli, ciascuno contato con la sua molteplicit\`{a}
algebrica. Oppure si pu\`{o} pensare che $A$ diagonalizzabile implica $%
a\left( \lambda _{1}\right) +a\left( \lambda _{2}\right) +...+a\left(
\lambda _{s}\right) =n$ e $a\left( \lambda _{i}\right) =g\left( \lambda
_{i}\right) $. $a\left( 0\right) =g\left( 0\right) $ \`{e} la dimensione di $%
\ker \left( A\right) $, che \`{e} $n-r\left( A\right) $, e la somma delle
molteplicit\`{a} algebriche degli autovalori non nulli dev'essere $r\left(
A\right) $.

\begin{enumerate}
\item Considero la successione di Fibonacci $\left\{ F_{n}\right\} $,
definita per ricorrenza con $a_{0}=0$, $a_{1}=1$, $a_{n+2}=a_{n}+a_{n+1}$, e
il vettore $\mathbf{v}_{n}=\left( 
\begin{array}{c}
a_{n} \\ 
a_{n+1}%
\end{array}%
\right) $. Voglio scrivere in forma chiusa, per ogni $n$, $\mathbf{v}_{n}$,
mostrare che $a_{n}=\frac{\phi _{1}^{n}-\phi _{2}^{n}}{\sqrt{5}}$, con $\phi
_{1}=\frac{1+\sqrt{5}}{2}$ e $\phi _{2}=1-\phi _{1}$, e che $%
\lim_{n\rightarrow +\infty }\frac{a_{n+1}}{a_{n}}=\phi _{1}$, la sezione
aurea.

$\mathbf{v}_{0}=\left( 
\begin{array}{c}
0 \\ 
1%
\end{array}%
\right) $. Voglio scrivere $\mathbf{v}_{n}$ usando una matrice: $\mathbf{v}%
_{n}=\left( 
\begin{array}{c}
a_{n} \\ 
a_{n+1}%
\end{array}%
\right) =\left( 
\begin{array}{c}
a_{n} \\ 
a_{n-1}+a_{n}%
\end{array}%
\right) =\left[ 
\begin{array}{cc}
0 & 1 \\ 
1 & 1%
\end{array}%
\right] \left[ 
\begin{array}{c}
a_{n-1} \\ 
a_{n}%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
0 & 1 \\ 
1 & 1%
\end{array}%
\right] \mathbf{v}_{n-1}$. La definizione per ricorrenza della successione 
\`{e} $\mathbf{v}_{n}=\left[ 
\begin{array}{cc}
0 & 1 \\ 
1 & 1%
\end{array}%
\right] \mathbf{v}_{n-1}=A\mathbf{v}_{n-1}$. Essendo $\mathbf{v}_{n-1}=A%
\mathbf{v}_{n-2}$ e cos\`{\i} via, si ha $\mathbf{v}_{n}=A^{n}\mathbf{v}_{0}$%
: per calcolare $\mathbf{v}_{n}$ occorre calcolare $A^{n}$: in questo \`{e}
utile la diagonalizzazione. Infatti, se $A$ \`{e} diagonalizzabile e $%
D=P^{-1}AP$, $A=PDP^{-1}$ e $A^{n}=\left( PDP^{-1}\right) \left(
PDP^{-1}\right) ....\left( PDP^{-1}\right) =PD\left( P^{-1}P\right) D\left(
P^{-1}...\right) ...\left( ...P\right) DP^{-1}=PD^{n}P^{-1}$. Se $D=\left[ 
\begin{array}{ccc}
d_{1} & ... & 0 \\ 
... & ... & ... \\ 
0 & ... & d_{n}%
\end{array}%
\right] $, vale $D^{n}=\left[ 
\begin{array}{ccc}
d_{1}^{n} & ... & 0 \\ 
... & ... & ... \\ 
0 & ... & d_{n}^{n}%
\end{array}%
\right] $, quindi \`{e} molto p\`{u} semplice da calcolare rispetto ad $%
A^{n} $. Quindi mi chiedo se $A$ \`{e} diagonalizzabile e calcolo
eventualmente $D$. $\det \left( A-\lambda Id\right) =\det \left[ 
\begin{array}{cc}
-\lambda & 1 \\ 
1 & 1-\lambda%
\end{array}%
\right] =\left( -\lambda \right) \left( 1-\lambda \right) -1=\lambda
^{2}-\lambda -1$ (potevo anche notare $tr\left( A\right) =1$, $\det A=-1$). $%
\lambda _{1}=\phi _{1}=\frac{1+\sqrt{5}}{2}$, $\lambda _{2}=\phi _{2}=\frac{%
1-\sqrt{5}}{2}$: sono due autovalori semplici e regolari, quindi $A$ \`{e}
diagonalizzabile con matrice diagonale $D=\left[ 
\begin{array}{cc}
\phi _{1} & 0 \\ 
0 & \phi _{2}%
\end{array}%
\right] $ e $D^{n}=\left[ 
\begin{array}{cc}
\phi _{1}^{n} & 0 \\ 
0 & \phi _{2}^{n}%
\end{array}%
\right] $. Cerco una base di ciascun autospazio.

$V_{\phi _{1}}=\ker \left( A-\phi _{1}Id\right) =\ker \left( \left[ 
\begin{array}{cc}
-\phi _{1} & 1 \\ 
1 & 1-\phi _{1}%
\end{array}%
\right] \right) $: so gi\`{a} che il rango della matrice non \`{e} massimo,
altrimenti l'unico vettore in $V_{\phi _{1}}$ sarebbe il vettore nullo
(impossibile per definizione di autovettore). Perci\`{o} la matrice ridotta
a scala dev'essere $\left[ 
\begin{array}{cc}
-\phi _{1} & 1 \\ 
0 & 0%
\end{array}%
\right] $: risolvendo trovo che il generico vettore in $V_{\phi _{1}}$ \`{e} 
$\mathbf{x}=t\left( 
\begin{array}{c}
1 \\ 
\phi _{1}%
\end{array}%
\right) $.

$V_{\phi _{2}}=\ker \left( A-\phi _{2}Id\right) =\ker \left( \left[ 
\begin{array}{cc}
-\phi _{2} & 1 \\ 
1 & 1-\phi _{2}%
\end{array}%
\right] \right) $: la matrice ridotta a scala dev'essere $\left[ 
\begin{array}{cc}
-\phi _{2} & 1 \\ 
0 & 0%
\end{array}%
\right] $; risolvendo trovo che il generico vettore in $V_{\phi _{2}}$ \`{e} 
$\mathbf{x}=t\left( 
\begin{array}{c}
1 \\ 
\phi _{2}%
\end{array}%
\right) $.

Quindi una base di $K^{2}$ formata da autovettori \`{e} $P=\left[ 
\begin{array}{cc}
1 & 1 \\ 
\phi _{1} & \phi _{2}%
\end{array}%
\right] $; $P^{-1}=\frac{1}{\phi _{2}-\phi _{1}}\left[ 
\begin{array}{cc}
\phi _{2} & -1 \\ 
-\phi _{1} & 1%
\end{array}%
\right] =\frac{1}{\sqrt{5}}\left[ 
\begin{array}{cc}
-\phi _{2} & 1 \\ 
\phi _{1} & -1%
\end{array}%
\right] $. Calcolo $\mathbf{v}_{n}=A^{n}\mathbf{v}_{0}=PD^{n}P^{-1}\mathbf{v}%
_{0}=\left[ 
\begin{array}{cc}
1 & 1 \\ 
\phi _{1} & \phi _{2}%
\end{array}%
\right] \left[ 
\begin{array}{cc}
\phi _{1}^{n} & 0 \\ 
0 & \phi _{2}^{n}%
\end{array}%
\right] \frac{1}{\sqrt{5}}\left[ 
\begin{array}{cc}
-\phi _{2} & 1 \\ 
\phi _{1} & -1%
\end{array}%
\right] \left[ 
\begin{array}{c}
0 \\ 
1%
\end{array}%
\right] =$ $\frac{1}{\sqrt{5}}\left[ 
\begin{array}{cc}
\phi _{1}^{n} & \phi _{2}^{n} \\ 
\phi _{1}^{n+1} & \phi _{2}^{n+1}%
\end{array}%
\right] \left[ 
\begin{array}{c}
1 \\ 
-1%
\end{array}%
\right] =\frac{1}{\sqrt{5}}\left[ 
\begin{array}{c}
\phi _{1}^{n}-\phi _{2}^{n} \\ 
\phi _{1}^{n+1}-\phi _{2}^{n+1}%
\end{array}%
\right] =\left[ 
\begin{array}{c}
a_{n} \\ 
a_{n+1}%
\end{array}%
\right] $. Quindi $a_{n}=\frac{\phi _{1}^{n}-\phi _{2}^{n}}{\sqrt{5}}$. $%
\lim_{n\rightarrow +\infty }\frac{\phi _{1}^{n+1}-\phi _{2}^{n+1}}{\phi
_{1}^{n}-\phi _{2}^{n}}=\frac{\phi _{1}^{n+1}}{\phi _{1}^{n}}=\phi _{1}$
perch\'{e} $\left\vert \phi _{2}\right\vert <1$.
\end{enumerate}

\subsubsection{Polinomi di matrici}

Dato $P\left( t\right) \in \mathbf{K}\left[ t\right] $, $P\left( t\right)
=a_{0}+a_{1}t+...+a_{d}t^{d}$, e $A\in M_{\mathbf{K}}\left( n,n\right) $, $%
P\left( A\right)
=a_{0}A^{0}+a_{1}A+...+a_{d}A^{d}=a_{0}Id+a_{1}A+...+a_{d}A^{d}$ \`{e} una
matrice che si ottiene combinando linearmente le potenze di $A$.

\textbf{Teo} 
\begin{gather*}
\text{Hp}\text{: }A\in M_{\mathbf{K}}\left( n,n\right) \text{, }\mathbf{v}%
\text{ \`{e} un autovettore di }A\text{ con autovalore }\lambda \\
\text{Ts}\text{: (i) }\mathbf{v}\text{ \`{e} un autovettore di }A^{k}\text{
con autovalore }\lambda ^{k} \\
\text{(ii) se }\det A\neq 0\text{, }\mathbf{v}\text{ \`{e} un autovettore di 
}A^{-1}\text{ con autovalore }\frac{1}{\lambda } \\
\text{(iii) se }P\left( t\right) =a_{0}+a_{1}t+...+a_{d}t^{d}\text{, }%
\mathbf{v}\text{ \`{e} un autovettore di }P\left( A\right) \text{ con
autovalore }P\left( \lambda \right) \\
\text{(iv) se }A,B\text{ sono simili, }P\left( A\right) ,P\left( B\right) 
\text{ sono simili}
\end{gather*}

\textbf{Dim} (i) Se $\mathbf{v}$ \`{e} tale che $A\mathbf{v=}\lambda \mathbf{%
v}$, allora $A^{2}\mathbf{v}=A\left( A\mathbf{v}\right) =A\left( \lambda 
\mathbf{v}\right) =\lambda \left( A\mathbf{v}\right) =\lambda ^{2}\mathbf{v}$%
. Questo vale in generale: per induzione $A^{k}\mathbf{v}=A\left( A^{k-1}%
\mathbf{v}\right) =A\left( \lambda ^{k-1}\mathbf{v}\right) =\lambda
^{k-1}\left( A\mathbf{v}\right) =\lambda ^{k}\mathbf{v}$.

(ii) Se $\det A\neq 0$, essendo $\det A=\prod_{i=1}^{n}\lambda _{i}=\lambda
_{1}\lambda _{2}...\lambda _{n}$, ogni autovalore \`{e} non nullo (infatti
se non c'\`{e} l'autovalore nullo $r\left( A\right) =n$, coerentemente con $%
\det A\neq 0$). Inoltre $\exists $ $A^{-1}$. $A\mathbf{v}=\lambda \mathbf{v}%
\Longleftrightarrow A^{-1}\left( A\mathbf{v}\right) =A^{-1}\left( \lambda 
\mathbf{v}\right) \Longleftrightarrow \mathbf{v}=\lambda \left( A^{-1}%
\mathbf{v}\right) \Longleftrightarrow \frac{1}{\lambda }\mathbf{v}=A^{-1}%
\mathbf{v}$. $\lambda $ \`{e} invertibile sul campo $\mathbf{K}$ perch\'{e}
non nullo. Quindi $\mathbf{v}$ \`{e} un autovettore di $A^{-1}$ con
autovalore $\frac{1}{\lambda }$.

(iii) $P\left( t\right) =a_{0}+a_{1}t+...+a_{d}t^{d}$. $P\left( A\right)
=a_{0}Id+a_{1}A+...+a_{d}A^{d}$. $A\mathbf{v=}\lambda \mathbf{v}$. Voglio
mostrare che $P\left( A\right) \mathbf{v}=P\left( \lambda \right) \mathbf{v}$%
. Quindi $P\left( A\right) \mathbf{v}=\left(
a_{0}Id+a_{1}A+...+a_{d}A^{d}\right) \mathbf{v}=a_{0}Id\mathbf{v}%
+a_{1}\left( A\mathbf{v}\right) +...+a_{d}\left( A^{d}\mathbf{v}\right) $
per propriet\`{a} distributiva del prodotto matriciale. Ottengo quindi $a_{0}%
\mathbf{v}+a_{1}\left( \lambda \mathbf{v}\right) +...+a_{d}\left( \lambda
^{d}\mathbf{v}\right) $ (utilizzando anche il punto (i)), che si riscrive,
fattorizzando rispetto a $\mathbf{v}$, come $\left( a_{0}+a_{1}\lambda
+...+a_{d}\lambda ^{k}\right) \mathbf{v}=P\left( \lambda \right) \mathbf{v}$.

(iv) $B=S^{-1}AS$. Voglio mostrare che esiste una matrice invertibile tale
che $P\left( B\right) =C^{-1}P\left( A\right) C$. $P\left( B\right)
=a_{0}Id+a_{1}B+...+a_{d}B^{d}=a_{0}\left( S^{-1}S\right) +a_{1}\left(
S^{-1}AS\right) +...+a_{d}\left( S^{-1}A^{d}S\right) =$ come prima, $%
a_{0}\left( S^{-1}IdS\right) +a_{1}\left( S^{-1}AS\right) +...+a_{d}\left(
S^{-1}A^{d}S\right) =S^{-1}\left( a_{0}Id+a_{1}A+...+a_{d}A^{d}\right)
S=S^{-1}P\left( A\right) S$. Quindi non solo esiste una matrice invertibile
tale che $P\left( B\right) =C^{-1}P\left( A\right) C$, ma tale matrice \`{e}
anche la stessa che stabilisce la similitudine tra $A$ e $B$. $\blacksquare $

Quindi, se $A$ \`{e} diagonalizzabile, per (iv) anche $P\left( A\right) $ 
\`{e} diagonalizzabile. Infatti, se si prende come $B$ una matrice diagonale
cui $A$ \`{e} simile, si ha che $P\left( A\right) $ \`{e} simile a $P\left(
D\right) $, cio\`{e} $P\left( A\right) $ \`{e} diagonalizzabile con gli
autovalori $P\left( \lambda _{1}\right) ,...,P\left( \lambda _{n}\right) $.

\begin{enumerate}
\item Se $\lambda =1$ \`{e} un autovalore di $A$ con una certa molteplicit%
\`{a} algebrica, non necessariamente $\lambda =1$ \`{e} autovalore di $A^{k}$
con la stessa molteplicit\`{a} algebrica. Infatti, se $A$ ha anche un
autovalore $-1$ e $k$ \`{e} pari, $V_{1}\left( A^{k}\right) =V_{-1}\left(
A\right) \oplus V_{1}\left( A\right) $ e $a_{1}\left( A^{k}\right)
=a_{1}\left( A\right) +a_{-1}\left( A\right) $.
\end{enumerate}

In generale si ha $V_{\lambda }\left( A\right) \subseteq V_{\lambda
^{k}}\left( A^{k}\right) $; l'inclusione pu\`{o} essere propria anche nel
caso di matrici diagonalizzabili che abbiano tra i loro autovalori due
autovalori opposti.

\begin{enumerate}
\item $A=\left[ 
\begin{array}{cc}
1 & 0 \\ 
0 & -1%
\end{array}%
\right] $ \`{e} tale che $V_{1}\left( A\right) \subset V_{1}\left(
A^{2}\right) =%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$.
\end{enumerate}

Se $A$ non \`{e} diagonalizzabile, $P\left( A\right) $ potrebbe invece
essere diagonalizzabile; in generale, potrebbe avere autovettori che $A$ non
ha.

\begin{enumerate}
\item La matrice $A=\left[ 
\begin{array}{cc}
0 & 1 \\ 
0 & 0%
\end{array}%
\right] $ non \`{e} diagonalizzabile e ha come base di $V_{0}$ $\left\{
\left( 
\begin{array}{c}
1 \\ 
0%
\end{array}%
\right) \right\} $. $A^{2}=\left[ 
\begin{array}{cc}
0 & 0 \\ 
0 & 0%
\end{array}%
\right] $ \`{e} invece diagonalizzabile e ha come base di $V_{0}$ $\left\{
\left( 
\begin{array}{c}
1 \\ 
0%
\end{array}%
\right) ,\left( 
\begin{array}{c}
0 \\ 
1%
\end{array}%
\right) \right\} $. In generale, se $A$ non \`{e} diagonalizzabile ed \`{e}
nilpotente, allora $\exists $ $k:A^{k}=0_{M}$, e $A^{k}$ \`{e}
diagonalizzabile. In tal caso $V_{0}\left( A\right) \subseteq V_{0}\left(
A^{k}\right) $ diventa $V_{0}\left( A\right) \subset V_{0}\left(
A^{k}\right) $.

\item $A$ \`{e} nilpotente (cio\`{e} $\exists $ $k:A^{k}=0_{M}$) se e solo
se tutti gli autovalori di $A$ sono nulli. Infatti, se $A$ \`{e} nilpotente,
esiste $k:A^{k}=0_{M}$, ma se $\lambda $ \`{e} un qualsiasi autovalori di $A$
vale $A\mathbf{v}=\lambda \mathbf{v}$, quindi $A^{k}v=A^{k-1}\lambda \mathbf{%
v}=0_{M}$, da cui $\lambda =0$.
\end{enumerate}

Inoltre, se $A$ \`{e} simmetrica, anche $P\left( A\right) $ \`{e}
simmetrica, perch\'{e} sia $A$ che $P\left( A\right) $ sono ortogonalmente
diagonalizzabili.

Se $A$ ha autovalori non nulli, $A^{k}$ ha autovalori non nulli.

\textbf{Teorema (Cayley-Hamilton)}%
\begin{eqnarray*}
\text{Hp}\text{: } &&A\in M_{\mathbf{K}}\left( n,n\right) \text{, }%
p_{A}\left( x\right) =\det \left( A-xId\right)  \\
\text{Ts}\text{: } &&A\text{ \`{e} radice del proprio polinomio
caratteristico: }p_{A}\left( A\right) =0_{M}
\end{eqnarray*}

Questo teorema mostra che se $A$ ha tutti autovalori nulli (per cui $\det
\left( A-\lambda Id\right) =k\lambda ^{n}$), allora $A$ \`{e} nilpotente:
infatti $kA^{n}=0_{M}$.

\textbf{Dim} Si dimostra nel caso particolare di $A$ diagonalizzabile.
Considero $B=\left\{ \mathbf{v}_{1}\mathbf{,v}_{2}\mathbf{,...,v}%
_{n}\right\} $ base diagonalizzante (di autovettori): $A\mathbf{v}%
_{i}=\lambda _{i}\mathbf{v}_{i}$ $\forall $ $i=1,...,n$. $p_{A}\left(
A\right) \mathbf{v}_{i}=p_{A}\left( \lambda _{i}\right) \mathbf{v}_{i}$ (si
applica (iii) nel caso in cui il polinomio considerato \`{e} il polinomio
caratteristico) $=\mathbf{0}$ perch\'{e} $p_{A}\left( \lambda _{i}\right) =0$%
, $\forall $ $i=1,...,n$. Quindi $\mathbf{v}_{i}\in \ker \left( p_{A}\left(
A\right) \right) $ $\forall $ $i=1,...,n$ e $\ker \left( P_{A}\left(
A\right) \right) =\mathbf{K}^{n}$, perch\'{e} il nucleo dell'applicazione
lineare $P_{A}\left( A\right) \mathbf{x}$ \`{e} un sottospazio di $\mathbf{K}%
^{n}$, ma se contiene $n$ vettori linearmente indipendenti (\`{e} il massimo
numero di vettori linearmente indipendenti: non posso averne di pi\`{u} perch%
\'{e} $\dim \left( \ker \left( P_{A}\left( A\right) \right) \right) \leq n$,
quindi sono anche generatori, altrimenti potrei completarli a una base
aggiungendo altri vettori linearmente indipendenti), anche $\mathbf{K}^{n}$
contiene quegli $n$ vettori linearmente indipendenti, che costituiscono una
base di entrambi gli spazi. Avendo la stessa dimensione ed essendo $\ker
\left( P_{A}\left( A\right) \right) \subseteq \mathbf{K}^{n}$, i due spazi
coincidono. Affinch\'{e} sia $\dim \ker \left( P_{A}\left( A\right) \right)
=n$, dev'essere $r\left( P_{A}\left( A\right) \right) =0$: l'unica matrice
che ha rango nullo \`{e} la matrice nulla, quindi $P_{A}\left( A\right) =%
\left[ 
\begin{array}{ccc}
0 & ... & 0 \\ 
... & ... & ... \\ 
0 & ... & 0%
\end{array}%
\right] $. $\blacksquare $

Questo teorema si applica nel calcolo della matrice inversa. Considero $A$
invertibile: allora $\det A\neq 0$. $p_{A}\left( x\right) =\left( -1\right)
^{n}x^{n}+\left( -1\right) ^{n-1}tr\left( A\right) x^{n-1}+...+\left( \det
A\right) x^{0}$. $p_{A}\left( A\right) =\left( -1\right) ^{n}A^{n}+\left(
-1\right) ^{n-1}tr\left( A\right) A^{n-1}+...+\left( \det A\right) Id=0_{M}$
per il teorema visto. Postmoltiplico per $A^{-1}$: $\left( -1\right)
^{n}A^{n-1}+\left( -1\right) ^{n-1}tr\left( A\right)
A^{n-2}+...+a_{1}AA^{-1}+\left( \det A\right) IdA^{-1}=0_{M}$. Quindi $%
A^{-1}=\frac{\left( -1\right) ^{n+1}A^{n-1}+\left( -1\right) ^{n}tr\left(
A\right) A^{n-2}+...-a_{1}Id}{\det A}$: si pu\`{o} calcolare la matrice
inversa calcolando varie potenze di $A$. Pu\`{o} essere conveniente quando $%
p_{A}\left( x\right) $ ha molti termini nulli.

\begin{enumerate}
\item $A=\left[ 
\begin{array}{cc}
3 & 1 \\ 
0 & 2%
\end{array}%
\right] $. $p_{A}\left( x\right) =\det \left[ 
\begin{array}{cc}
3-x & 1 \\ 
0 & 2-x%
\end{array}%
\right] =\left( 3-x\right) \left( 2-x\right) =x^{2}-5x+6$: $\det A=6$, $%
tr\left( A\right) =5$. $p_{A}\left( A\right) =A^{2}-5A+6=\left[ 
\begin{array}{cc}
0 & 0 \\ 
0 & 0%
\end{array}%
\right] $ per il teorema di Cayley-Hamilton. Premoltiplicando per $A^{-1}$,
si ottiene $A-5Id+6A^{-1}=\left[ 
\begin{array}{cc}
0 & 0 \\ 
0 & 0%
\end{array}%
\right] $, dunque $A^{-1}=\frac{5Id-A}{6}=\frac{1}{6}\left[ 
\begin{array}{cc}
2 & -1 \\ 
0 & 3%
\end{array}%
\right] $, che \`{e} coerente con la formula vista.
\end{enumerate}

\subsection{Forma canonica di Jordan e pseudodiagonalizzazione}

I teoremi visti sulla diagonalizzabilit\`{a} permettono di stabilire che, se 
$A$ \`{e} diagonalizzabile, il rappresentante canonico della classe di
equivalenza di matrici simili ad $A$ (quindi la matrice migliore, tra tutte
quelle che rappresentano la stessa applicazione lineare) \`{e} la matrice
diagonale con gli autovalori della matrice, ripetuti con la loro molteplicit%
\`{a} algebrica, sulla diagonale.

(1) Cosa succede se $A$ \`{e} quadrata e non diagonalizzabile? Come si trova
un rappresentante della classe di equivalenza di matrici simili ad $A$, cio%
\`{e} come si sceglie la matrice migliore per rappresentare l'applicazione
lineare?

Posso scegliere $\mathbf{K}=%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
$ in modo che la prima ipotesi del secondo criterio di diagonalizzabilit\`{a}
sia sempre soddisfatta: data $A\in M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( n,n\right) $, $\lambda _{1},...,\lambda _{s}$ sono autovalori
complessi tali che $a_{\lambda _{1}}+...+a_{\lambda _{s}}=n$.

Se, data $A\in M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( n,n\right) $, $a_{\lambda _{1}}+...+a_{\lambda _{s}}=n$, allora $A$ 
\`{e} simile a una matrice complessa $B\in M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( n,n\right) $, detta matrice a blocchi di Jordan, del tipo $\left[ 
\begin{array}{cccc}
B_{\lambda _{1}} & 0 & ... & 0 \\ 
0 & B_{\lambda _{2}} & ... & 0 \\ 
... & ... & ... & ... \\ 
0 & 0 & ... & B_{\lambda _{s}}%
\end{array}%
\right] $, dove $B_{\lambda _{1}},...,B_{\lambda _{s}}$ sono matrici
triangolari superiori $B_{\lambda _{i}}\in M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( a_{\lambda _{i}},a_{\lambda _{i}}\right) $, con $\lambda _{i}$ sulla
diagonale principale, tali che $r\left( B_{\lambda _{i}}-\lambda
_{i}Id\right) =a_{\lambda _{i}}-g_{\lambda _{i}}$ ($g_{\lambda _{i}}=\dim
\left( \ker \left( B_{\lambda _{i}}-\lambda _{i}Id\right) \right)
=a_{\lambda _{i}}-r\left( B_{\lambda _{i}}-\lambda _{i}Id\right) $: ordine
della matrice meno rango della matrice). Se $a_{\lambda _{i}}=g_{\lambda
_{i}}$, $r\left( B_{\lambda _{i}}-\lambda _{i}Id\right) =0$, cio\`{e} $%
B_{\lambda _{i}}-\lambda _{i}Id=0_{M}\Longleftrightarrow B_{\lambda
_{i}}=\lambda _{i}Id$, cio\`{e} $B_{\lambda _{i}}$ \`{e} diagonale. Ogni
matrice $B_{\lambda _{i}}$ ha come unico autovalore $\lambda _{i}$, con
molteplicit\`{a} $a\left( \lambda _{i}\right) $.

Questa \`{e} una struttura "abbastanza simile" a quella di una matrice
diagonale.

\begin{enumerate}
\item Si \`{e} visto, prima del secondo criterio di diagonalizzabilit\`{a},
un esempio di matrice $A=\left[ 
\begin{array}{cccc}
0 & -1 & 0 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 3 \\ 
0 & 0 & 0 & 0%
\end{array}%
\right] $ non diagonalizzabile n\'{e} su $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ n\'{e} su $%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
$ il cui polinomio caratteristico aveva molteplicit\`{a} algebrica della
radice $\lambda =0$ $a_{0}=4$ e molteplicit\`{a} geometrica $\dim \left(
V_{0}\right) =g_{0}=2$. $A$ non \`{e} diagonalizzabile, ma \`{e} (in
generale, E' SIMILE A?) una matrice di Jordan con blocco triangolare
superiore $B_{0}\in M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( 4,4\right) $, con gli autovalori $0$ sulla diagonale e tale che $%
r\left( B_{0}\right) =4-2=2$.

\item $A\in M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( 3,3\right) $ ha due autovalori $\lambda _{1},\lambda _{2}:a_{\lambda
_{1}}=1=g_{\lambda _{1}}$, ma $a_{\lambda _{2}}=2>1=g_{\lambda _{2}}$. $A$
non \`{e} diagonalizzabile, ma come matrice complessa \`{e} simile a una
matrice formata da due blocchi triangolari superiori $B_{\lambda _{1}}\in M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( 1,1\right) ,B_{\lambda _{2}}\in M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( 2,2\right) $. $B_{\lambda _{1}}=\left[ \lambda _{1}\right] $, $%
B_{\lambda _{2}}=\left[ 
\begin{array}{cc}
\lambda _{2} & ? \\ 
0 & \lambda _{2}%
\end{array}%
\right] :r\left( B_{\lambda _{2}}-\lambda _{2}Id\right) =a_{\lambda
_{2}}-g_{\lambda _{2}}=1$. In $?$ non pu\`{o} esserci $0$ (altrimenti $A$
sarebbe diagonalizzabile, contro l'ipotesi); dev'essere $r\left( B_{\lambda
_{2}}-\lambda _{2}Id\right) =1$, cio\`{e} $r\left( \left[ 
\begin{array}{cc}
\lambda _{2}-\lambda _{2} & ? \\ 
0 & \lambda _{2}-\lambda _{2}%
\end{array}%
\right] \right) =1$, quindi per avere rango uno \`{e} sufficiente che in $?$
ci sia un qualsiasi elemento non nullo, e. g. $1$. Perci\`{o} $B=\left[ 
\begin{array}{ccc}
\lambda _{1} & 0 & 0 \\ 
0 & \lambda _{2} & 1 \\ 
0 & 0 & \lambda _{2}%
\end{array}%
\right] =\left[ 
\begin{array}{cc}
B_{\lambda _{1}} & 0 \\ 
0 & B_{\lambda _{2}}%
\end{array}%
\right] $.
\end{enumerate}

(2) Cosa succede se $A$ non \`{e} neanche quadrata? La diagonalizzazione di
una matrice $A$ serve a trovare, nella classe di equivalenza di matrici
simili ad $A$ (che rappresentano tutte le stessa applicazione lineare $L$),
la matrice rappresentativa che semplifichi il pi\`{u} possibile i calcoli
legati all'uso di $L$. Questa \`{e} la matrice diagonale, se $A$ \`{e}
diagonalizzabile. Ma se $L$ non ha dominio e codominio isomorfi (cui quindi
non si pu\`{o} applicare n\'{e} la definizione di similitudine n\'{e} di
diagonalizzazione, essendo la matrice rappresentativa non quadrata), quale
si sceglie come matrice rappresentativa di $L$ che semplifichi i calcoli?

Si affronta quindi il caso di applicazione lineare $L:V\rightarrow W$, $\dim
V=n$, $\dim W=m$.

\textbf{Teorema di rappresentazione pseudodiagonale}%
\begin{gather*}
\text{Hp}\text{: }L:V\rightarrow W\text{ \`{e} un'applicazione lineare, }%
\dim V=n\text{, }\dim W=m \\
\text{Ts}\text{: }\exists \text{ una base }B\text{ di }V\text{ e una base }C%
\text{ di }W\text{ tali che }M_{B}^{C}\left( L\right) =\left[ 
\begin{array}{ccccc}
1 & 0 & 0 & ... & 0 \\ 
0 & 1 & 0 & ... & 0 \\ 
0 & 0 & 0 & ... & 0 \\ 
... & ... & ... & ... & ... \\ 
0 & 0 & 0 & ... & 0%
\end{array}%
\right]
\end{gather*}

cio\`{e} la matrice rappresentativa non \`{e} diagonale (la definizione di
matrice diagonale si applica a matrici quadrate), ma ha "molti zeri e un po'
di uni sulla diagonale": \`{e} una matrice a blocchi $\left[ 
\begin{array}{cc}
Id & 0 \\ 
0 & 0%
\end{array}%
\right] $, con $Id$ di dimensione $r\left( A\right) $ (come si vedr\`{a}
nella dimostrazione).

\textbf{Dim} Considero una base dell'immagine $\left\{ \mathbf{w}_{1}\mathbf{%
,...,w}_{r}\right\} $, dove $r$ \`{e} il rango di $L$ (la dimensione
dell'immagine): voglio completarla a una base di $W$: $C=\left\{ \mathbf{w}%
_{1}\mathbf{,...,w}_{r}\right\} \cup \left\{ \mathbf{w}_{r+1}\mathbf{,...,w}%
_{m}\right\} $. Considero $\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{r}\right\} 
$ tali che $L\left( \mathbf{v}_{1}\right) =\mathbf{w}_{1}$,..., $L\left( 
\mathbf{v}_{r}\right) =\mathbf{w}_{r}$: essendo $\mathbf{w}_{1}\mathbf{,...,w%
}_{r}$ linearmente indipendenti, mostro che $\mathbf{v}_{1}\mathbf{,...,v}%
_{r}$ sono linearmente indipendenti (se fosse e. g. $\mathbf{v}_{3}\mathbf{=v%
}_{2}\mathbf{+v}_{1}$, si avrebbe $L\left( \mathbf{v}_{3}\right) =L\left( 
\mathbf{v}_{1}\right) +L\left( \mathbf{v}_{2}\right) =\mathbf{w}_{1}\mathbf{%
+w}_{2}=L\left( \mathbf{v}_{3}\right) =\mathbf{w}_{3}$, e $\mathbf{w}_{1}%
\mathbf{,...,w}_{r}$ non sarebbero linearmente indipendenti). Infatti,
considero $\lambda _{1}\mathbf{v}_{1}+\mathbf{...}+\lambda _{r}\mathbf{v}%
_{r}=\mathbf{0}$: $L\left( \lambda _{1}\mathbf{v}_{1}+\mathbf{...}+\lambda
_{r}\mathbf{v}_{r}\right) =L\left( \mathbf{0}\right) \Longleftrightarrow
\lambda _{1}\mathbf{w}_{1}\mathbf{+...+}\lambda _{r}\mathbf{w}_{r}=\mathbf{0}
$. Ma allora $\lambda _{1}=...=\lambda _{r}=0$ perch\'{e} $\mathbf{w}_{1}%
\mathbf{,...,w}_{r}$ sono linearmente indipendenti. Considero una base di $%
\ker \left( L\right) $ $\left\{ \mathbf{v}_{r+1}\mathbf{,...,v}_{n}\right\} $%
: poich\'{e} $\mathbf{v}_{1}\mathbf{,...,v}_{r}$ sono linearmente
indipendenti, $Span\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{r}\right\} \cap
\ker \left( L\right) =\left\{ \mathbf{0}\right\} $ ($L\left( \lambda _{1}%
\mathbf{v}_{1}\mathbf{+...+}\lambda _{r}\mathbf{v}_{r}\right) =\mathbf{%
0\Longleftrightarrow }\lambda _{1}\mathbf{w}_{1}\mathbf{+...+}\lambda _{r}%
\mathbf{w}_{r}=\mathbf{0\Longleftrightarrow }\lambda _{1}=...=\lambda _{r}=0$%
). Allora per la formula di Grassman $\dim \left( Span\left\{ \mathbf{v}_{1}%
\mathbf{,...,v}_{r}\right\} +\ker \left( L\right) \right) =\dim \left(
Span\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{r}\right\} \right) +\dim \left(
\ker \left( L\right) \right) =r+n-r=n$ ($Span\left\{ \mathbf{v}_{1}\mathbf{%
,...,v}_{r}\right\} +\ker \left( L\right) =Span\left\{ \mathbf{v}_{1}\mathbf{%
,...,v}_{r}\right\} \oplus \ker \left( L\right) $), i. e. l'unione dei
generatori di $Span\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{r}\right\} $ e di $%
\ker \left( L\right) $ \`{e} un insieme di vettori linearmente indipendenti
(altrimenti ci sarebbe un vettore non nullo nell'intersezione). Quindi $%
B=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{r}\right\} \cup \left\{ \mathbf{v}%
_{r+1}\mathbf{,...,v}_{n}\right\} $ \`{e} una base di $V$. Posso costruire
la matrice rappresentativa:%
\begin{equation*}
M_{B}^{C}\left( L\right) =\left[ 
\begin{array}{cccccccc}
L\left( \mathbf{v}_{1}\right) & L\left( \mathbf{v}_{2}\right) & ... & 
L\left( \mathbf{v}_{r}\right) & L\left( \mathbf{v}_{r+1}\right) & ... & 
L\left( \mathbf{v}_{n}\right) &  \\ 
1 & 0 & ... & 0 & 0 & ... & 0 & \mathbf{w}_{1} \\ 
0 & 1 & ... & 0 & 0 & ... & 0 & \mathbf{w}_{2} \\ 
... & ... & ... & ... & ... & ... & ... & ... \\ 
0 & 0 & ... & 1 & 0 & ... & 0 & \mathbf{w}_{r} \\ 
0 & 0 & ... & 0 & 0 & ... & 0 & \mathbf{w}_{r+1} \\ 
... & ... & ... & ... & ... & ... & ... & ... \\ 
0 & 0 & ... & 0 & 0 & ... & 0 & \mathbf{w}_{n}%
\end{array}%
\right]
\end{equation*}

Non si possono avere pi\`{u} zeri: il rango di $M_{B}^{C}\left( L\right) $
dev'essere $r$. $\blacksquare $

La matrice rappresentativa pseudodiagonale \`{e} molto bella per fare i
calcoli, ma c'\`{e} un lato negativo dovuto al fatto che non ci sono
informazioni sulle propriet\`{a} di $B,C$ rispetto a cui essa rappresenta $L$%
; se si usa la matrice rappresentativa di $L$ rispetto alle basi canoniche,
la matrice potrebbe non essere bella, ma le basi sono ben note. Quindi non 
\`{e} ovvio qual \`{e} la migliore scelta di rappresentazione di $L$.

\section{Spazi euclidei}

Il campo considerato d'ora in poi \`{e} $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$.

\textbf{Def} Si dice spazio euclideo $V$ uno spazio vettoriale sul campo $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ dotato di un prodotto scalare $\langle \_,\_\rangle :V\times V\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ che soddisfa le seguenti propriet\`{a}:

\begin{description}
\item[-] commutativit\`{a}: $\forall $ $\mathbf{v,w}\in V$, $\langle \mathbf{%
v,w}\rangle =\langle \mathbf{w,v}\rangle $

\item[-] bilinearit\`{a}: $\forall $ $\mathbf{v,w,u}\in V$, $\langle \mathbf{%
v+w,u}\rangle =\langle \mathbf{v,u}\rangle +\langle \mathbf{w,u}\rangle $; $%
\forall $ $\mathbf{v,w,u}\in V$, $\langle \mathbf{u,v+w}\rangle =\langle 
\mathbf{u,v}\rangle +\langle \mathbf{u,w}\rangle $; $\forall $ $\mathbf{v,w}%
\in V$, $\forall $ $\lambda \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, $\langle \lambda \mathbf{v,w}\rangle =\lambda \langle \mathbf{v,w}\rangle
=\langle \mathbf{v,}\lambda \mathbf{w}\rangle $

\item[-] positivit\`{a}: $\forall $ $\mathbf{v}\in V$, $\langle \mathbf{v,v}%
\rangle \geq 0$; $\forall $ $\mathbf{v}\in V$, $\langle \mathbf{v,v}\rangle
=0\Longleftrightarrow \mathbf{v=0}$
\end{description}

\begin{enumerate}
\item Il prodotto scalare standard tra $\mathbf{v}=\left( 
\begin{array}{c}
v_{1} \\ 
v_{2}%
\end{array}%
\right) $ e $\mathbf{w}=\left( 
\begin{array}{c}
w_{1} \\ 
w_{2}%
\end{array}%
\right) $, in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$, \`{e} $\langle \mathbf{v,w}\rangle =\mathbf{v\cdot w}%
=v_{1}w_{1}+v_{2}w_{2}$. Il prodotto scalare standard tra $\mathbf{v}=\left( 
\begin{array}{c}
v_{1} \\ 
v_{2} \\ 
v_{3}%
\end{array}%
\right) $ e $\mathbf{w}=\left( 
\begin{array}{c}
w_{1} \\ 
w_{2} \\ 
w_{3}%
\end{array}%
\right) $, in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$, \`{e} $\langle \mathbf{v,w}\rangle =\mathbf{v\cdot w}%
=v_{1}w_{1}+v_{2}w_{2}+v_{3}w_{3}$.

\item Il prodotto scalare standard tra $\mathbf{v}=\left( 
\begin{array}{c}
v_{1} \\ 
... \\ 
v_{n}%
\end{array}%
\right) $ e $\mathbf{w}=\left( 
\begin{array}{c}
w_{1} \\ 
... \\ 
w_{n}%
\end{array}%
\right) $, in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, \`{e} $\langle \mathbf{v,w}\rangle
=v_{1}w_{1}+v_{2}w_{2}+...+v_{n}w_{n}=\mathbf{v}^{T}\mathbf{w}=\left[
v_{1}|...|v_{n}\right] \left( 
\begin{array}{c}
w_{1} \\ 
... \\ 
w_{n}%
\end{array}%
\right) =\sum_{i=1}^{n}v_{i}w_{i}$. La commutativit\`{a} segue dalla
commutativit\`{a} in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, la bilinearit\`{a} si mostra con la propriet\`{a} distributiva del
prodotto matriciale, ecc ($\left\langle \mathbf{v,v}\right\rangle
=v_{1}^{2}+...+v_{n}^{2}\geq 0$).

\item Posto $I=\left[ a,b\right] \subseteq 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, $L^{2}\left( I\right) =\left\{ f:I\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
:\int_{a}^{b}f^{2}\left( t\right) dt\text{ \`{e} finito}\right\} $ \`{e} lo
spazio vettoriale delle funzioni quadrato-integrabili (come si mostra che $%
f+g$ \`{e} tale che $\int_{a}^{b}\left( f+g\right)
^{2}dt=\int_{a}^{b}f^{2}\left( t\right) dt+\int_{a}^{b}g^{2}\left( t\right)
dt+\int_{a}^{b}f\left( t\right) g\left( t\right) dt$: il terzo \`{e}
finito?). Il prodotto scalare in $L^{2}\left( I\right) $ \`{e} definito come
segue: $\forall $ $f,g\in L^{2}\left( I\right) $, $\langle f,g\rangle
=\int_{a}^{b}f\left( t\right) g\left( t\right) dt$ (si pu\`{o} vedere una
funzione come un vettore con un'infinit\`{a} non numerabile di componenti:
dato $t$, $f\left( t\right) $ \`{e} una sua componente). $L^{2}\left(
I\right) $ \`{e} quindi uno spazio euclideo dotato di tale prodotto scalare.
La commutativit\`{a} segue dalla commutativit\`{a} del prodotto di funzioni,
la bilinearit\`{a} dalla linearit\`{a} dell'integrazione definita, $\langle
f,f\rangle =\int_{a}^{b}f^{2}\left( t\right) dt\geq 0$ per positivit\`{a}
dell'integrale. Si mostra che $\langle f,f\rangle =0\Longleftrightarrow
f\left( t\right) =0$ $\forall $ $t\in I$.

$L^{2}\left( I\right) $ \`{e} un insieme fondamentale in analisi del segnale.
\end{enumerate}

\textbf{Def} Dato uno spazio euclideo $V$ dotato di un prodotto scalare $%
\langle \_,\_\rangle $, si dice norma la funzione $\left\vert \left\vert
\_\right\vert \right\vert :V\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ definita come $\left\vert \left\vert \mathbf{v}\right\vert \right\vert =%
\sqrt{\langle \mathbf{v,v}\rangle }$ $\forall $ $\mathbf{v}\in V$.

Ha le seguenti propriet\`{a}:

\begin{description}
\item[-] positivit\`{a} e annullamento: $\left\vert \left\vert \mathbf{v}%
\right\vert \right\vert \geq 0$; $\left\vert \left\vert \mathbf{v}%
\right\vert \right\vert =0\Longleftrightarrow \mathbf{v=0}$

\item[-] omogeneit\`{a}: $\left\vert \left\vert t\mathbf{v}\right\vert
\right\vert =\left\vert t\right\vert \left\vert \left\vert \mathbf{v}%
\right\vert \right\vert $ (infatti $\left\vert \left\vert t\mathbf{v}%
\right\vert \right\vert =\sqrt{\langle t\mathbf{v,}t\mathbf{v}\rangle }=%
\sqrt{t^{2}\langle \mathbf{v,v}\rangle }=\left\vert t\right\vert \sqrt{%
\langle \mathbf{v,v}\rangle }$).
\end{description}

Tale definizione ha senso perch\'{e} la radice si pu\`{o} calcolare $\forall 
$ $\mathbf{v}$, per positivit\`{a} del prodotto scalare. La norma \`{e} una
generalizzazione del valore assoluto in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$: $\sqrt{x\cdot x}=\left\vert x\right\vert $, $\left\vert x\right\vert \geq
0$, ecc.

\textbf{Teorema (disuguaglianza di Schwarz)}%
\begin{gather*}
\text{Hp}\text{: }\left( V,\langle \_,\_\rangle \right) \text{ \`{e} uno
spazio euclideo} \\
\text{Ts}\text{: }\forall \text{ }\mathbf{v,w}\in V\text{, }\left\vert
\langle \mathbf{v,w\rangle }\right\vert \leq \left\vert \left\vert \mathbf{v}%
\right\vert \right\vert \left\vert \left\vert \mathbf{w}\right\vert
\right\vert 
\end{gather*}

E' una conseguenza naturale della struttura di prodotto scalare.

\textbf{Dim} Se $\mathbf{w=0}$, $\left\vert \langle \mathbf{v,0\rangle }%
\right\vert =\left\vert \langle \mathbf{v,w-w\rangle }\right\vert
=\left\vert \langle \mathbf{v,w\rangle }-\langle \mathbf{v,w\rangle }%
\right\vert =0$ (o per bilinearit\`{a}) $\leq \left\vert \left\vert \mathbf{v%
}\right\vert \right\vert \left\vert \left\vert \mathbf{0}\right\vert
\right\vert =0$. Se $\mathbf{w\neq 0}$, considero $\mathbf{v}+x\mathbf{w}$, $%
x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$. $\left\vert \left\vert \mathbf{v}+x\mathbf{w}\right\vert \right\vert
^{2}=\langle \mathbf{v}+x\mathbf{w,v}+x\mathbf{w\rangle =}\langle \mathbf{v}%
+x\mathbf{w,v\rangle +}\langle \mathbf{v}+x\mathbf{w,}x\mathbf{w\rangle =}$ $%
\langle \mathbf{v,v\rangle +}\langle x\mathbf{w,v\rangle +}\langle \mathbf{v,%
}x\mathbf{w\rangle +}\langle x\mathbf{w,}x\mathbf{w\rangle =}\left\vert
\left\vert \mathbf{v}\right\vert \right\vert ^{2}+2\langle \mathbf{v,}x%
\mathbf{w\rangle +}\left\vert \left\vert x\mathbf{w}\right\vert \right\vert
^{2}=\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}+2\langle 
\mathbf{v,}x\mathbf{w\rangle +}x^{2}\left\vert \left\vert \mathbf{w}%
\right\vert \right\vert ^{2}$ per commutativit\`{a} e omogeneit\`{a} del
prodotto scalare. Poich\'{e} $\left\vert \left\vert \mathbf{v}\right\vert
\right\vert ^{2}+2\langle \mathbf{v,}x\mathbf{w\rangle +}x^{2}\left\vert
\left\vert \mathbf{w}\right\vert \right\vert ^{2}=\left\vert \left\vert 
\mathbf{v}+x\mathbf{w}\right\vert \right\vert ^{2}\geq 0$ per positivit\`{a}
del prodotto scalare, $\left\vert \left\vert \mathbf{v}\right\vert
\right\vert ^{2}+2\langle \mathbf{v,}x\mathbf{w\rangle +}x^{2}\left\vert
\left\vert \mathbf{w}\right\vert \right\vert ^{2}\geq 0$. Se si considera $%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}+2\langle 
\mathbf{v,}x\mathbf{w\rangle +}x^{2}\left\vert \left\vert \mathbf{w}%
\right\vert \right\vert ^{2}=\left\vert \left\vert \mathbf{w}\right\vert
\right\vert ^{2}x^{2}+2\langle \mathbf{v,w\rangle }x\mathbf{+}\left\vert
\left\vert \mathbf{v}\right\vert \right\vert ^{2}$ una funzione di $x$, $%
f\left( x\right) \geq 0$ $\forall $ $x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\Longleftrightarrow \Delta \leq 0$, i. e. $\Delta =4\langle \mathbf{%
v,w\rangle }^{2}-4\left\vert \left\vert \mathbf{w}\right\vert \right\vert
^{2}\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}\leq
0\Longleftrightarrow \langle \mathbf{v,w\rangle }^{2}\leq \left\vert
\left\vert \mathbf{w}\right\vert \right\vert ^{2}\left\vert \left\vert 
\mathbf{v}\right\vert \right\vert ^{2}\Longleftrightarrow \sqrt{\langle 
\mathbf{v,w\rangle }^{2}}\leq \sqrt{\left\vert \left\vert \mathbf{w}%
\right\vert \right\vert ^{2}\left\vert \left\vert \mathbf{v}\right\vert
\right\vert ^{2}}\Longleftrightarrow \left\vert \langle \mathbf{v,w\rangle }%
\right\vert \leq \left\vert \left\vert \mathbf{w}\right\vert \right\vert
\left\vert \left\vert \mathbf{v}\right\vert \right\vert $. $\blacksquare $

Dal teorema segue $\left\vert \langle \mathbf{v,w\rangle }\right\vert \leq
\left\vert \left\vert \mathbf{v}\right\vert \right\vert \left\vert
\left\vert \mathbf{w}\right\vert \right\vert \Longleftrightarrow -\left\vert
\left\vert \mathbf{v}\right\vert \right\vert \left\vert \left\vert \mathbf{w}%
\right\vert \right\vert \leq \langle \mathbf{v,w\rangle }\leq \left\vert
\left\vert \mathbf{v}\right\vert \right\vert \left\vert \left\vert \mathbf{w}%
\right\vert \right\vert \Longleftrightarrow -1\leq \frac{\langle \mathbf{%
v,w\rangle }}{\left\vert \left\vert \mathbf{v}\right\vert \right\vert
\left\vert \left\vert \mathbf{w}\right\vert \right\vert }\leq 1$. $\frac{%
\langle \mathbf{v,w\rangle }}{\left\vert \left\vert \mathbf{v}\right\vert
\right\vert \left\vert \left\vert \mathbf{w}\right\vert \right\vert }$ si pu%
\`{o} interpretare come coseno di un angolo.

\textbf{Def} Dati due vettori $\mathbf{v,w\neq 0}$ in uno spazio euclideo,
si definisce angolo tra $\mathbf{v}$ e $\mathbf{w}$ il numero reale,
appartenente a $\left[ 0,\pi \right] $, $\mathbf{\hat{v}w}=\arccos \frac{%
\langle \mathbf{v,w\rangle }}{\left\vert \left\vert \mathbf{v}\right\vert
\right\vert \left\vert \left\vert \mathbf{w}\right\vert \right\vert }$.

\begin{enumerate}
\item In $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$ e $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$ $\mathbf{\hat{v}w}=\arccos \frac{\langle \mathbf{v,w\rangle }}{%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert \left\vert
\left\vert \mathbf{w}\right\vert \right\vert }\Longleftrightarrow \left\vert
\left\vert \mathbf{v}\right\vert \right\vert \left\vert \left\vert \mathbf{w}%
\right\vert \right\vert \cos \mathbf{\hat{v}w}=\langle \mathbf{v,w\rangle }$%
, e $\mathbf{\hat{v}w}$ \`{e} l'angolo tra i due vettori secondo l'usuale
interpretazione geometrica di angolo.

\item In $L^{2}\left( I\right) $ l'angolo tra le funzioni $f$ e $g$ \`{e} $%
\hat{f}g=\arccos \frac{\int_{a}^{b}f\left( t\right) g\left( t\right) dt}{%
\sqrt{\int_{a}^{b}f^{2}\left( t\right) dt}\sqrt{\int_{a}^{b}g^{2}\left(
t\right) dt}}$.
\end{enumerate}

Si fanno altri esempi di spazi euclidei.

\begin{enumerate}
\item Considero lo spazio euclideo $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ e una matrice simmetrica $B$ (che \`{e} invariante per trasposizione).
Dati due vettori $\mathbf{v,w}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, si definisce il prodotto tra $\mathbf{v}$ e $\mathbf{w}$ indotto
dalla matrice $B$ come $\langle \mathbf{v,w\rangle }_{B}=\mathbf{v}^{T}B%
\mathbf{w}$. E' commutativo perch\'{e} $B$ \`{e} simmetrica, bilineare per
la propriet\`{a} distributiva del prodotto matriciale. Affinch\'{e} sia un
prodotto scalare deve valere $\langle \mathbf{v,v\rangle }_{B}=\mathbf{v}%
^{T}B\mathbf{v\geq 0}$ e $\langle \mathbf{v,v\rangle }_{B}=\mathbf{%
0\Longleftrightarrow v=0}$: impareremo come capire se vale ci\`{o}, usando
gli autovalori di $B$ ($B$ dev'essere definita positiva). Tale prodotto
scalare, quando $B=Id$, si dice prodotto scalare standard di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ ed \`{e} quello usato finora: se $\mathbf{v}=\left[ 
\begin{array}{c}
v_{1} \\ 
... \\ 
v_{n}%
\end{array}%
\right] $ e $\mathbf{w}=\left( 
\begin{array}{c}
w_{1} \\ 
... \\ 
w_{n}%
\end{array}%
\right) $, $\langle \mathbf{v,w\rangle }_{Id}=\mathbf{v}^{T}Id\mathbf{w=v}%
^{T}\mathbf{w=}\left[ v_{1}|...|v_{n}\right] \left[ 
\begin{array}{c}
w_{1} \\ 
... \\ 
w_{n}%
\end{array}%
\right] $. Quindi $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ \`{e} uno spazio euclideo rispetto a entrambe tali definizioni di
prodotto scalare: la seconda \`{e} una generalizzazione della prima.

\item Considero lo spazio euclideo $M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( m,n\right) $ e definisco $\langle A,B\mathbf{\rangle }=tr\left(
B^{T}A\right) $ (la trasposizione \`{e} necessaria per poter fare il
prodotto). La bilinearit\`{a} segue dalla linearit\`{a} della trasposizione,
dalla propriet\`{a} distributiva e associativa del prodotto matriciale e
dalla linearit\`{a} della traccia. La commutativit\`{a} si pu\`{o} mostrare
usando la descrizione analitica del prodotto matriciale, oppure notando che $%
\left\langle A,B\right\rangle =tr\left( B^{T}A\right) =tr\left( \left(
B^{T}A\right) ^{T}\right) =tr\left( A^{T}B\right) =\left\langle
B,A\right\rangle $ (la traccia \`{e} invariante per trasposizione). Per la
positivit\`{a} serve che $tr\left( A^{T}A\right) \geq 0$ $\forall $ $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( m,n\right) $ e che $tr\left( A^{T}A\right) =0\Longleftrightarrow
A=0_{M}$. Se $A=\left[ C_{1}\left( A\right) |...|C_{n}\left( A\right) \right]
$ e $A^{T}=\left[ 
\begin{array}{c}
C_{1}\left( A\right) \\ 
... \\ 
C_{n}\left( A\right)%
\end{array}%
\right] $, $\left( A^{T}A\right) _{ii}=R_{i}\left( A^{T}\right) C_{i}\left(
A\right) =\left\langle C_{i}\left( A\right) ,C_{i}\left( A\right)
\right\rangle =\left\vert \left\vert C_{i}\left( A\right) \right\vert
\right\vert ^{2}$ (secondo la definizione di norma nello spazio euclideo $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ con prodotto scalare standard). Quindi la traccia \`{e} $tr\left(
A^{T}A\right) =\sum_{i=1}^{n}\left( A^{T}A\right)
_{ii}=\sum_{i=1}^{n}\left\vert \left\vert C_{i}\left( A\right) \right\vert
\right\vert ^{2}$: \`{e} maggiore o uguale di zero per ogni $A$ perch\'{e}
somma di quadrati, ed \`{e} nulla se e solo se ogni quadrato \`{e} zero, cio%
\`{e} $\left\vert \left\vert C_{i}\left( A\right) \right\vert \right\vert
^{2}=0$ $\forall $ $i$, cio\`{e} $\left\vert \left\vert C_{i}\left( A\right)
\right\vert \right\vert =0$ $\forall $ $i$, cio\`{e}, per la positivit\`{a}
della norma nello spazio euclideo $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ con prodotto scalare standard, $C_{i}\left( A\right) =\mathbf{0}$ $%
\forall $ $i$, cio\`{e} $A$ \`{e} la matrice nulla.

\item \textit{Dato lo spazio euclideo }$M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $\textit{, con }$\langle A,B\rangle =tr\left(
B^{T}A\right) $\textit{, determinare un'applicazione lineare }$L:M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) \rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n^{2}}$\textit{\ tale che }$\left\langle A,A\right\rangle =L\left(
A\right) \cdot L\left( A\right) $\textit{, dove }$\cdot $\textit{\ indica il
prodotto scalare euclideo standard di }$%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n^{2}}$\textit{.}

$\left\langle A,A\right\rangle =tr\left( A^{T}A\right) =\sum_{i=1}^{n}\left(
A^{T}A\right) _{ii}=\sum_{i=1}^{n}\left(
\sum_{j=1}^{n}a_{ij}^{T}a_{ji}\right) =\sum_{i=1}^{n}\left(
\sum_{j=1}^{n}a_{ji}a_{ji}\right) =\sum_{i=1}^{n}\left(
\sum_{j=1}^{n}a_{ji}^{2}\right) $. Questo significa sommare tutti i quadrati
degli elementi della matrice: \`{e} equivalente a calcolare il prodotto
scalare standard tra due vettori delle coordinate della matrice in base
canonica. Quindi, posto $A=a_{11}E_{11}+...+a_{nn}E_{nn}$, $L\left(
E_{ij}\right) =\mathbf{e}_{ij}$ e $L\left( A\right) =\left( 
\begin{array}{c}
a_{11} \\ 
a_{12} \\ 
... \\ 
a_{nn}%
\end{array}%
\right) $.

Useremo la traccia per stabilire quando alcune matrici si assomigliano.
\end{enumerate}

\textbf{Teorema (disuguaglianza triangolare)}%
\begin{eqnarray*}
\text{Hp}\text{: } &&\left( V,\langle \_,\_\mathbf{\rangle }\right) \text{ 
\`{e} uno spazio euclideo; }\mathbf{v,w}\in V \\
\text{Ts}\text{: } &&\left\vert \left\vert \mathbf{v+w}\right\vert
\right\vert \leq \left\vert \left\vert \mathbf{v}\right\vert \right\vert
+\left\vert \left\vert \mathbf{w}\right\vert \right\vert
\end{eqnarray*}

Questo mostra che la norma non \`{e} un'applicazione lineare.

\textbf{Dim} Calcolo $\left\vert \left\vert \mathbf{v+w}\right\vert
\right\vert ^{2}=\mathbf{\langle v+w,v+w\rangle =\langle v+w,v\rangle
+\langle v+w,w\rangle }$ $\mathbf{=\langle v,v\rangle +\langle w,v\rangle
+\langle v,w\rangle +\langle w,w\rangle =}


\mathbf{\langle v,v\rangle +}2\mathbf{%
\langle v,w\rangle +\langle w,w\rangle \leq \langle v,v\rangle +}2\left\vert 
\mathbf{\langle v,w\rangle }\right\vert \mathbf{+\langle w,w\rangle }$ (la
maggiorazione \`{e} dovuta al fatto che $\left\vert \mathbf{\langle
v,w\rangle }\right\vert \geq \mathbf{\langle v,w\rangle }$) $\leq \left\vert
\left\vert \mathbf{v}\right\vert \right\vert ^{2}\mathbf{+}2\left\vert
\left\vert \mathbf{v}\right\vert \right\vert \left\vert \left\vert \mathbf{w}%
\right\vert \right\vert \mathbf{+}\left\vert \left\vert \mathbf{w}%
\right\vert \right\vert ^{2}$ (per la disuguaglianza di Schwarz) $=\left(
\left\vert \left\vert \mathbf{v}\right\vert \right\vert +\left\vert
\left\vert \mathbf{w}\right\vert \right\vert \right) ^{2}$. Si \`{e} quindi
scritto che $\left\vert \left\vert \mathbf{v}+\mathbf{w}\right\vert
\right\vert ^{2}\leq \left( \left\vert \left\vert \mathbf{v}\right\vert
\right\vert +\left\vert \left\vert \mathbf{w}\right\vert \right\vert \right)
^{2}$: per la positivit\`{a} della norma, questo \`{e} equivalente a $%
\left\vert \left\vert \mathbf{v}+\mathbf{w}\right\vert \right\vert \leq
\left\vert \left\vert \mathbf{v}\right\vert \right\vert +\left\vert
\left\vert \mathbf{w}\right\vert \right\vert $. $\blacksquare $

\textbf{Def} Dato uno spazio euclideo $\left( V,\langle \_,\_\mathbf{\rangle 
}\right) $, due vettori $\mathbf{v,w}\in V$ si dicono ortogonali con tale
prodotto scalare se $\langle \mathbf{v,w\rangle }=0$.

Quindi il vettore nullo \`{e} ortogonale a ogni vettore.

\begin{enumerate}
\item Prendo $V=L^{2}\left( \left[ 0,2\pi \right] \right) $, $f\left(
x\right) =1$, $g\left( x\right) =\cos x$. $\langle 1,\cos x\mathbf{\rangle =}%
\int_{0}^{2\pi }\cos xdx=0$, quindi $f$ e $g$ sono vettori ortogonali in $%
L^{2}\left( \left[ 0,2\pi \right] \right) $; lo stesso vale per $1$ e $\sin
x $, e per $\sin x$ e $\cos x$.
\end{enumerate}

\textbf{Teorema (Pitagora generalizzato)}%
\begin{gather*}
\text{Hp}\text{: }\left( V,\langle \_,\_\mathbf{\rangle }\right) \text{ \`{e}
uno spazio euclideo; }\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{k}\right\} 
\text{ \`{e} un insieme di vettori non nulli } \\
\text{di }V\text{ tali che }\forall \text{ }i,j=1,...,k\text{, }i\neq j\text{
implica }\langle \mathbf{v}_{i}\mathbf{,v}_{j}\mathbf{\rangle }=0 \\
\text{Ts}\text{: (i) }\left\vert \left\vert \alpha _{1}\mathbf{v}%
_{1}+...+\alpha _{k}\mathbf{v}_{k}\right\vert \right\vert ^{2}=\alpha
_{1}^{2}\left\vert \left\vert \mathbf{v}_{1}\right\vert \right\vert
^{2}+...+\alpha _{k}^{2}\left\vert \left\vert \mathbf{v}_{k}\right\vert
\right\vert ^{2}\text{, con }\alpha _{1},...,\alpha _{k}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\\
\text{(ii) }\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{k}\right\} \text{ \`{e}
un insieme di vettori linearmente indipendenti}
\end{gather*}

L'ipotesi significa che ogni coppia di vettori distinti \`{e} fatta da
vettori ortogonali. Si ha quindi un nuovo strumento per verificare
l'indipendenza lineare.

\textbf{Dim} (i) Usando la bilinearit\`{a} si ha $\left\vert \left\vert
\alpha _{1}\mathbf{v}_{1}+...+\alpha _{k}\mathbf{v}_{k}\right\vert
\right\vert ^{2}=\langle \alpha _{1}\mathbf{v}_{1}+...+\alpha _{k}\mathbf{v}%
_{k},\alpha _{1}\mathbf{v}_{1}+...+\alpha _{k}\mathbf{v}_{k}\mathbf{\rangle =%
}$ $\langle \alpha _{1}\mathbf{v}_{1},\alpha _{1}\mathbf{v}_{1}\mathbf{%
\rangle +...}\langle \alpha _{1}\mathbf{v}_{1},\alpha _{k}\mathbf{v}_{k}%
\mathbf{\rangle +...+}\langle \alpha _{k}\mathbf{v}_{k},\alpha _{1}\mathbf{v}%
_{1}\mathbf{\rangle +...+}\langle \alpha _{k}\mathbf{v}_{k},\alpha _{k}%
\mathbf{v}_{k}\mathbf{\rangle =}$ $\alpha _{1}^{2}\langle \mathbf{v}_{1},%
\mathbf{v}_{1}\mathbf{\rangle }+...+\alpha _{k}^{2}\langle \mathbf{v}_{k},%
\mathbf{v}_{k}\mathbf{\rangle +}2\langle \alpha _{1}\mathbf{v}_{1},\alpha
_{2}\mathbf{v}_{2}\mathbf{\rangle +...+}2\langle \alpha _{k-1}\mathbf{v}%
_{k-1},\alpha _{k}\mathbf{v}_{k}\mathbf{\rangle =}$ $\alpha _{1}^{2}\langle 
\mathbf{v}_{1},\mathbf{v}_{1}\mathbf{\rangle }+...+\alpha _{k}^{2}\langle 
\mathbf{v}_{k},\mathbf{v}_{k}\mathbf{\rangle +}2\alpha _{1}\alpha
_{2}\langle \mathbf{v}_{1},\mathbf{v}_{2}\mathbf{\rangle +...+}2\alpha
_{k-1}\alpha _{k}\langle \mathbf{v}_{k-1},\mathbf{v}_{k}\mathbf{\rangle =}%
\alpha _{1}^{2}\langle \mathbf{v}_{1},\mathbf{v}_{1}\mathbf{\rangle }%
+...+\alpha _{k}^{2}\langle \mathbf{v}_{k},\mathbf{v}_{k}\mathbf{\rangle =}%
\alpha _{1}^{2}\left\vert \left\vert \mathbf{v}_{1}\right\vert \right\vert
^{2}+...+\alpha _{k}^{2}\left\vert \left\vert \mathbf{v}_{k}\right\vert
\right\vert ^{2}$ perch\'{e} tutti i prodotti scalari misti sono nulli,
essendo $\langle \mathbf{v}_{i}\mathbf{,v}_{j}\mathbf{\rangle }=0$ se $i\neq
j$ per ipotesi.

(ii) Considero $\alpha _{1}\mathbf{v}_{1}+...+\alpha _{k}\mathbf{v}_{k}=%
\mathbf{0}$: \`{e} equivalente a $\left\vert \left\vert \alpha _{1}\mathbf{v}%
_{1}+...+\alpha _{k}\mathbf{v}_{k}\right\vert \right\vert =0$, che implica $%
\left\vert \left\vert \alpha _{1}\mathbf{v}_{1}+...+\alpha _{k}\mathbf{v}%
_{k}\right\vert \right\vert ^{2}=0$, i. e., per il primo punto del teorema, $%
\alpha _{1}^{2}\left\vert \left\vert \mathbf{v}_{1}\right\vert \right\vert
^{2}+...+\alpha _{k}^{2}\left\vert \left\vert \mathbf{v}_{k}\right\vert
\right\vert ^{2}=0$: una somma di quadrati \`{e} nulla se e solo se ogni
quadrato \`{e} nullo, cio\`{e} $\alpha _{i}^{2}\left\vert \left\vert \mathbf{%
v}_{i}\right\vert \right\vert ^{2}=0$ per ogni $i=1,...,k$, ma poich\'{e} $%
\mathbf{v}_{i}\mathbf{\neq 0}$ per ipotesi, $\alpha _{i}=0$ per ogni $%
i=1,...,k$ e i vettori sono linearmente indipendenti per definizione. $%
\blacksquare $

\subsection{Basi ortogonali e proiezioni ortogonali}

\textbf{Def} Dato uno spazio euclideo $\left( V,\langle \_,\_\mathbf{\rangle 
}\right) $, una base $B=\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{n}\right\} $
di $V$ si dice base ortogonale di $V$ se $\forall $ $i,j=1,...,n$, $i\neq j$
implica $\langle \mathbf{v}_{i}\mathbf{,v}_{j}\mathbf{\rangle }=0$; si dice
base ortonormale di $V$ se $\forall $ $i,j=1,...,n$, $i\neq j$ implica $%
\langle \mathbf{v}_{i}\mathbf{,v}_{j}\mathbf{\rangle }=0$ e $\langle \mathbf{%
v}_{i},\mathbf{v}_{i}\mathbf{\rangle }=1$ per ogni $i=1,...,n$.

Quindi una base ortonormale \`{e} fatta di vettori a due a due ortogonali e
con norma unitaria.

\begin{enumerate}
\item Considero $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, dotato di prodotto scalare standard. La base canonica di $V$ $\left\{ 
\mathbf{e}_{1}\mathbf{,...,e}_{n}\right\} $ \`{e} ortonormale.

\item Considero lo spazio euclideo $L^{2}\left( \left[ 0,2\pi \right]
\right) $, dotato di prodotto scalare $\langle f,g\mathbf{\rangle =}%
\int_{a}^{b}f\left( t\right) g\left( t\right) dt$. $V=Span\left( \mathbf{1}%
,\sin x,\cos x\right) $ sottospazio di $L^{2}$ \`{e} euclideo rispetto al
prodotto scalare di $L^{2}$. $\left\{ 1,\sin x,\cos x\right\} $ \`{e} una
base ortogonale di $V$ (si \`{e} calcolato sopra), ma non normale: $%
\left\vert \left\vert \mathbf{1}\right\vert \right\vert ^{2}=\langle \mathbf{%
1,1\rangle }=\int_{0}^{2\pi }1dx=2\pi \neq 1$.
\end{enumerate}

\textbf{Teorema (decomposizione rispetto a una base ortogonale)}%
\begin{gather*}
\text{Hp}\text{: }V\text{ \`{e} uno spazio euclideo, }H\subset V\text{ \`{e}
un sottospazio vettoriale di }V\text{;} \\
B=\left\{ \mathbf{b}_{1}\mathbf{,...,b}_{h}\right\} \text{ \`{e} una base
ortogonale di }H \\
\text{Ts}\text{: (i) }\forall \text{ }\mathbf{v}\in H\text{, }\mathbf{v}=%
\frac{\langle \mathbf{v,b}_{1}\mathbf{\rangle }}{\left\vert \left\vert 
\mathbf{b}_{1}\right\vert \right\vert ^{2}}\mathbf{b}_{1}+...+\frac{\langle 
\mathbf{v,b}_{h}\mathbf{\rangle }}{\left\vert \left\vert \mathbf{b}%
_{h}\right\vert \right\vert ^{2}}\mathbf{b}_{h} \\
\text{(ii) }\forall \text{ }\mathbf{v,w}\in H\text{, se }\mathbf{v}=x_{1}%
\mathbf{b}_{1}+...+x_{h}\mathbf{b}_{h}\text{, }\mathbf{w}=y_{1}\mathbf{b}%
_{1}+...+y_{h}\mathbf{b}_{h}\text{,} \\
\langle \mathbf{v,w\rangle }\mathbf{=}x_{1}y_{1}\left\vert \left\vert 
\mathbf{b}_{1}\right\vert \right\vert ^{2}+...+x_{h}y_{h}\left\vert
\left\vert \mathbf{b}_{h}\right\vert \right\vert ^{2}=\left(
x_{1}|...|x_{h}\right) \left[ 
\begin{array}{ccc}
\left\vert \left\vert \mathbf{b}_{1}\right\vert \right\vert ^{2} & \mathbf{%
...} & \mathbf{0} \\ 
\mathbf{...} & \mathbf{...} & \mathbf{...} \\ 
\mathbf{0} & \mathbf{...} & \left\vert \left\vert \mathbf{b}_{h}\right\vert
\right\vert ^{2}%
\end{array}%
\right] \left( 
\begin{array}{c}
y_{1} \\ 
... \\ 
y_{h}%
\end{array}%
\right) 
\end{gather*}

Quindi, le propriet\`{a} aggiuntive conferite a $H$ dalla definizione di un
prodotto scalare forniscono un nuovo strumento per determinare le coordinate
di un vettore di $H$ rispetto a una base ortogonale. Inoltre, (ii) d\`{a}
una formula analitica per calcolare il prodotto scalare in $V$ (rispetto a
cui si ha una base ortogonale): si pu\`{o} calcolare il prodotto scalare tra
due vettori senza ricorrere alla definizione, ma usando una formula
analitica che descrive il prodotto scalare "localmente", cio\`{e}
relativamente alla base scelta di $H$, sfruttando l'isomorfismo tra $%
Span\left( \mathbf{b}_{1}\mathbf{,...,b}_{h}\right) =V$ e $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{h}$. Questo significa che ogni prodotto scalare pu\`{o} essere calcolato
come prodotto scalare non standard in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{h}$ indotto dalla matrice diagonale delle norme dei vettori della base.

$\frac{\langle \mathbf{v,b}_{i}\mathbf{\rangle }}{\left\vert \left\vert 
\mathbf{b}_{i}\right\vert \right\vert ^{2}}$ si dice coefficiente di Fourier
di $\mathbf{v}$ rispetto a $\mathbf{b}_{i}$.

\textbf{Dim} (i) La rappresentazione rispetto a una base \`{e} unica: $%
\mathbf{v}=x_{1}\mathbf{b}_{1}+...+x_{n}\mathbf{b}_{h}$. Calcolo $\langle 
\mathbf{v,b}_{i}\mathbf{\rangle =}\langle x_{1}\mathbf{b}_{1}+...+x_{n}%
\mathbf{b}_{h},\mathbf{b}_{i}\rangle =x_{1}\langle \mathbf{b}_{1}\mathbf{,b}%
_{i}\mathbf{\rangle +...+}x_{i}\langle \mathbf{b}_{i}\mathbf{,b}_{i}\mathbf{%
\rangle +...+}x_{n}\langle \mathbf{b}_{h}\mathbf{,b}_{i}\mathbf{\rangle }%
=x_{i}\langle \mathbf{b}_{i}\mathbf{,b}_{i}\mathbf{\rangle }$ perch\'{e} la
base \`{e} ortogonale. Quindi $\langle \mathbf{v,b}_{i}\mathbf{\rangle }%
=x_{i}\left\vert \left\vert \mathbf{b}_{i}\right\vert \right\vert ^{2}$: $%
\mathbf{b}_{i}\neq \mathbf{0}$ perch\'{e} \`{e} un vettore della base,
quindi $\left\vert \left\vert \mathbf{b}_{i}\right\vert \right\vert ^{2}\neq
0$ e $x_{i}=\frac{\langle \mathbf{v,b}_{i}\mathbf{\rangle }}{\left\vert
\left\vert \mathbf{b}_{i}\right\vert \right\vert ^{2}}$.

(ii) $\mathbf{v}=x_{1}\mathbf{b}_{1}+...+x_{h}\mathbf{b}_{h}$, $\mathbf{w}%
=y_{1}\mathbf{b}_{1}+...+y_{h}\mathbf{b}_{h}$; $\langle \mathbf{v,w\rangle =}%
\langle x_{1}\mathbf{b}_{1}+...+x_{h}\mathbf{b}_{h}\mathbf{,}y_{1}\mathbf{%
\mathbf{b}_{1}+...+}y_{h}\mathbf{\mathbf{b}_{h}\rangle }$ $\mathbf{=}\langle
x_{1}\mathbf{b}_{1}\mathbf{,}y_{1}\mathbf{\mathbf{b}_{1}\rangle +...+}%
\langle x_{h}\mathbf{b}_{h}\mathbf{,}y_{h}\mathbf{\mathbf{b}_{h}\rangle +}%
x_{1}y_{2}\langle \mathbf{b}_{1}\mathbf{,\mathbf{b}_{2}\rangle +...+}%
x_{2}y_{1}\langle \mathbf{b}_{2}\mathbf{,\mathbf{b}_{1}\rangle +...=}$ (per
ortogonalit\`{a} della base) $\langle x_{1}\mathbf{b}_{1}\mathbf{,}y_{1}%
\mathbf{\mathbf{b}_{1}\rangle +...+}\langle x_{h}\mathbf{b}_{h}\mathbf{,}%
y_{h}\mathbf{\mathbf{b}_{h}\rangle =}x_{1}y_{1}\left\vert \left\vert \mathbf{%
b}_{1}\right\vert \right\vert ^{2}+...+x_{h}y_{h}\left\vert \left\vert 
\mathbf{b}_{h}\right\vert \right\vert ^{2}$. $\blacksquare $

\textbf{Corollario}%
\begin{gather*}
\text{Hp}\text{: }V\text{ \`{e} uno spazio euclideo, }H\subset V\text{ \`{e}
un sottospazio vettoriale di }V\text{;} \\
B=\left\{ \mathbf{b}_{1}\mathbf{,...,b}_{h}\right\} \text{ \`{e} una base
ortonormale di }H \\
\text{Ts}\text{: (i) }\forall \text{ }\mathbf{v}\in H\text{, }\mathbf{v}%
=\langle \mathbf{v,b}_{1}\mathbf{\rangle b}_{1}+...+\langle \mathbf{v,b}_{h}%
\mathbf{\rangle b}_{h} \\
\text{(ii) }\forall \text{ }\mathbf{v,w}\in H\text{, se }\mathbf{v}=x_{1}%
\mathbf{b}_{1}+...+x_{h}\mathbf{b}_{h}\text{, }\mathbf{w}=y_{1}\mathbf{b}%
_{1}+...+y_{h}\mathbf{b}_{h}\text{,} \\
\langle \mathbf{v,w\rangle }\mathbf{=}x_{1}y_{1}+...+x_{h}y_{h}=\left(
x_{1}|...|x_{h}\right) \left[ 
\begin{array}{ccc}
1 & \mathbf{...} & \mathbf{0} \\ 
\mathbf{...} & \mathbf{...} & \mathbf{...} \\ 
\mathbf{0} & \mathbf{...} & 1%
\end{array}%
\right] \left( 
\begin{array}{c}
y_{1} \\ 
... \\ 
y_{h}%
\end{array}%
\right) \\
\text{e }\left\vert \left\vert \mathbf{v}\right\vert \right\vert =\sqrt{%
\langle \mathbf{v,v\rangle }}\mathbf{=}\sqrt{x_{1}^{2}+...+x_{h}^{2}}
\end{gather*}

poich\'{e} $\left\vert \left\vert \mathbf{b}_{i}\right\vert \right\vert
^{2}=1$ per ogni $i=1,...,h$.

Quindi fondamentalmente il prodotto scalare in un qualsiasi spazio euclideo
pu\`{o} essere espresso come prodotto scalare standard in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{h}$ tra i vettori delle coordinate rispetto a una base ortonormale: cio%
\`{e} l'isomorfismo tra $V$ e $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{h}$ mediante una mappa delle coordinate rispetto a una base ortonormale
preserva il prodotto scalare.

\begin{enumerate}
\item Considero $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$ con la struttura euclidea standard. $B=\left\{ \left( 
\begin{array}{c}
2 \\ 
7%
\end{array}%
\right) ,\left( 
\begin{array}{c}
7 \\ 
-2%
\end{array}%
\right) \right\} $ \`{e} una base ortogonale di $V$, ma non normale. Cerco
la decomposizione di $\left( 
\begin{array}{c}
13 \\ 
5%
\end{array}%
\right) $ rispetto a $B$: senza il teorema, avrei risolto il sistema lineare 
$x_{1}\left( 
\begin{array}{c}
2 \\ 
7%
\end{array}%
\right) +x_{2}\left( 
\begin{array}{c}
7 \\ 
-2%
\end{array}%
\right) =\left( 
\begin{array}{c}
13 \\ 
5%
\end{array}%
\right) $. Ora so che $x_{1}=\frac{\langle \left( 
\begin{array}{c}
13 \\ 
5%
\end{array}%
\right) \mathbf{,}\left( 
\begin{array}{c}
2 \\ 
7%
\end{array}%
\right) \mathbf{\rangle }}{\left\vert \left\vert \left( 
\begin{array}{c}
2 \\ 
7%
\end{array}%
\right) \right\vert \right\vert ^{2}}=\frac{61}{53}$ e $x_{2}=\frac{\langle
\left( 
\begin{array}{c}
13 \\ 
5%
\end{array}%
\right) \mathbf{,}\left( 
\begin{array}{c}
7 \\ 
-2%
\end{array}%
\right) \mathbf{\rangle }}{\left\vert \left\vert \left( 
\begin{array}{c}
7 \\ 
-2%
\end{array}%
\right) \right\vert \right\vert ^{2}}=\frac{81}{53}$.

\item Considero $L^{2}\left( \left[ 0,2\pi \right] \right) $ e $V=Span\left(
1,\cos x,\sin x\right) $ ($\left\{ 1,\cos ,\sin \right\} $ \`{e} una base
ortogonale, non normale), $f\left( x\right) =\lambda _{1}+\lambda _{2}\sin
x+\lambda _{3}\cos x$, $g\left( x\right) =\mu _{1}+\mu _{2}\sin x+\mu
_{3}\cos x$. $\langle f\left( x\right) \mathbf{,}g\left( x\right) \mathbf{%
\rangle =}\int_{0}^{2\pi }\left( \lambda _{1}+\lambda _{2}\sin x+\lambda
_{3}\cos x\right) \left( \mu _{1}+\mu _{2}\sin x+\mu _{3}\cos x\right) dx$
per definizione. Ma usando il teorema, si ha $\langle f\left( x\right) 
\mathbf{,}g\left( x\right) \rangle =\lambda _{1}\mu _{1}\left\vert
\left\vert 1\right\vert \right\vert ^{2}+\lambda _{2}\mu _{2}\left\vert
\left\vert \sin x\right\vert \right\vert ^{2}+\lambda _{3}\mu _{3}\left\vert
\left\vert \cos x\right\vert \right\vert ^{2}=\left[ \lambda _{1}|\lambda
_{2}|\lambda _{3}\right] \left[ 
\begin{array}{ccc}
\left\vert \left\vert 1\right\vert \right\vert ^{2} & 0 & 0 \\ 
0 & \left\vert \left\vert \sin x\right\vert \right\vert ^{2} & 0 \\ 
0 & 0 & \left\vert \left\vert \cos x\right\vert \right\vert ^{2}%
\end{array}%
\right] \left[ 
\begin{array}{c}
\mu _{1} \\ 
\mu _{2} \\ 
\mu _{3}%
\end{array}%
\right] $. Si osserva che, dato $V$ isomorfo a $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$, $\left( V,\langle \_\mathbf{,}\_\rangle \right) $ \`{e} isomorfo a $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$ con prodotto scalare non standard, indotto dalla matrice $\left[ 
\begin{array}{ccc}
\left\vert \left\vert 1\right\vert \right\vert ^{2} & 0 & 0 \\ 
0 & \left\vert \left\vert \sin x\right\vert \right\vert ^{2} & 0 \\ 
0 & 0 & \left\vert \left\vert \cos x\right\vert \right\vert ^{2}%
\end{array}%
\right] $. Quindi anche la struttura di spazio euclideo pu\`{o} essere
espressa in termini matriciali.

\item Se $W$ \`{e} uno spazio euclideo e $f:V\rightarrow W$ \`{e}
un'applicazione lineare iniettiva, $f$ conferisce a $V$ la struttura di
spazio euclideo, con prodotto scalare tra $\mathbf{v}_{1}$ e $\mathbf{v}_{2}$
definito da $\left\langle \mathbf{v}_{1}\mathbf{,v}_{2}\right\rangle
_{V}=\langle f\left( \mathbf{v}_{1}\right) \mathbf{,}f\left( \mathbf{v}%
_{2}\right) \mathbf{\rangle }_{W}$ per ogni $\mathbf{v}_{1}\mathbf{,v}%
_{2}\in V$. Quindi ogni spazio vettoriale $V$ a dimensione finita, con base $%
B$, \`{e} dotato di un prodotto scalare indotto dalla mappa delle coordinate 
$X_{B}:V\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, che \`{e} iniettiva: il prodotto scalare tra $\mathbf{v}_{1}\mathbf{,v%
}_{2}\in V$ \`{e} $\left\langle \mathbf{v}_{1}\mathbf{,v}_{2}\right\rangle
_{V}=\left\langle X_{B}\left( \mathbf{v}_{1}\right) ,X_{B}\left( \mathbf{v}%
_{2}\right) \right\rangle _{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}}$ (se $X_{B}\left( \mathbf{v}_{1}\right) =\mathbf{u}_{1},X_{B}\left( 
\mathbf{v}_{2}\right) =\mathbf{u}_{2}$, $\left\langle \mathbf{u}_{1}\mathbf{%
,u}_{2}\right\rangle _{_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}}}=\left\langle P\left( \mathbf{u}_{1}\right) ,P\left( \mathbf{u}%
_{2}\right) \right\rangle _{V}$ con $P$ mappa delle coordinate). Perci\`{o}
ogni spazio vettoriale $V$ a dimensione finita pu\`{o} essere dotato di
infinite strutture di spazio euclideo (una per ogni base).
\end{enumerate}

Per il teorema visto, conoscere una base ortogonale di $H$ semplifica molte
cose, ad esempio permette di calcolare facilmente il prodotto scalare tra
vettori di $H$. E' quindi ragionevole chiedersi se \`{e} possibile costruire
una base ortogonale o ortonormale per qualsiasi sottospazio $H\subset V$: se
s\`{\i}, come si costruisce?

\begin{enumerate}
\item Considero $H$ sottospazio di $V$ spazio euclideo, $H=Span\left( 
\mathbf{v,w}\right) $, $\dim H=2$ ($H$ isomorfo a $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$). $B=\left\{ \mathbf{v,w}\right\} $ \`{e} una base generica di $H$;
posso trovare un insieme di vettori $\left\{ \mathbf{b}_{1}\mathbf{,b}%
_{2}\right\} $ tale che $\langle \mathbf{b}_{1}\mathbf{,b}_{2}\mathbf{%
\rangle }=0$ e che $Span\left( \mathbf{b}_{1}\mathbf{,b}_{2}\right)
=Span\left( \mathbf{v,w}\right) $? Sto cercando una base ortogonale. Prendo $%
\mathbf{b}_{1}\mathbf{=v}$ e cerco $\mathbf{b}_{2}=x\mathbf{v}+y\mathbf{w}$
(lo scrivo in questa forma perch\'{e} dev'essere $Span\left( \mathbf{b}_{1}%
\mathbf{,b}_{2}\right) =Span\left( \mathbf{v,w}\right) $), con $x\neq 0$:
quindi posso considerare $\mathbf{b}_{2}=\mathbf{v}+y\mathbf{w}$. Ora cerco $%
y:\langle \mathbf{b}_{1}\mathbf{,b}_{2}\mathbf{\rangle }=0$. $\langle 
\mathbf{b}_{1}\mathbf{,b}_{2}\mathbf{\rangle }=\langle \mathbf{v,v+}y\mathbf{%
w\rangle }=\langle \mathbf{v,v\rangle +}y\langle \mathbf{v,w\rangle =}%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}+y\langle 
\mathbf{v,w\rangle }$. $\langle \mathbf{b}_{1}\mathbf{,b}_{2}\mathbf{\rangle 
}=0\Longleftrightarrow \left\vert \left\vert \mathbf{v}\right\vert
\right\vert ^{2}+y\langle \mathbf{v,w\rangle }=0\Longleftrightarrow y=-\frac{%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}}{\langle 
\mathbf{v,w\rangle }}$: $y$ \`{e} ben definito perch\'{e} $\langle \mathbf{%
v,w\rangle }\neq 0$, dato che si \`{e} partiti da una base non ortogonale.
Quindi la base ortogonale trovata di $H$ \`{e} $\left\{ \mathbf{v,v}-\frac{%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}}{\langle 
\mathbf{v,w\rangle }}\mathbf{w}\right\} $, o, considerando un multiplo del
secondo vettore, $\left\{ \mathbf{v,w-}\frac{\langle \mathbf{v,w\rangle }}{%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}}\mathbf{v}%
\right\} $: $\frac{\langle \mathbf{v,w\rangle }}{\left\vert \left\vert 
\mathbf{v}\right\vert \right\vert ^{2}}$ \`{e} il coefficiente di Fourier di 
$\mathbf{w}$ rispetto a $\mathbf{v}$. $\frac{\langle \mathbf{v,w\rangle }}{%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}}\mathbf{v}$ 
\`{e} il vettore proiezione ortogonale di $\mathbf{w}$ su $\mathbf{v}$.
Infatti in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{2}$ $\frac{\langle \mathbf{v,w\rangle }}{\left\vert \left\vert \mathbf{v}%
\right\vert \right\vert ^{2}}\mathbf{v=}\frac{\left\vert \left\vert \mathbf{v%
}\right\vert \right\vert \left\vert \left\vert \mathbf{w}\right\vert
\right\vert \cos \theta }{\left\vert \left\vert \mathbf{v}\right\vert
\right\vert }\cdot \frac{\mathbf{v}}{\left\vert \left\vert \mathbf{v}%
\right\vert \right\vert }=\left\vert \left\vert \mathbf{w}\right\vert
\right\vert \cos \theta \cdot \mathbf{1}$: $\frac{\mathbf{v}}{\left\vert
\left\vert \mathbf{v}\right\vert \right\vert }$ \`{e} un vettore di norma
unitaria.
\end{enumerate}

\textbf{Def} Sia $\left( V,\langle \_,\_\mathbf{\rangle }\right) $ uno
spazio euclideo. Se $S$ \`{e} un qualsiasi sottinsieme di $V$, il
sottinsieme di $V$ $S^{\perp }=\left\{ \mathbf{v}\in V:\langle \mathbf{%
v,w\rangle }=0\text{ }\forall \text{ }\mathbf{w}\in S\right\} $ \`{e}
l'insieme dei vettori di $V$ che sono ortogonali a ogni vettore di $S$.

\textbf{Proposizione}%
\begin{eqnarray*}
\text{Hp} &\text{: }&V,\langle \_,\_\mathbf{\rangle }\text{ \`{e} uno spazio
euclideo, }S\subset V \\
\text{Ts} &\text{: }&S^{\perp }\text{ \`{e} un sottospazio vettoriale di }V
\end{eqnarray*}

\textbf{Dim} Mostro che $\forall $ $\mathbf{v}_{1}\mathbf{,v}_{2}\in
S^{\perp }$, per ogni $\lambda _{1},\lambda _{2}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, $\lambda _{1}\mathbf{v}_{1}+\lambda _{2}\mathbf{v}_{2}\in S^{\perp }$.
Infatti per ogni $\mathbf{w}\in S$ $\langle \lambda _{1}\mathbf{v}_{1}%
\mathbf{+}\lambda _{2}\mathbf{v}_{2},\mathbf{w\rangle =}\lambda _{1}\langle 
\mathbf{v}_{1}\mathbf{,w\rangle +}\lambda _{2}\langle \mathbf{v}_{2}\mathbf{%
,w\rangle }=0$ perch\'{e} $\mathbf{v}_{1}\mathbf{,v}_{2}\in S^{\perp }$,
quindi anche $\lambda _{1}\mathbf{v}_{1}+\lambda _{2}\mathbf{v}_{2}\in
S^{\perp }$. $\blacksquare $

Ci interessa in particolare il caso di $S=H$ sottospazio vettoriale.

\textbf{Proposizione} 
\begin{gather*}
\text{Hp}\text{: }V,\langle \_,\_\mathbf{\rangle }\text{ \`{e} uno spazio
euclideo, }H\text{ \`{e} un sottospazio vettoriale di }V\text{, }B=\left\{ 
\mathbf{b}_{1}\mathbf{,...,b}_{h}\right\} \text{ \`{e} una base di }H \\
\text{Ts}\text{: }H^{\perp }=B^{\perp }
\end{gather*}

cio\`{e} per trovare il complemento ortogonale di $H$ \`{e} sufficiente
trovare quello di $B$, che ha un insieme finito di elementi.

\textbf{Dim} Mostro che $H^{\perp }\subseteq B^{\perp }$, cio\`{e} se $%
\mathbf{v}$ \`{e} tale che $\langle \mathbf{v,w\rangle }=0$ $\forall $ $%
\mathbf{w}\in H$, allora $\mathbf{v}$ \`{e} tale che $\langle \mathbf{v,b}%
_{i}\rangle =0$ $\forall $ $\mathbf{b}_{i}\in B$. Infatti, se $\mathbf{v}$ 
\`{e} ortogonale a ogni vettore di $H$, allora \`{e} ortogonale in
particolare a ogni vettore di $B$, perch\'{e} i vettori di $B$ appartengono
ad $H$. Mostro che $B^{\perp }\subseteq H^{\perp }$, cio\`{e} se $\mathbf{v}$
\`{e} tale che $\langle \mathbf{v,b}_{i}\rangle =0$ $\forall $ $\mathbf{b}%
_{i}\in B$, allora $\mathbf{v}$ \`{e} tale che $\langle \mathbf{v,w\rangle }%
=0$ $\forall $ $\mathbf{w}\in H$. Infatti, se $\mathbf{w}=x_{1}\mathbf{b}%
_{1}+...+x_{h}\mathbf{b}_{h}$, $\langle \mathbf{v,w\rangle =}\langle \mathbf{%
v,}x_{1}\mathbf{\mathbf{b}_{1}+...+}x_{h}\mathbf{\mathbf{b}_{h}\rangle =}%
x_{1}\langle \mathbf{v,b}_{1}\mathbf{\rangle +...+}x_{h}\langle \mathbf{v,b}%
_{h}\rangle =0$ perch\'{e} $\mathbf{v}$ \`{e} ortogonale a ogni vettore di $%
B $. Quindi $H^{\perp }=B^{\perp }$. $\blacksquare $

\begin{enumerate}
\item Dati i sottospazi $U,V$, se $U\subseteq V$, allora $V^{\perp
}\subseteq U^{\perp }$.
\end{enumerate}

\textbf{Teorema (proiezione ortogonale per spazi euclidei a dimensione
finita)}%
\begin{gather*}
\text{Hp}\text{: }\left( V,\langle \_,\_\mathbf{\rangle }\right) \text{ \`{e}
uno spazio euclideo, }\dim V<+\infty \text{, }H\text{ \`{e} un sottospazio
vettoriale di }V \\
\text{Ts}\text{: (i) }H^{\perp }\cap H=\left\{ \mathbf{0}\right\}  \\
\text{(ii) }\forall \text{ }\mathbf{v}\in V\text{ }\exists \text{ }!\mathbf{v%
}_{H}\in H:\mathbf{v-v}_{H}\in H^{\perp } \\
\text{(iii) }\left\vert \left\vert \mathbf{v-v}_{H}\right\vert \right\vert
\leq \left\vert \left\vert \mathbf{v-w}\right\vert \right\vert \text{ per
ogni }\mathbf{w}\in H
\end{gather*}

cio\`{e} l'intersezione tra i due sottospazi \`{e} la pi\`{u} piccola
possibile. $\mathbf{v}$ in (ii) \`{e} quello che nell'esempio precedente era 
$\mathbf{w}$ e $\mathbf{v}_{H}$ era la proiezione di $\mathbf{w}$ su $%
\mathbf{v}$. (iii) significa che $\mathbf{v}_{H}$, tra tutti quelli di $H$, 
\`{e} il vettore pi\`{u} vicino a $\mathbf{v}$, cio\`{e} \`{e} la migliore
approssimazione di $\mathbf{v}$ che si pu\`{o} ricavare da $H$. (ii)
significa che ogni vettore di $V$ pu\`{o} essere scritto come somma di un
vettore di $H$ e un vettore di $H^{\perp }$: quindi $V=H+H^{\perp }$. Poich%
\'{e} questo \`{e} possibile in un unico modo (aggiungendo (i)), si ha $%
V=H\oplus H^{\perp }$.

\textbf{Dim} (i) Se $\mathbf{v}\in H^{\perp }\cap H$, $\mathbf{v}$ \`{e} un
vettore di $H$ che \`{e} ortogonale a tutti i vettori di $H$, incluso se
stesso, quindi $\langle \mathbf{v,v}\rangle =0$: questo implica $\mathbf{v=0}
$ per positivit\`{a} del prodotto scalare.

(ii) Considero $B=\left\{ \mathbf{b}_{1}\mathbf{,...,b}_{h}\right\} $ base
ortogonale di $H$ (in realt\`{a} non abbiamo ancora mostrato che esiste).
Dato $\mathbf{v}\in V$, cerco $\mathbf{v}_{H}=x_{1}\mathbf{b}_{1}+...+x_{h}%
\mathbf{b}_{h}:\mathbf{v-v}_{H}=\mathbf{v}-x_{1}\mathbf{b}_{1}-...-x_{h}%
\mathbf{b}_{h}\in H^{\perp }$. $H^{\perp }=B^{\perp }$, quindi $\mathbf{v-v}%
_{H}\in H^{\perp }\Longleftrightarrow \mathbf{v-v}_{H}\in B^{\perp
}\Longleftrightarrow $ $\langle \mathbf{v-v}_{H}\mathbf{,b}_{i}\rangle =0$ $%
\forall $ $\mathbf{b}_{i}\in B$. $\langle \mathbf{v-v}_{H}\mathbf{,b}%
_{i}\rangle =\langle \mathbf{v}-x_{1}\mathbf{b}_{1}-...x_{i}\mathbf{b}%
_{i}-...-x_{h}\mathbf{b}_{h},\mathbf{b}_{i}\rangle =$ $\langle \mathbf{v},%
\mathbf{b}_{i}\rangle -x_{1}\langle \mathbf{b}_{1},\mathbf{b}_{i}\rangle
-...-x_{i}\langle \mathbf{b}_{i},\mathbf{b}_{i}\rangle -...-x_{h}\langle 
\mathbf{b}_{h},\mathbf{b}_{i}\rangle =$ (per ortogonalit\`{a} della base) $%
\langle \mathbf{v},\mathbf{b}_{i}\rangle -x_{i}\langle \mathbf{b}_{i},%
\mathbf{b}_{i}\rangle $. Quindi $\langle \mathbf{v-v}_{H}\mathbf{,b}%
_{i}\rangle =0$ se e solo se $\langle \mathbf{v},\mathbf{b}_{i}\rangle
-x_{i}\langle \mathbf{b}_{i},\mathbf{b}_{i}\rangle =\langle \mathbf{v},%
\mathbf{b}_{i}\rangle -x_{i}\left\vert \left\vert \mathbf{b}_{i}\right\vert
\right\vert ^{2}=0$, cio\`{e} $x_{i}=\frac{\langle \mathbf{v},\mathbf{b}%
_{i}\rangle }{\left\vert \left\vert \mathbf{b}_{i}\right\vert \right\vert
^{2}}$: \`{e} il coefficiente di Fourier di $\mathbf{v}$ rispetto a $\mathbf{%
b}_{i}$. Dunque $\exists $ $\mathbf{v}_{H}$ con le propriet\`{a} richieste: $%
\mathbf{v}_{H}=\frac{\langle \mathbf{v},\mathbf{b}_{1}\rangle }{\left\vert
\left\vert \mathbf{b}_{1}\right\vert \right\vert ^{2}}\mathbf{b}_{1}+...+%
\frac{\langle \mathbf{v},\mathbf{b}_{h}\rangle }{\left\vert \left\vert 
\mathbf{b}_{h}\right\vert \right\vert ^{2}}\mathbf{b}_{h}$. Mostro che \`{e}
unico. Suppongo esista un secondo $\mathbf{v}_{H}^{\prime }$ con le propriet%
\`{a} richieste: allora $\mathbf{v=v}_{H}\mathbf{+}\left( \mathbf{v-v}%
_{H}\right) $ e $\mathbf{v=v}_{H}^{\prime }\mathbf{+}\left( \mathbf{v-v}%
_{H}^{\prime }\right) $. Questo \`{e} equivalente a $\mathbf{v-v}=\mathbf{0}=%
\mathbf{v}_{H}+\left( \mathbf{v-v}_{H}\right) -\mathbf{v}_{H}^{\prime }%
\mathbf{-}\left( \mathbf{v-v}_{H}^{\prime }\right) $, cio\`{e} $\mathbf{v}%
_{H}\mathbf{-v}_{H}^{\prime }\mathbf{=v-v}_{H}^{\prime }\mathbf{-}\left( 
\mathbf{v-v}_{H}\right) $. $\mathbf{v}_{H}\mathbf{-v}_{H}^{\prime }\in H$, $%
\mathbf{v-v}_{H}^{\prime }\mathbf{-}\left( \mathbf{v-v}_{H}\right) \in
H^{\perp }$ (sono entrambi sottospazi): l'unico elemento nell'intersezione 
\`{e} il vettore nullo, quindi pu\`{o} essere solo $\mathbf{v}_{H}=\mathbf{v}%
_{H}^{\prime }$.

(iii) Mostro che $\left\vert \left\vert \mathbf{v-v}_{H}\right\vert
\right\vert \leq \left\vert \left\vert \mathbf{v-w}\right\vert \right\vert $
per ogni $\mathbf{w}\in H$. Considero $\mathbf{v-w=v-v}_{H}\mathbf{+v}_{H}%
\mathbf{-w=}\left( \mathbf{v-v}_{H}\right) \mathbf{+}\left( \mathbf{v}_{H}%
\mathbf{-w}\right) $. $\mathbf{v}_{H}\mathbf{-w}\in H$ perch\'{e}
combinazione lineare di vettori di $H$, $\mathbf{v-v}_{H}\in H^{\perp }$ per
costruzione, quindi $\langle \mathbf{v}_{H}-\mathbf{w},\mathbf{v-v}%
_{H}\rangle =0$. Posso applicare il teorema di Pitagora a $\left\{ \mathbf{v}%
_{H}-\mathbf{w},\mathbf{v-v}_{H}\right\} $: $\left\vert \left\vert \mathbf{v}%
_{H}-\mathbf{w}\right\vert \right\vert ^{2}+\left\vert \left\vert \mathbf{v-v%
}_{H}\right\vert \right\vert ^{2}=\left\vert \left\vert \mathbf{v-w}%
\right\vert \right\vert ^{2}$. $\left\vert \left\vert \mathbf{v}_{H}-\mathbf{%
w}\right\vert \right\vert ^{2}+\left\vert \left\vert \mathbf{v-v}%
_{H}\right\vert \right\vert ^{2}\geq \left\vert \left\vert \mathbf{v-v}%
_{H}\right\vert \right\vert ^{2}$, quindi $\left\vert \left\vert \mathbf{v-w}%
\right\vert \right\vert ^{2}\geq \left\vert \left\vert \mathbf{v-v}%
_{H}\right\vert \right\vert ^{2}\Longleftrightarrow \left\vert \left\vert 
\mathbf{v-w}\right\vert \right\vert \geq \left\vert \left\vert \mathbf{v-v}%
_{H}\right\vert \right\vert $. $\blacksquare $

\textbf{Corollario}%
\begin{gather*}
\text{Hp}\text{: }\left( V,\langle \_,\_\mathbf{\rangle }\right) \text{ \`{e}
uno spazio euclideo, }\dim V<+\infty \text{, }H\text{ \`{e} un sottospazio
vettoriale di }V \\
\text{Ts}\text{: (i) }H^{\perp }\oplus H=V \\
\text{(ii) }\dim H^{\perp }=\dim V-\dim H \\
\text{(iii) }\left( H^{\perp }\right) ^{\perp }=H
\end{gather*}

(iii) non vale in spazi a dimensione infinita.

\textbf{Dim} (iii) Se $\mathbf{v}\in H$, $\mathbf{v}$ \`{e} ortogonale a
tutti gli elementi di $H^{\perp }$: $\mathbf{v\perp w}$ per ogni $\mathbf{w}%
\in H^{\perp }$, cio\`{e} $\mathbf{v}\in \left( H^{\perp }\right) ^{\perp }$%
: quindi $H\subseteq \left( H^{\perp }\right) ^{\perp }$. Per (i) applicata
a $H^{\perp }$, vale $\left( H^{\perp }\right) ^{\perp }\oplus H^{\perp }=V$%
, quindi $\dim \left( H^{\perp }\right) ^{\perp }=\dim V-\dim H^{\perp }$.
Poich\'{e} vale anche $H^{\perp }\oplus H=V$, $\dim H=\dim V-\dim H^{\perp }$%
: quindi $\dim \left( H^{\perp }\right) ^{\perp }=\dim H$, ed essendo $%
H\subseteq \left( H^{\perp }\right) ^{\perp }$, $H=\left( \ H^{\perp
}\right) ^{\perp }$. $\blacksquare $

$H^{\perp }$ si dice complemento ortogonale di $H$.

Dunque, ogni vettore $\mathbf{v}\in V$ pu\`{o} essere scritto nella forma $%
\mathbf{v=v}_{H}\mathbf{+v}_{H^{\perp }}$, dove $\mathbf{v}_{H^{\perp }}\in
H^{\perp }$ e $\mathbf{v}_{H}\in H$: data $B=\left\{ \mathbf{b}_{1}\mathbf{%
,...,b}_{h}\right\} $ base ortogonale di $H$, $\mathbf{v}_{H}=\frac{\langle 
\mathbf{v},\mathbf{b}_{1}\rangle }{\left\vert \left\vert \mathbf{b}%
_{1}\right\vert \right\vert ^{2}}\mathbf{b}_{1}+...+\frac{\langle \mathbf{v},%
\mathbf{b}_{h}\rangle }{\left\vert \left\vert \mathbf{b}_{h}\right\vert
\right\vert ^{2}}\mathbf{b}_{h}$ si dice proiezione ortogonale di $\mathbf{v}
$ sul sottospazio $H$.

\textbf{Teorema (algoritmo di Gram-Schmidt)}

\begin{gather*}
\text{Hp}\text{: }\left( V,\langle \_,\_\mathbf{\rangle }\right) \text{ \`{e}
uno spazio euclideo, }\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{d}\right\} 
\text{ \`{e} un insieme di vettori linearmente indipendenti} \\
\text{Ts}\text{:}\text{ esistono }d\text{ vettori }\left\{ \mathbf{b}_{1}%
\mathbf{,..,b}_{d}\right\} :\forall \text{ }k=1,...,d\text{ }\left\{ \mathbf{%
b}_{1}\mathbf{,..,b}_{k}\right\} \text{ \`{e} una base ortogonale } \\
\text{del sottospazio}\text{generato da }\left\{ \mathbf{v}_{1}\mathbf{,...,v%
}_{k}\right\}
\end{gather*}

Quindi considero $\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{k}\right\} $, con $%
k $ che varia da $1$ a $d$, e per ogni $k$ posso trovare una base ortogonale
di $Span\left( \mathbf{v}_{1}\mathbf{,...,v}_{k}\right) $: se $k=1$ $\left\{ 
\mathbf{b}_{1}\right\} $, se $k=2$ $\left\{ \mathbf{b}_{1},\mathbf{b}%
_{2}\right\} $, se $k=3$ $\left\{ \mathbf{b}_{1},\mathbf{b}_{2}\mathbf{,b}%
_{3}\right\} $,..., se $k=d$ $\left\{ \mathbf{b}_{1},\mathbf{b}_{2}\mathbf{,b%
}_{3},...,\mathbf{b}_{d}\right\} $. Una base con un solo vettore \`{e}
sempre ortogonale. Dato $k=2$ e $\left\{ \mathbf{v}_{1}\mathbf{,v}%
_{2}\right\} $, la base \`{e} formata da $\mathbf{b}_{1}\mathbf{=v}_{1}$ e%
\textbf{\ }$\mathbf{b}_{2}\mathbf{=v}_{2}-\frac{\langle \mathbf{v}_{2}%
\mathbf{,b}_{1}\rangle }{\left\vert \left\vert \mathbf{b}_{1}\right\vert
\right\vert ^{2}}\mathbf{b}_{1}$: $\frac{\langle \mathbf{v}_{2}\mathbf{,b}%
_{1}\rangle }{\left\vert \left\vert \mathbf{b}_{1}\right\vert \right\vert
^{2}}\mathbf{b}_{1}$ \`{e} la proiezione ortogonale di $\mathbf{v}_{2}$ su $%
Span\left( \mathbf{b}_{1}\right) $ (infatti voglio che $\mathbf{b}_{2}\in
H^{\perp }$ con $H=Span\left( \mathbf{b}_{1}\right) $, quindi, essendo $%
\mathbf{v}_{2}\mathbf{=b}_{H}\mathbf{+b}_{H^{\perp }}$, prendo solo la
componente ortogonale a $H$). Dato $k=3$ e $\left\{ \mathbf{v}_{1}\mathbf{,v}%
_{2},\mathbf{v}_{3}\right\} $, la base \`{e} formata da $\mathbf{b}_{1}%
\mathbf{=v}_{1}$,\textbf{\ }$\mathbf{b}_{2}\mathbf{=v}_{2}-\frac{\langle 
\mathbf{v}_{2}\mathbf{,b}_{1}\rangle }{\left\vert \left\vert \mathbf{b}%
_{1}\right\vert \right\vert ^{2}}\mathbf{b}_{1}$ e $\mathbf{b}_{3}\mathbf{=v}%
_{3}-\left( \frac{\langle \mathbf{v}_{3}\mathbf{,b}_{1}\rangle }{\left\vert
\left\vert \mathbf{b}_{1}\right\vert \right\vert ^{2}}\mathbf{b}_{1}+\frac{%
\langle \mathbf{v}_{3}\mathbf{,b}_{2}\rangle }{\left\vert \left\vert \mathbf{%
b}_{2}\right\vert \right\vert ^{2}}\mathbf{b}_{2}\right) $: $\frac{\langle 
\mathbf{v}_{3}\mathbf{,b}_{1}\rangle }{\left\vert \left\vert \mathbf{b}%
_{1}\right\vert \right\vert ^{2}}\mathbf{b}_{1}+\frac{\langle \mathbf{v}_{3}%
\mathbf{,b}_{2}\rangle }{\left\vert \left\vert \mathbf{b}_{2}\right\vert
\right\vert ^{2}}\mathbf{b}_{2}$ \`{e} la proiezione ortogonale di $\mathbf{v%
}_{3}$ su $Span\left( \mathbf{b}_{1},\mathbf{b}_{2}\right) $. Analogamente, $%
\mathbf{b}_{d}=\mathbf{v}_{d}-\sum_{i=1}^{d-1}\frac{\langle \mathbf{v}_{d}%
\mathbf{,b}_{i}\rangle }{\left\vert \left\vert \mathbf{b}_{i}\right\vert
\right\vert ^{2}}\mathbf{b}_{i}$.

Infatti, se scrivo un vettore $\mathbf{v}=\frac{\langle \mathbf{v,b}_{1}%
\mathbf{\rangle }}{\left\vert \left\vert \mathbf{b}_{1}\right\vert
\right\vert ^{2}}\mathbf{b}_{1}+...+\frac{\langle \mathbf{v,b}_{h}\mathbf{%
\rangle }}{\left\vert \left\vert \mathbf{b}_{h}\right\vert \right\vert ^{2}}%
\mathbf{b}_{h}$, sottraendo $\sum_{i=1}^{d-1}\frac{\langle \mathbf{v}_{d}%
\mathbf{,b}_{i}\rangle }{\left\vert \left\vert \mathbf{b}_{i}\right\vert
\right\vert ^{2}}\mathbf{b}_{i}$ rimane la componente ortogonale a ogni $%
\mathbf{b}_{i}$ per $i=1,...,d-1$.

Se voglio avere una base ortonormale, normalizzo ogni vettore della base
ortogonale moltiplicandolo per il reciproco della sua norma: la base
ortonormale \`{e} $\left\{ \mathbf{\tilde{b}}_{1}\mathbf{,..,\tilde{b}}%
_{d}\right\} =\left\{ \frac{\mathbf{b}_{1}}{\left\vert \left\vert \mathbf{b}%
_{1}\right\vert \right\vert }\mathbf{,..,}\frac{\mathbf{b}_{d}}{\left\vert
\left\vert \mathbf{b}_{d}\right\vert \right\vert }\right\} $.

\begin{enumerate}
\item $V=L^{2}\left( \left[ 0,2\pi \right] \right) $. Tutte le funzioni
continue definite in $\left[ 0,2\pi \right] $ appartengono a $V$, in
particolare le funzioni trigonometriche $\left\{ \sin \left( kx\right) ,\cos
\left( kx\right) ,...\right\} $. Interpreto $f\left( x\right) \in V$ come
segnale di un suono: voglio comprimere il segnale, cio\`{e} scrivere $f$
come combinazione lineare dei $2k+1$ vettori $\left\{ 1,...,\sin \left(
kx\right) ,\cos \left( kx\right) \right\} $. Qual \`{e} l'elemento di $%
Span\left( 1,...,\sin \left( kx\right) ,\cos \left( kx\right) \right) $ che
meglio approssima $f$, cio\`{e} la $g$ tale che $\left\vert \left\vert
f-g\right\vert \right\vert $ \`{e} minima? Per il teorema visto $g$ \`{e} la
proiezione ortogonale di $f$ su $Span\left( 1,...,\sin \left( kx\right)
,\cos \left( kx\right) \right) $.

\item $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
\left[ x\right] _{\leq 2}$\textit{, con }$\left\langle p\left( x\right)
,q\left( x\right) \right\rangle =\int_{0}^{2}p\left( x\right) q\left(
x\right) dx$, $H=\left\{ p\left( x\right) \in V:p\left( 2\right) =0\right\} $%
\textit{. Trovare una base ortogonale di }$H$\textit{\ il cui primo elemento
sia }$p\left( x\right) =x-2$\textit{.}

Si pu\`{o} procedere in due modi. Una volta appurato che $\dim H=2$, si ha
che una base generica di $V$, il cui primo elemento sia $p\left( x\right) $, 
\`{e} $\left\{ x-2,\left( x-2\right) \left( x-a\right) \right\} $ al variare
di $a\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, infatti $x-2,\left( x-2\right) \left( x-a\right) $ sono due vettori
linearmente indipendenti di $H$. Si cerca quindi $a:\int_{0}^{2}\left(
x-2\right) \left( x-2\right) \left( x-a\right) dx=0$. Risolvendo l'equazione
rispetto ad $a$, si trova $a=\frac{1}{2}$, quindi una base ortogonale \`{e} $%
\left\{ x-2,\left( x-2\right) \left( x-\frac{1}{2}\right) \right\} $.

Si pu\`{o} anche usare l'algoritmo di Gram-Schmidt su una base qualsiasi
come $\left\{ x-2,x^{2}-4\right\} =\left\{ \mathbf{b}_{1}\mathbf{,b}%
_{2}\right\} $. Dato $\mathbf{v}_{1}=x-2$, $\mathbf{v}_{2}\mathbf{=b}%
_{2}-P_{Span\left( \mathbf{v}_{1}\right) }\left( \mathbf{b}_{2}\right) =%
\mathbf{b}_{2}-\frac{\left\langle \mathbf{b}_{2},\mathbf{v}_{1}\right\rangle 
}{\left\vert \left\vert \mathbf{v}_{1}\right\vert \right\vert }\mathbf{v}%
_{1}=x-2-\frac{\int_{0}^{2}\left( x^{2}-4\right) \left( x-2\right) dx}{%
\int_{0}^{2}\left( x-2\right) ^{2}dx}\left( x-2\right) =\left( x-2\right)
\left( x-\frac{1}{2}\right) $.

\item \textit{Dato lo spazio euclideo }$M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( 2,2\right) $\textit{\ e definito }$\langle A,B\rangle =tr\left(
B^{T}A\right) $\textit{, trovare tutte le matrici ortogonali ad }$A=\left[ 
\begin{array}{cc}
1 & 2 \\ 
0 & 0%
\end{array}%
\right] $\textit{.}

E' richiesto di trovare $H^{\perp }$, dove $H=Span\left( A\right) $. Poich%
\'{e} $H^{\perp }=B^{\perp }$, dove $B=\left\{ A\right\} $, cerco l'insieme
delle matrici ortogonali ad $A$. Una generica matrice $B$ \`{e} $\left[ 
\begin{array}{cc}
a & b \\ 
c & d%
\end{array}%
\right] $: se \`{e} ortogonale ad $A$, vale $tr\left( B^{T}A\right)
=tr\left( \left[ 
\begin{array}{cc}
a & c \\ 
b & d%
\end{array}%
\right] \left[ 
\begin{array}{cc}
1 & 2 \\ 
0 & 0%
\end{array}%
\right] \right) =tr\left( 
\begin{array}{cc}
a & 2a \\ 
b & 2b%
\end{array}%
\right) =a+2b=0$. Risolvendo il sistema si ottiene $\left( 
\begin{array}{c}
a \\ 
b \\ 
c \\ 
d%
\end{array}%
\right) =t\left( 
\begin{array}{c}
0 \\ 
0 \\ 
1 \\ 
0%
\end{array}%
\right) +s\left( 
\begin{array}{c}
0 \\ 
0 \\ 
0 \\ 
1%
\end{array}%
\right) +u\left( 
\begin{array}{c}
-2 \\ 
1 \\ 
0 \\ 
0%
\end{array}%
\right) $, quindi una base di $H^{\perp }$ \`{e} $\left\{ \left[ 
\begin{array}{cc}
0 & 0 \\ 
1 & 0%
\end{array}%
\right] ,\left[ 
\begin{array}{cc}
0 & 0 \\ 
0 & 1%
\end{array}%
\right] ,\left[ 
\begin{array}{cc}
-2 & 1 \\ 
0 & 0%
\end{array}%
\right] \right\} $.
\end{enumerate}

Quindi, se $\left( V,\langle \_,\_\rangle \right) $ \`{e} uno spazio
euclideo a dimensione finita e $H\subset V$ \`{e} un sottospazio vettoriale,
la funzione $P_{H}:V\rightarrow V$ che associa a ogni vettore $\mathbf{v}$
la sua proiezione ortogonale su $H$ $\mathbf{v}_{H}$ (usato con lo stesso
significato del teorema sulla proiezione ortogonale) \`{e} un'applicazione
lineare, perch\'{e} \`{e} sia omogenea che additiva. Se $B=\left\{ \mathbf{b}%
_{1}\mathbf{,...,b}_{h}\right\} $ \`{e} una base ortogonale di $H$, il
vettore proiezione ortogonale di $\mathbf{v}$ su $H$ \`{e} $\mathbf{v}_{H}=%
\frac{\langle \mathbf{v},\mathbf{b}_{1}\rangle }{\left\vert \left\vert 
\mathbf{b}_{1}\right\vert \right\vert ^{2}}\mathbf{b}_{1}+...+\frac{\langle 
\mathbf{v},\mathbf{b}_{h}\rangle }{\left\vert \left\vert \mathbf{b}%
_{h}\right\vert \right\vert ^{2}}\mathbf{b}_{h}$.

Ha quindi senso chiedersi quale nucleo, immagine, autovalori, autovettori ha 
$P_{H}$. Con osservazioni geometriche possiamo dire che $\func{Im}\left(
P_{H}\right) =H$ (tutti gli elementi cui si applica la proiezione
appartengono ad $H$) e $\ker \left( P_{H}\right) $ \`{e} l'insieme dei
vettori ortogonali ad $H$: $\ker \left( P_{H}\right) =H^{\perp }$ (come si
evince anche dalla formula analitica). Se si prende come $H$ un piano
passante per l'origine, $\ker \left( P_{H}\right) $ \`{e} la retta
ortogonale al piano passante per l'origine. Il teorema di nullit\`{a} pi\`{u}
rango afferma che $\dim V=\dim \left( \func{Im}P_{H}\right) +\dim \left(
\ker P_{H}\right) =\dim H+\dim H^{\perp }$, che \`{e} la stessa cosa
affermata dalla formula di Grassman per la somma diretta $V=H\oplus H^{\perp
}$.

Osservo che $P_{H}\circ P_{H}=P_{H}$, perch\'{e} $P_{H}$ associa a $\mathbf{v%
}$ la sua proiezione ortogonale su $H$ $\mathbf{v}_{H}$, e $P_{H}$ applicata
a $\mathbf{v}_{H}$ lo lascia inalterato (si dimostra mostrando che $P_{H}$
applicata a qualsiasi vettore di una base ortogonale di $H$ lo lascia
inalterato): $P_{H}^{2}=P_{H}$, e $P_{H}$ si dice pertanto applicazione
lineare idempotente ($P_{H}^{k}=P_{H}$ per ogni $k\in 
%TCIMACRO{\U{2115} }%
%BeginExpansion
\mathbb{N}
%EndExpansion
$); la sua matrice rappresentativa \`{e} una matrice idempotente, cio\`{e} $%
P_{H}\left( \mathbf{v}\right) =\mathbf{v}$ $\forall $ $\mathbf{v}\in H$.

Per ricavare $H$ avendo la matrice $P_{H}$ \`{e} sufficiente trovare
l'immagine dell'applicazione lineare descritta da $P_{H}$ (oppure
l'autospazio relativo all'autovalore $1$).

Sia $\lambda $ un autovalore di $P_{H}$ e $\mathbf{v\neq 0}$ un autovettore
relativo a $\lambda $. Allora $P_{H}\left( \mathbf{v}\right) =\lambda 
\mathbf{v}$ e $P_{H}\left( P_{H}\left( \mathbf{v}\right) \right)
=P_{H}\left( \lambda \mathbf{v}\right) =\lambda ^{2}\mathbf{v}$: poich\'{e} $%
P_{H}$ \`{e} idempotente, dev'essere $\lambda \mathbf{v}=\lambda ^{2}\mathbf{%
v\Longleftrightarrow v}\left( \lambda ^{2}-\lambda \right) =\mathbf{0}$.
Poich\'{e} $\mathbf{v\neq 0}$, $\lambda =1$ o $\lambda =0$: significa che
gli unici autovalori possibili per $P_{H}$ (o per una matrice che
rappresenta una proiezione ortogonale) sono $0$ e $1$ (ogni matrice di
proiezione \`{e} semidefinita positiva). Gli autospazi relativi sono $%
V_{0}=\ker P_{H}=H^{\perp }$ e $V_{1}=\left\{ \mathbf{v}\in V:P_{H}\left( 
\mathbf{v}\right) =\mathbf{v}\right\} =H$, che sono i vettori lasciati
immutati dalla trasformazione. Quindi $V_{0}\oplus V_{1}=H+H^{\perp }=V$.
Poich\'{e} gli autospazi di $P_{H}$ sono in somma diretta, $P_{H}$ \`{e}
diagonalizzabile: esiste una base di $V$ formata da autovettori di $P_{H}$
che permette di ottenere una matrice rappresentativa diagonale; tale base 
\`{e} formata dall'unione delle basi degli autospazi di $P_{H}$. Grazie
all'algoritmo di Gram-Schmidt si pu\`{o} costruire una base ortonormale $%
B_{H}$ di $H$, una base ortonormale $B_{H^{\perp }}$ di $H^{\perp }$, e $%
B=B_{H^{\perp }}\cup B_{H}$ \`{e} una base ortonormale di $V$ (per le
caratteristiche di $H$ e $H^{\perp }$, l'unione delle due basi ortonormali 
\`{e} ortonormale). Quindi si riesce a rappresentare $P_{H}$ con una matrice
diagonale rispetto a una base di autovettori, che \`{e} ortogonale e ha
quindi tutte le buone propriet\`{a} della base canonica.

\textbf{Def} Una matrice $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ si dice ortogonalmente diagonalizzabile se \`{e}
diagonalizzabile mediante una base ortogonale (o ortonormale: moltiplicare
per scalari non cambia la propriet\`{a} di essere autovettori) di
autovettori di $A$.

Quindi $A$ \`{e} ortogonalmente diagonalizzabile se esiste una base
ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ formata da autovettori di $A$. Se $A$ \`{e} ortogonalmente
diagonalizzabile, non tutte le matrici che rappresentano la stessa
applicazione lineare descritta da $A$ sono ortogonalmente diagonalizzabili
(matrici simili non necessariamente hanno gli stessi autovettori).

Come riconoscere le matrici con queste propriet\`{a}?

\textbf{Teorema fondamentale dell'ortogonalit\`{a}} 
\begin{gather*}
\text{Hp}\text{: }A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) \\
\text{Ts}\text{: (i) }\ker A=\left( \func{row}A\right) ^{\perp } \\
\text{(ii) }\func{row}A=\left( \ker A\right) ^{\perp } \\
\text{(iii) }\ker A^{T}=\left( \func{col}A\right) ^{\perp } \\
\text{(iv) }\func{col}A=\left( \ker A^{T}\right) ^{\perp }
\end{gather*}

\textbf{Dim} (i) Se $A=\left[ 
\begin{array}{c}
\mathbf{v}_{1} \\ 
... \\ 
\mathbf{v}_{m}%
\end{array}%
\right] $, $\func{row}A=Span\left( \mathbf{v}_{1}\mathbf{,...,v}_{m}\right)
\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$. $\mathbf{x}\in \ker A\Longleftrightarrow A\mathbf{x=0}%
\Longleftrightarrow \left[ 
\begin{array}{c}
\mathbf{v}_{1} \\ 
... \\ 
\mathbf{v}_{m}%
\end{array}%
\right] \mathbf{x=0}$ $\Longleftrightarrow \left[ 
\begin{array}{c}
\langle \mathbf{v}_{1},\mathbf{x\rangle } \\ 
... \\ 
\langle \mathbf{v}_{m}\mathbf{,x}\rangle%
\end{array}%
\right] \mathbf{=0}\Longleftrightarrow \langle \mathbf{v}_{i},\mathbf{%
x\rangle }=0$ per ogni $i=1,...,m$, e questo \`{e} equivalente a dire che $%
\mathbf{x}$ \`{e} ortogonale a ogni riga di $A$ (cio\`{e} a ogni vettore di
una base di $\func{row}A$), che \`{e} equivalente a dire che \`{e}
ortogonale a $\func{row}A$.

(ii) $\ker A=\left( \func{row}A\right) ^{\perp }\Longleftrightarrow \left(
\ker A\right) ^{\perp }=\func{row}A$: se due insiemi sono uguali, anche i
loro complementi ortogonali coincidono.

(iii) $\ker A^{T}=\left( \func{row}A^{T}\right) ^{\perp }$ per (i), ma $%
\func{row}A^{T}=\func{col}A$ perch\'{e} la trasposizione scambia righe e
colonne, quindi $\ker A^{T}=\left( \func{col}A\right) ^{\perp }$.

(iv) $\ker A^{T}=\left( \func{col}A\right) ^{\perp }\Longleftrightarrow
\left( \ker A^{T}\right) ^{\perp }=\func{col}A$, come in (ii). $\blacksquare 
$

\begin{enumerate}
\item Dato $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, $H=\left( \left[ 
\begin{array}{c}
x_{1} \\ 
... \\ 
x_{n}%
\end{array}%
\right] \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}:a_{1}x_{1}+...+a_{n}x_{n}=0\right) $ \`{e} un iperpiano di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$. Se $A=\left[ a_{1}|...|a_{n}\right] $, $\ker A=H$ ($\left( \func{row}%
A\right) ^{\perp }$) e $H^{\perp }=\left( \ker A\right) ^{\perp }=\func{row}%
A=Span\left( 
\begin{array}{c}
a_{1} \\ 
... \\ 
a_{n}%
\end{array}%
\right) $. Quindi questo teorema permette di trovare molto facilmente $%
H^{\perp }$, senza bisogno di trovare una base di $H$ n\'{e} risolvere un
sistema lineare.
\end{enumerate}

\subsection{Sistemi lineari sovradeterminati}

Considero un sistema lineare $A\mathbf{x=b}$ non risolubile: per il teorema
di Rouch\'{e}-Capelli, la non risolubilit\`{a} \`{e} equivalente a dire che $%
r\left( A|\mathbf{b}\right) =r\left( A\right) +1$, che \`{e} equivalente a
dire che $\mathbf{b}\notin \func{col}\left( A\right) $. Dunque non c'\`{e}
una soluzione: ha senso chiedersi qual \`{e} la migliore soluzione
approssimata, i. e. $\mathbf{\tilde{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}:A\mathbf{\tilde{x}}$ d\`{a} un vettore che \`{e} il pi\`{u} vicino
possibile a $\mathbf{b}$, cio\`{e} $\mathbf{\tilde{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}:\left\vert \left\vert A\mathbf{\tilde{x}-b}\right\vert \right\vert
\leq \left\vert \left\vert A\mathbf{x-b}\right\vert \right\vert $ per ogni $%
\mathbf{x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}$.

\textbf{Def} Dato un sistema lineare $A\mathbf{x=b}$, $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( m,n\right) $, si dice che $\mathbf{\tilde{x}}$ \`{e} una soluzione
ai minimi quadrati del sistema se $A\mathbf{\tilde{x}}$ \`{e} la proiezione
ortogonale di $\mathbf{b}$ su $\func{col}A$ ($A\mathbf{\tilde{x}}=P_{\func{%
col}\left( A\right) }\left( \mathbf{b}\right) $).

E' un'applicazione del teorema della proiezione ortogonale con $H=\func{col}%
A $, $\mathbf{w}=A\mathbf{x}$, $\mathbf{v=b}$. Quindi $A\mathbf{\tilde{x}}$ 
\`{e}, tra tutti i vettori di $\func{col}A$ (per cui quindi il sistema \`{e}
risolubile), il pi\`{u} vicino a $\mathbf{b}$.

Quindi ora si ha un metodo pi\`{u} generale di risoluzione dei sistemi
lineari: se $\mathbf{b}\in \func{col}A$ (cio\`{e} $P_{\func{col}\left(
A\right) }\left( \mathbf{b}\right) =\mathbf{b}$), si risolve normalmente $A%
\mathbf{x=b}$ (e ogni soluzione esatta \`{e} anche soluzione ai minimi
quadrati), se $\mathbf{b}\not\in \func{col}A$ si risolve $A\mathbf{x=b}%
^{\prime }$ con $P_{\func{col}\left( A\right) }\left( \mathbf{b}\right) =%
\mathbf{b}^{\prime }$, che ha sicuramente soluzione perch\'{e} $\mathbf{b}%
^{\prime }\in \func{col}A$.

\textbf{Teorema (soluzione ai minimi quadrati)} 
\begin{eqnarray*}
\text{Hp}\text{: } &&A\mathbf{x=b}\text{, }A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( m,n\right)  \\
\text{Ts}\text{: } &&\mathbf{\tilde{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}\text{ \`{e} una soluzione ai minimi quadrati di }A\mathbf{x=b}\text{ }%
\Longleftrightarrow \mathbf{\tilde{x}}\text{ \`{e} soluzione di }A^{T}A%
\mathbf{x=}A^{T}\mathbf{b}\text{ }
\end{eqnarray*}

$A^{T}A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $. Risolvendo $A^{T}A\mathbf{x=}A^{T}\mathbf{b}$, si pu%
\`{o} poi calcolare $P_{\func{col}\left( A\right) }\left( \mathbf{b}\right)
=A\mathbf{\tilde{x}}$, con $\mathbf{\tilde{x}}$ soluzione ai minimi quadrati.

\textbf{Dim} (i) Per ipotesi $\mathbf{\tilde{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}$ \`{e} una soluzione ai minimi quadrati di $A\mathbf{x=b}$, quindi $A%
\mathbf{\tilde{x}}=P_{\func{col}\left( A\right) }\left( \mathbf{b}\right) =%
\mathbf{b}^{\prime }$. $\mathbf{b-b}^{\prime }\mathbf{=b}-P_{\func{col}%
\left( A\right) }\left( \mathbf{b}\right) \in \left( \func{col}A\right)
^{\perp }$ (per il teorema sulla proiezione ortogonale) e $\left( \func{col}%
A\right) ^{\perp }=\ker \left( A^{T}\right) $. Quindi $\mathbf{b}-A\mathbf{%
\tilde{x}\in }\ker \left( A^{T}\right) $, cio\`{e} $A^{T}\left( \mathbf{b}-A%
\mathbf{\tilde{x}}\right) =\mathbf{0\Longleftrightarrow }A^{T}A\mathbf{%
\tilde{x}=}A^{T}\mathbf{b}$.

(ii) $\mathbf{\tilde{x}}$ \`{e} soluzione di $A^{T}A\mathbf{x=}A^{T}\mathbf{b%
}$, cio\`{e} $A^{T}\left( A\mathbf{\tilde{x}-b}\right) =\mathbf{0}$, quindi $%
\mathbf{b-A\mathbf{\tilde{x}}}\in \ker \left( A^{T}\right) =\left( \func{col}%
A\right) ^{\perp }$. Poich\'{e} $A\mathbf{\tilde{x}\in }\func{col}A$,
dev'essere $A\mathbf{\tilde{x}=}P_{\func{col}A}\left( \mathbf{b}\right) $. $%
\blacksquare $

In vari casi concreti, se e. g. si dispone di pi\`{u} osservazioni per il
valore di alcuni parametri $\alpha ,\beta ,\gamma $, non esistono $\alpha
,\beta ,\gamma $ che siano perfettamente in accordo con tutte le
osservazioni: il sistema che ha come vettore dei termini noti le
osservazioni per tali parametri (e. g. $\alpha _{i,i+1}^{E},\beta
_{i,i+1}^{E},\gamma _{i,i+1}^{E},\alpha _{i+1,i+2}^{E},\beta
_{i+1,i+2}^{E},\gamma _{i+1,i+2}^{E}$) e come incognite $\alpha ,\beta
,\gamma $ probabilmente non ha soluzione. Allora si risolve il sistema ai
minimi quadrati associato e si trova $\left( 
\begin{array}{c}
\alpha \\ 
\beta \\ 
\gamma%
\end{array}%
\right) $ che si avvicina il pi\`{u} possibile ai valori osservati.

\textbf{Def} Dato il sistema $A\mathbf{x=b}$, il sistema $A^{T}A\mathbf{x=}%
A^{T}\mathbf{b}$ si dice sistema delle equazioni normali associato a $A%
\mathbf{x=b}$.

Se $\mathbf{b}\in \func{col}A$, ogni soluzione esatta \`{e} anche soluzione
ai minimi quadrati, cio\`{e} $A\mathbf{x=b}$ e $A^{T}A\mathbf{x}=A^{T}%
\mathbf{b}$ hanno le stesse soluzioni.

Quante soluzioni ha $A^{T}A\mathbf{x=}A^{T}\mathbf{b}$?

\textbf{Proposizione}%
\begin{gather*}
\text{Hp}\text{: }A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( m,n\right) \\
\text{Ts}\text{: (i) }\ker A=\ker \left( A^{T}A\right) \\
\text{(ii) }r\left( A\right) =r\left( A^{T}A\right)
\end{gather*}

\textbf{Dim} (i) Mostro $\ker A\subseteq \ker \left( A^{T}A\right) $: $%
\mathbf{v}\in \ker A\Longleftrightarrow A\mathbf{v=0\Longrightarrow }%
A^{T}\left( A\mathbf{v}\right) \mathbf{=}A^{T}\mathbf{0}\Longleftrightarrow
A^{T}\left( A\mathbf{v}\right) =\mathbf{0\Longrightarrow v\in }\ker \left(
A^{T}A\right) $. Mostro che $\ker \left( A^{T}A\right) \subseteq \ker A$: $%
\mathbf{v}\in \ker \left( A^{T}A\right) \Longleftrightarrow A^{T}A\mathbf{v=0%
}$. Premoltiplico per $\mathbf{v}^{T}$: $\mathbf{v}^{T}A^{T}A\mathbf{%
v=\langle v},\mathbf{0\rangle =0}$. D'altra parte $\mathbf{v}^{T}A^{T}A%
\mathbf{v=}\left( \mathbf{v}^{T}A^{T}\right) A\mathbf{v=}\left( A\mathbf{v}%
\right) ^{T}A\mathbf{v=\langle }A\mathbf{v},A\mathbf{v\rangle }$. Quindi $%
\mathbf{\langle }A\mathbf{v},A\mathbf{v\rangle =0\Longleftrightarrow }%
\left\vert \left\vert A\mathbf{v}\right\vert \right\vert ^{2}=\mathbf{%
0\Longleftrightarrow }A\mathbf{v=0\Longleftrightarrow v\in }\ker A$. Dunque $%
\ker A=\ker \left( A^{T}A\right) $.

(ii) Poich\'{e} $\ker A=\ker \left( A^{T}A\right) $, $\dim \left( \ker
A\right) =\dim \left( \ker \left( A^{T}A\right) \right) $. Per il teorema di
nullit\`{a} pi\`{u} rango $r\left( A\right) =n-\dim \left( \ker A\right) $ e 
$r\left( A^{T}A\right) =n-\dim \left( \ker A^{T}A\right) $, quindi $r\left(
A\right) =r\left( A^{T}A\right) $. $\blacksquare $

\textbf{Corollario}%
\begin{gather*}
\text{Hp}\text{: }A\mathbf{x=b}\text{, }A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( m,n\right) \\
\text{Ts}\text{:}\text{ la soluzione ai minimi quadrati di }A\mathbf{x=b}%
\text{ \`{e} unica }\Longleftrightarrow r\left( A\right) =n\text{ }
\end{gather*}

Infatti la soluzione ai minimi quadrati di $A\mathbf{x=b}$ \`{e} unica se e
solo se $A^{T}A\mathbf{x=}A^{T}\mathbf{b}$ ha un'unica soluzione, cio\`{e}
se e solo se $r\left( A^{T}A\right) =n=r\left( A\right) $, per il teorema di
Cramer.

\subsection{Matrice rappresentativa di una proiezione ortogonale}

Data $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( m,n\right) $ e il sistema $A\mathbf{x=b}$, il sistema delle
equazioni normali a esso associato \`{e} $A^{T}A\mathbf{x=}A^{T}\mathbf{b}$;
se $\mathbf{\tilde{x}}$ \`{e} una soluzione ai minimi quadrati di $A\mathbf{%
x=b}$, $A\mathbf{\tilde{x}=}P_{\func{col}\left( A\right) }\left( \mathbf{b}%
\right) $. Affinch\'{e} la soluzione di $A^{T}A\mathbf{x=}A^{T}\mathbf{b}$
sia unica occorre $r\left( A\right) =n$. Con questa ipotesi si cerca di
trovare la matrice rappresentativa di $P_{\func{col}\left( A\right) }\left( 
\mathbf{x}\right) $.

Poich\'{e} $r\left( A^{T}A\right) =n$, $A^{T}A$ \`{e} invertibile, quindi $%
A^{T}A\mathbf{x=}A^{T}\mathbf{b\Longleftrightarrow }\left( A^{T}A\right)
^{-1}\left( A^{T}A\right) \mathbf{x=}\left( A^{T}A\right) ^{-1}A^{T}\mathbf{%
b\Longleftrightarrow x=}\left( A^{T}A\right) ^{-1}A^{T}\mathbf{b}$. Chiamo
questa soluzione $\mathbf{\tilde{x}}$: dato che \`{e} una soluzione ai
minimi quadrati, vale $A\mathbf{\tilde{x}=}P_{\func{col}\left( A\right)
}\left( \mathbf{b}\right) $. Quindi $P_{\func{col}\left( A\right) }\left( 
\mathbf{b}\right) =A\left( A^{T}A\right) ^{-1}A^{T}\mathbf{b}$, e la matrice
rappresentativa della proiezione ortogonale su $\func{col}\left( A\right) $,
rispetto alla base canonica, \`{e} $A\left( A^{T}A\right) ^{-1}A^{T}$. Ha
una forma poco bella.

Tale formula si pu\`{o} usare per la matrice rappresentativa della
proiezione ortogonale su qualsiasi sottospazio $H$, a patto di conoscere una
sua base $B$: in tal caso la matrice $A$ avr\`{a} come colonne i vettori di $%
B$, cos\`{\i} che $\func{col}\left( A\right) =H$ e $r\left( A\right) =n$.

\begin{enumerate}
\item $B=\left[ 
\begin{array}{ccc}
3 & 0 & 1 \\ 
1 & 1 & 1 \\ 
1 & -2 & -1%
\end{array}%
\right] $: qual \`{e} la matrice rappresentativa della proiezione ortogonale
su $\func{col}B$? Prima di applicare la formula devo assicurarmi che $%
r\left( B\right) =3$. Riducendo a scala trovo che solo le prime due colonne
sono linearmente indipendenti, quindi posso applicare la formula a $A=\left[ 
\begin{array}{cc}
3 & 0 \\ 
1 & 1 \\ 
1 & -2%
\end{array}%
\right] $ (il sottospazio su cui si calcola la proiezione ortogonale \`{e}
lo stesso) e calcolo $A\left( A^{T}A\right) ^{-1}A^{T}$.
\end{enumerate}

Poich\'{e} si \`{e} visto che l'applicazione lineare proiezione ortogonale 
\`{e} idempotente, dev'esserlo anche la sua matrice rappresentativa: infatti 
$\left( A\left( A^{T}A\right) ^{-1}A^{T}\right) \left( A\left( A^{T}A\right)
^{-1}A^{T}\right) =A\left( A^{T}A\right) ^{-1}\left( A^{T}A\right) \left(
A^{T}A\right) ^{-1}A^{T}=A\left( A^{T}A\right) ^{-1}A^{T}$ poich\'{e} $%
\left( A^{T}A\right) \left( A^{T}A\right) ^{-1}=Id_{n}$.

\textbf{Proposizione}%
\begin{gather*}
\text{Hp}\text{: }\left( V,\langle \_,\_\rangle \right) \text{ \`{e} uno
spazio euclideo di dimensione finita, }H\subseteq V\text{ \`{e} un } \\
\text{sottospazio con }\dim H=h\leq n\text{, }B=\left\{ \mathbf{b}_{1}%
\mathbf{,...,b}_{h}\right\} \text{ \`{e} una base ortonormale di }H \\
\text{Ts}\text{:}\text{ la matrice rappresentativa della proiezione
ortogonale su }H\text{ \`{e} }AA^{T}\text{, con }A=\left[ \mathbf{b}_{1}|...|%
\mathbf{b}_{h}\right]
\end{gather*}

La matrice di proiezione \`{e} sempre quadrata e di ordine uguale alla
dimensione di $V$. In base alla forma vista prima $A\left( A^{T}A\right)
^{-1}A^{T}$, se $B$ \`{e} ortonormale, $\left( A^{T}A\right) ^{-1}=Id_{n}$.

\textbf{Dim} Per costruzione, essendo $B$ una base di $H$, $r\left( A\right)
=h$, quindi la soluzione di $A^{T}A\mathbf{x=}A^{T}\mathbf{b}$ (soluzione ai
minimi quadrati) \`{e} unica. La matrice rappresentativa della proiezione
ortogonale su $H$ \`{e} $A\left( A^{T}A\right) ^{-1}A^{T}$, per quanto
visto. Mostro che $\left( A^{T}A\right) ^{-1}=Id_{n}$: calcolo $A^{T}A=\left[
\begin{array}{c}
\mathbf{b}_{1} \\ 
... \\ 
\mathbf{b}_{h}%
\end{array}%
\right] \left[ \mathbf{b}_{1}|...|\mathbf{b}_{h}\right] =\left[ 
\begin{array}{ccc}
\mathbf{b}_{1}^{T}\mathbf{b}_{1} & \mathbf{b}_{1}^{T}\mathbf{b}_{2} & ... \\ 
\mathbf{b}_{2}^{T}\mathbf{b}_{1} & \mathbf{b}_{2}^{T}\mathbf{b}_{2} & ... \\ 
... & ... & ...%
\end{array}%
\right] =$ $\left[ 
\begin{array}{ccc}
\langle \mathbf{b}_{1},\mathbf{b}_{1}\rangle & \langle \mathbf{b}_{1},%
\mathbf{b}_{2}\rangle & ... \\ 
\langle \mathbf{b}_{2},\mathbf{b}_{1}\rangle & \langle \mathbf{b}_{2},%
\mathbf{b}_{2}\rangle & ... \\ 
... & ... & ...%
\end{array}%
\right] =\left[ 
\begin{array}{ccc}
\left\vert \left\vert \mathbf{b}_{1}\right\vert \right\vert ^{2} & 0 & ...
\\ 
0 & \left\vert \left\vert \mathbf{b}_{2}\right\vert \right\vert ^{2} & ...
\\ 
... & ... & ...%
\end{array}%
\right] =\left[ 
\begin{array}{ccc}
1 & 0 & ... \\ 
0 & 1 & ... \\ 
... & ... & ...%
\end{array}%
\right] $ perch\'{e} la base \`{e} ortogonale e normale. Quindi $A\left(
A^{T}A\right) ^{-1}A^{T}=AA^{T}$. $\blacksquare $

Quindi per calcolare la matrice rappresentativa di $P_{H}$ ci sono pi\`{u}
possibilit\`{a}: se $\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{h}\right\} $ 
\`{e} una base di $H$,

\begin{description}
\item[-] pongo $A=\left[ \mathbf{v}_{1}|...|\mathbf{v}_{h}\right] $ e
calcolo $A\left( A^{T}A\right) ^{-1}A^{T}$

\item[-] data la base $\left\{ \mathbf{v}_{1}\mathbf{,...,v}_{h}\right\} $,
ne ricavo una base ortonormale $\left\{ \mathbf{b}_{1}\mathbf{,...,b}%
_{h}\right\} $ con l'algoritmo di Gram-Schmidt e calcolo $BB^{T}$, con $B=%
\left[ \mathbf{b}_{1}|\mathbf{...|b}_{h}\right] $

\item[-] usando il teorema di rappresentazione, calcolo esplicitamente $%
P_{H}\left( \mathbf{e}_{1}\right) ,...P_{H}\left( \mathbf{e}_{h}\right) $
utilizzando i coefficienti di Fourier, dopo aver ricavato una base
ortonormale $\left\{ \mathbf{b}_{1}\mathbf{,...,b}_{h}\right\} $ con
l'algoritmo di Gram-Schmidt.
\end{description}

\begin{enumerate}
\item Sia $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$ dotato di prodotto scalare standard, $H=Span\left( \left[ 
\begin{array}{c}
1 \\ 
2 \\ 
0%
\end{array}%
\right] \right) $. Qual \`{e} la matrice rappresentativa di $P_{H}:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{3}$? Uso la seconda tecnica: devo solo normalizzare l'unico generatore
presente, quindi ottengo come base ortonormale di $H$ $\left\{ \frac{1}{%
\sqrt{5}}\left( 
\begin{array}{c}
1 \\ 
2 \\ 
0%
\end{array}%
\right) \right\} $ e $B=\frac{1}{\sqrt{5}}\left[ 
\begin{array}{c}
1 \\ 
2 \\ 
0%
\end{array}%
\right] $. Calcolo $AA^{T}=\frac{1}{5}\left[ 
\begin{array}{c}
1 \\ 
2 \\ 
0%
\end{array}%
\right] \left[ 1|2|0\right] =\frac{1}{5}\left[ 
\begin{array}{ccc}
1 & 2 & 0 \\ 
2 & 4 & 0 \\ 
0 & 0 & 0%
\end{array}%
\right] $. La matrice ottenuta \`{e} simmetrica, cio\`{e} \`{e} uguale alla
sua trasposta.

\item Se $P$ \`{e} la matrice rappresentativa della proiezione ortogonale su
un sottospazio $H$, $Q=Id-P$ \`{e} la matrice rappresentativa della
proiezione su $H^{\perp }$. Infatti, se $\mathbf{v}_{H}$ \`{e} la proiezione
di $\mathbf{v}$ su $H$, $\mathbf{v}_{H^{\perp }}=\mathbf{v-v}_{H}$ \`{e} la
proiezione di $\mathbf{v}$ su $H^{\perp }$. Quindi la proiezione di $\mathbf{%
v}$ su $H^{\perp }$ \`{e} $Q\mathbf{v}=\mathbf{v}_{H^{\perp }}=\mathbf{v-v}%
_{H}=Id\mathbf{v-}P\mathbf{v}=\left( Id-P\right) \mathbf{v}$.
\end{enumerate}

In generale, per qualsiasi $A$, $P=AA^{T}$ \`{e} tale che $P^{T}=\left(
AA^{T}\right) ^{T}=AA^{T}=P$, quindi $P$ \`{e} simmetrica. Dunque ogni
matrice rappresentativa di una proiezione ortogonale \`{e} simmetrica (se ne
ricava che una matrice simmetrica non necessariamente \`{e} invertibile), e,
per quanto visto, diagonalizzabile: \`{e} simile a una matrice diagonale $D$
($P=SDS^{-1}\Longleftrightarrow D=S^{-1}PS$) e tale che $P^{T}=P$. Quindi $%
\left( SDS^{-1}\right) ^{T}=\left( S^{-1}\right) ^{T}D^{T}S^{T}=\left(
S^{-1}\right) ^{T}DS^{T}$ dev'essere uguale a $SDS^{-1}$. Parrebbe quindi
che $S^{-1}=S^{T}$.

\begin{enumerate}
\item Che $A$ sia simmetrica \`{e} condizione necessaria, ma non
sufficiente, affinch\'{e} una matrice rappresenti una proiezione ortogonale.
Infatti la matrice $\left[ 
\begin{array}{ccc}
0 & 0 & 1 \\ 
0 & 0 & 0 \\ 
1 & 0 & 0%
\end{array}%
\right] $ \`{e} simmetrica, ma ha come autovalori $0,1,-1$, quindi non
rappresenta una proiezione ortogonale poich\'{e} la matrice rappresentativa
di una proiezione ortogonale pu\`{o} avere solo $0,1$ come autovalori.

\end{enumerate}

\subsubsection{Matrici ortogonali}

\textbf{Def} Una matrice $Q\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ si dice ortogonale se $Q^{T}Q=Id_{n}$.

Significa che $Q$ \`{e} invertibile a sinistra, con inversa sinistra uguale
alla sua trasposta.

\textbf{Proposizione}%
\begin{gather*}
\text{Hp}\text{: }Q\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) \\
\text{Ts}\text{: }Q\text{ \`{e} ortogonale }\Longleftrightarrow \text{ le
sue colonne descrivono una base ortonormale di }%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}
\end{gather*}

\textbf{Dim} $Q=\left[ \mathbf{q}_{1}|...|\mathbf{q}_{n}\right] $: $Q^{T}Q=%
\left[ 
\begin{array}{c}
\mathbf{q}_{1} \\ 
... \\ 
\mathbf{q}_{n}%
\end{array}%
\right] \left[ \mathbf{q}_{1}|...|\mathbf{q}_{n}\right] =\left[ 
\begin{array}{ccc}
\mathbf{q}_{1}^{T}\mathbf{q}_{1} & \mathbf{q}_{1}^{T}\mathbf{q}_{2} & ... \\ 
\mathbf{q}_{2}^{T}\mathbf{q}_{1} & \mathbf{q}_{2}^{T}\mathbf{q}_{2} & ... \\ 
... & ... & ...%
\end{array}%
\right] $ $=\left[ 
\begin{array}{ccc}
\left\vert \left\vert \mathbf{q}_{1}\right\vert \right\vert ^{2} & 0 & ...
\\ 
0 & \left\vert \left\vert \mathbf{q}_{2}\right\vert \right\vert ^{2} & ...
\\ 
... & ... & ...%
\end{array}%
\right] =\left[ 
\begin{array}{ccc}
1 & 0 & ... \\ 
0 & 1 & ... \\ 
... & ... & ...%
\end{array}%
\right] =Id$ se e solo se $\left\{ \mathbf{q}_{1},...,\mathbf{q}_{n}\right\} 
$ \`{e} una base ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ secondo il prodotto scalare standard. $\blacksquare $

\begin{enumerate}
\item La matrice che rappresenta la riflessione rispetto a una retta che
forma un angolo $\theta $ con l'asse $x$ (simmetria assiale) \`{e} $%
S_{\theta }=\left[ 
\begin{array}{cc}
\cos 2\theta & \sin 2\theta \\ 
\sin 2\theta & -\cos 2\theta%
\end{array}%
\right] $: le due colonne costituiscono una base ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$.

La matrice che rappresenta la rotazione di un angolo $\theta $ in senso
orario \`{e} $R_{\theta }=\left[ 
\begin{array}{cc}
\cos \theta & -\sin \theta \\ 
\sin \theta & \cos \theta%
\end{array}%
\right] $: le due colonne costituiscono una base ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$.
\end{enumerate}

Se $Q$ \`{e} ortogonale, anche $Q^{T}$ \`{e} ortogonale, perch\'{e} \`{e}
anche inversa destra di $Q$, per cui $QQ^{T}=\left( Q^{T}\right)
^{T}Q^{T}=Id $. Quindi, se le colonne di $Q$ formano una base ortonormale di 
$%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, anche le sue righe formano una base ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$.

\textbf{Propriet\`{a}}

L'insieme delle matrici ortogonali \`{e} un gruppo rispetto al prodotto
matriciale, con elemento neutro $Id_{n}$ ed elemento inverso la matrice
inversa. Infatti:

\begin{description}
\item[-] $Id_{n}$ \`{e} una matrice ortogonale: $%
Id_{n}^{T}Id_{n}=Id_{n}Id_{n}=Id_{n}$.

\item[-] Se $Q_{1},Q_{2}$ sono ortogonali, $Q_{1}Q_{2}$ \`{e} ortogonale (cio%
\`{e} la composizione di isometrie \`{e} un'isometria): $\left(
Q_{1}Q_{2}\right) ^{T}\left( Q_{1}Q_{2}\right)
=Q_{2}^{T}Q_{1}^{T}Q_{1}Q_{2}=Q_{2}^{T}Id_{n}Q_{2}$ (perch\'{e} $Q_{1}$ \`{e}
ortogonale) $=Q_{2}^{T}Q_{2}=Id_{n}$ (perch\'{e} $Q_{2}$ \`{e} ortogonale).

\item[-] $\det Q=\pm 1$ per ogni $Q$ ortogonale. Infatti $\det \left(
Q^{T}Q\right) =\det Q^{T}\det Q=\det^{2}Q$ (una matrice e la sua trasposta
hanno lo stesso determinante). Dev'essere $\det \left( Q^{T}Q\right) =\det
Id=1$, quindi $\det^{2}Q=1\Longleftrightarrow \det Q=1\vee \det Q=-1$.

\item[-] Se $Q$ \`{e} ortogonale, $Q$ \`{e} invertibile, infatti $\det Q\neq
0$, quindi $r\left( Q\right) =n$. Perci\`{o}, essendo $Q$ invertibile e
avendo come inversa sinistra $Q^{T}$, per il teorema dell'inversa si pu\`{o}
concludere che la matrice inversa di $Q$ \`{e} $Q^{T}$.
\end{description}

\begin{enumerate}
\item $\det S_{\theta }=-1$, $\det R_{\theta }=1$. Il segno del determinante
ha un significato geometrico. Infatti $\mathbf{e}_{1}\mathbf{\times e}%
_{2}=R\left( \mathbf{e}_{1}\right) \times R\left( \mathbf{e}_{2}\right) $,
mentre $\mathbf{e}_{1}\mathbf{\times e}_{2}=-S\left( \mathbf{e}_{1}\right)
\times S\left( \mathbf{e}_{2}\right) $: $\det R_{\theta }=1$ significa che
l'ordine dei due vettori \`{e} preservato dalla trasformazione (come nel
caso di rotazioni), $\det S_{\theta }=-1$ significa che \`{e} invertito
(come nel caso di simmetrie/riflessioni). Ne segue che la composizione di
due riflessioni \`{e} una rotazione, mentre la composizione di riflessione e
rotazione \`{e} una riflessione.
\end{enumerate}

\begin{description}
\item[-] Se $Q$ \`{e} ortogonale, la trasformazione indotta da $Q$ preserva
il prodotto scalare in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$: dati $\mathbf{v,w}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, $\langle \mathbf{v,w}\rangle =\langle Q\mathbf{v},Q\mathbf{w}\rangle $%
. Infatti $\langle Q\mathbf{v},Q\mathbf{w}\rangle =\left( Q\mathbf{v}\right)
^{T}Q\mathbf{w}=\mathbf{v}^{T}Q^{T}Q\mathbf{w=v}^{T}Id_{n}\mathbf{w=v}^{T}%
\mathbf{w=}\langle \mathbf{v,w}\rangle $. Anche la norma \`{e} preservata: $%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}=\langle \mathbf{%
v,v}\rangle =\langle Q\mathbf{v},Q\mathbf{v}\rangle =\left\vert \left\vert Q%
\mathbf{v}\right\vert \right\vert ^{2}$. Quindi, secondo la definizione di
angolo data, $\mathbf{\hat{v}w}=Q\left( \mathbf{v}\right) \hat{Q}\left( 
\mathbf{w}\right) $, cio\`{e} $Q$ preserva gli angoli: si dice che $Q$
rappresenta un'isometria (lineare) di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$. Infatti le matrici ortogonali codificano tutte le trasformazioni
rigide (senza dilatazioni o contrazioni), come rotazioni o simmetrie.

\item[-] Se $Q$ \`{e} ortogonale e $\lambda $ \`{e} un autovalore di $Q$ con
relativo autovettore $\mathbf{v}$, $Q\mathbf{v}=\lambda \mathbf{v}$ e $%
\left\vert \left\vert Q\mathbf{v}\right\vert \right\vert =\left\vert
\left\vert \lambda \mathbf{v}\right\vert \right\vert =\left\vert \lambda
\right\vert \left\vert \left\vert \mathbf{v}\right\vert \right\vert $. Per
quanto visto $\left\vert \left\vert \mathbf{v}\right\vert \right\vert
=\left\vert \left\vert Q\mathbf{v}\right\vert \right\vert $, quindi
dev'essere $\left\vert \lambda \right\vert \left\vert \left\vert \mathbf{v}%
\right\vert \right\vert =\left\vert \left\vert \mathbf{v}\right\vert
\right\vert \Longleftrightarrow \left\vert \lambda \right\vert =1$ ($\mathbf{%
v\neq 0}$). Quindi $\lambda =-1$ o $\lambda =1$ sono i possibili autovalori
di una matrice ortogonale.
\end{description}

Le matrici ortogonali che rappresentano simmetrie rispetto a un sottospazio
sono simmetriche e quindi ortogonalmente diagonalizzabili.

In generale le matrici di proiezione su $H$ non sono ortogonali, perch\'{e}
possono avere nucleo di dimensione non nulla, quindi rango non massimo e
determinante nullo.

Le matrici ortogonali possono anche essere viste come matrici di cambiamento
di base tra due basi ortonormali.

\subsection{Teorema spettrale}

Ora che si sono studiate le propriet\`{a} delle matrici ortogonali,
riprendiamo la definizione di matrice ortogonalmente diagonalizzabile: $A$
si dice ortogonalmente diagonalizzabile se \`{e} diagonalizzabile mediante
una base ortonormale di autovettori di $A$.

Sia $B=\left\{ \mathbf{b}_{1}\mathbf{,...,b}_{n}\right\} $ una base
ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ formata da autovettori di $A$: se $A$ \`{e} diagonalizzabile mediante
una base ortonormale di autovettori, data $P=\left[ \mathbf{b}_{1}\mathbf{%
|...|b}_{n}\right] $, $D=P^{-1}AP$. Essendo $B$ ortonormale, $P$ \`{e}
ortogonale, quindi $P^{-1}=P^{T}$. Perci\`{o} si pu\`{o} riformulare la
definizione tenendo conto delle propriet\`{a} delle matrici ortogonali.

\textbf{Def} Una matrice $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ si dice ortogonalmente diagonalizzabile se esiste una
matrice ortogonale $P\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ tale che $P^{T}AP$ \`{e} diagonale.

\textbf{Lemma 1}%
\begin{eqnarray*}
\text{Hp}\text{: } &&A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) \text{ \`{e} ortogonalmente diagonalizzabile} \\
\text{Ts}\text{: } &&A\text{ \`{e} simmetrica}
\end{eqnarray*}

\textbf{Dim} Per definizione $P^{T}AP=D$ \`{e} diagonale, con $P$ matrice
ortogonale. Mostro che $A=A^{T}$: $A=\left( P^{T}\right) ^{-1}DP^{-1}=\left(
P^{-1}\right) ^{-1}DP^{T}=PDP^{T}$ perch\'{e} $P$ ha come inversa $P^{T}$.
Allora $A^{T}=\left( PDP^{T}\right) ^{T}=\left( P^{T}\right)
^{T}D^{T}P^{T}=PDP^{T}=A$, dunque $A$ \`{e} simmetrica. $\blacksquare $

Dunque la simmetria \`{e} condizione necessaria per l'ortogonale
diagonalizzabilit\`{a}. Si vedr\`{a} che \`{e} anche sufficiente.

Se \`{e} noto che $A$ \`{e} ortogonalmente diagonalizzabile, allora lo \`{e}
anche semplicemente, quindi si possono usare le tecniche standard per
scrivere $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ come $V_{\lambda _{1}}\oplus ...\oplus V_{\lambda _{s}}$ e ottenere
una base di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ formata da autovettori di $A$: per ogni $\lambda _{i}$, $\left\{ 
\mathbf{v}_{i1}\mathbf{,...,v}_{ig\left( \lambda _{i}\right) }\right\} $ 
\`{e} una base di $V_{\lambda _{i}}$. Per costruire una base ortonormale, si
usa l'algoritmo di Gram-Schmidt all'interno di ogni autospazio e si ricava
una base ortonormale di ogni autospazio: $\left\{ \mathbf{b}_{i1}\mathbf{%
,...,b}_{ig\left( \lambda _{i}\right) }\right\} $ \`{e} una base ortonormale
di $V_{\lambda _{i}}$. Non si pu\`{o} usare l'algoritmo sull'unione di tutte
le basi degli autospazi, perch\'{e} combinando linearmente autovettori di
autospazi diversi non necessariamente si ottengono autovettori.

\textbf{Lemma 2}%
\begin{gather*}
\text{Hp}\text{:}\text{ }A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) \text{ \`{e} simmetrica, }\lambda ,\mu \text{ sono
autovalori distinti di }A\text{,} \\
\mathbf{v}\in V_{\lambda },\mathbf{w}\in V_{\mu }\text{ (autospazi relativi
agli autovalori }\lambda ,\mu \text{)} \\
\text{Ts}\text{: }\left\langle \mathbf{v,w}\right\rangle =0\text{ (}%
V_{\lambda }\subseteq \left( V_{\mu }\right) ^{\perp }\text{)}
\end{gather*}

Quindi autovettori di una matrice simmetrica relativi ad autospazi distinti
sono ortogonali: dati $\mathbf{v}\in V_{\lambda },\mathbf{w}\in V_{\mu }$,
questi sono ortogonali; in questo senso si pu\`{o} dire che due autospazi
distinti di una matrice simmetrica sono ortogonali. Perci\`{o}, avendo una
base ortonormale di ogni autospazio, unendo tutte le basi ortonormali si
ottiene una base formata da autovettori ancora ortonormale: $B=\left\{ 
\mathbf{b}_{11}\mathbf{,...,b}_{1g\left( \lambda _{s}\right) }\right\} \cup
...\cup \left\{ \mathbf{b}_{s1}\mathbf{,...,b}_{sg\left( \lambda _{s}\right)
}\right\} $.

\textbf{Dim} Calcolo $\left\langle A\mathbf{v,w}\right\rangle =\left( A%
\mathbf{v}\right) ^{T}\mathbf{w}=\left( \mathbf{v}^{T}A^{T}\right) \mathbf{w}%
=\mathbf{v}^{T}\left( A\mathbf{w}\right) =\left\langle \mathbf{v},A\mathbf{w}%
\right\rangle $, usando il fatto che $A$ \`{e} simmetrica. Calcolo $%
\left\langle A\mathbf{v,w}\right\rangle =\left\langle \lambda \mathbf{v,w}%
\right\rangle =\lambda \left\langle \mathbf{v,w}\right\rangle $ e $%
\left\langle \mathbf{v,}A\mathbf{w}\right\rangle =\left\langle \mathbf{v,}%
\mu \mathbf{w}\right\rangle =\mu \left\langle \mathbf{v,w}\right\rangle $.
Poich\'{e} $\left\langle A\mathbf{v,w}\right\rangle =\left\langle \mathbf{v}%
,A\mathbf{w}\right\rangle $, $\lambda \left\langle \mathbf{v,w}\right\rangle
=\mu \left\langle \mathbf{v,w}\right\rangle \Longleftrightarrow \lambda
\left\langle \mathbf{v,w}\right\rangle -\mu \left\langle \mathbf{v,w}%
\right\rangle =0\Longleftrightarrow \left( \lambda -\mu \right) \left\langle 
\mathbf{v,w}\right\rangle =0$: per la legge di annullamento del prodotto,
essendo $\lambda \neq \mu $, dev'essere $\left\langle \mathbf{v,w}%
\right\rangle =0$. $\blacksquare $

\begin{enumerate}
\item Nel caso in cui $A$ simmetrica abbia solo due autovalori e quindi solo
due autospazi, si ha $V_{\lambda }=\left( V_{\mu }\right) ^{\perp }$.
Infatti $V_{\lambda }\subseteq \left( V_{\mu }\right) ^{\perp }$, $\dim
V_{\lambda }=n-\dim V_{\mu }$ (perch\'{e} $A$ \`{e} diagonalizzabile) e $%
\dim \left( V_{\mu }\right) ^{\perp }=n-\dim V_{\mu }$, quindi $V_{\lambda
}=\left( V_{\mu }\right) ^{\perp }$. Si noti che quanto detto vale anche nel
caso di $A$ non simmetrica, tranne $V_{\lambda }\subseteq \left( V_{\mu
}\right) ^{\perp }$.
\end{enumerate}

\textbf{Def} Dato uno spazio euclideo $V,\left\langle \_,\_\right\rangle $
di dimensione finita e un'applicazione lineare $L:V\rightarrow V$, si dice
operatore aggiunto di $L$ un'applicazione lineare $L^{\ast }:V\rightarrow V$
tale che $\left\langle L\left( \mathbf{v}\right) \mathbf{,w}\right\rangle
=\left\langle \mathbf{v},L^{\ast }\left( \mathbf{w}\right) \right\rangle $ $%
\forall $ $\mathbf{v,w}\in V$.

La nozione di operatore aggiunto generalizza da matrici ad applicazioni
lineari qualsiasi l'operazione di trasposizione: se $L\left( \mathbf{v}%
\right) =A\mathbf{v}$, $L^{\ast }\left( \mathbf{v}\right) =A^{t}\mathbf{v}$.

\textbf{Def} Dato uno spazio euclideo $V,\left\langle \_,\_\right\rangle $
di dimensione finita, un'applicazione lineare $L:V\rightarrow V$ si dice
autoaggiunta se $\left\langle L\left( \mathbf{v}\right) \mathbf{,w}%
\right\rangle =\left\langle \mathbf{v},L\left( \mathbf{w}\right)
\right\rangle $ $\forall $ $\mathbf{v,w}\in V$.

Quindi $L$ \`{e} autoaggiunta se e solo se $L=L^{\ast }$.

Se $V=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ e $\left\langle \mathbf{v,w}\right\rangle $ \`{e} il prodotto scalare
canonico $\mathbf{v\cdot w}$, $L$ \`{e} autoaggiunta se e solo se $%
L=\tciLaplace _{A}$ con $A$ simmetrica. (se $A$ \`{e} simmetrica tutte le
matrici che rappresentano la stessa applicazione lineare sono simmetriche?
No a meno che si usi come cambiamento di base una matrice ortogonale)

\textbf{Lemma 3}%
\begin{eqnarray*}
\text{Hp} &\text{: }&A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) \text{ \`{e} simmetrica} \\
\text{Ts} &\text{: }&A\text{ ha almeno un autovalore reale}
\end{eqnarray*}

Vedendo $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ come $A\in M_{%
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
}\left( n,n\right) $, per il teorema fondamentale dell'algebra $p_{A}\left(
x\right) =\det \left( A-xId\right) \in 
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
\left[ x\right] $ \`{e} tale che $p_{A}\left( x\right) =\left( x-\alpha
_{1}\right) ...\left( x-\alpha _{n}\right) $, $\alpha _{i}\in 
%TCIMACRO{\U{2102} }%
%BeginExpansion
\mathbb{C}
%EndExpansion
$. Le radici di $p_{A}\left( x\right) $ potrebbero essere tutte complesse:
il lemma aggiunge che se $A$ \`{e} simmetrica, non sono tutte complesse.

\textbf{Teorema spettrale per operatori autoaggiunti}%
\begin{eqnarray*}
\text{Hp}\text{: } &&A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) \\
\text{Ts}\text{: } &&A\text{ \`{e} ortogonalmente diagonalizzabile}%
\Longleftrightarrow A\text{ \`{e} simmetrica}
\end{eqnarray*}

Quindi, essendo $A$ diagonalizzabile su $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, ha $n$ autovalori in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ contati con molteplicit\`{a} algebrica, i. e. i suoi autovalori sono tutti
reali.

\textbf{Dim} L'implicazione da sinistra a destra \`{e} dimostrata nel lemma
1.

Si dimostra l'implicazione da destra a sinistra per induzione su $n$. Se $%
n=1 $, $A=\left[ a_{11}\right] $ (necessariamente simmetrica) \`{e}
ortogonalmente diagonalizzabile perch\'{e}, essendo diagonale, \`{e}
diagonalizzabile e ogni base di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{1}=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ \`{e} ortogonale, essendo fatta di un solo elemento.

Suppongo, come ipotesi induttiva, che il teorema sia vero per tutti i $k\leq
n-1$. Voglio trovare un'opportuna matrice $k\times k$ cui applicare
l'ipotesi induttiva. Considero $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ simmetrica per ipotesi: per il lemma 3, ha almeno un
autovalore $\lambda \in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$; $V_{\lambda }=\left\{ \mathbf{v}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}:A\mathbf{v}=\lambda \mathbf{v}\right\} $. Pongo $H=V_{\lambda }^{\perp
} $ (\`{e} noto che $H\oplus V_{\lambda }=%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$); $g_{\lambda }\geq 1$, quindi $\dim H=n-\dim V_{\lambda }\leq n-1$.
Osservo che, $\forall $ $\mathbf{w}\in H$, $A\mathbf{w}\in H$ ($A\left(
H\right) \subseteq H$), cio\`{e} $\left\langle A\mathbf{w,v}\right\rangle =0$
per ogni $\mathbf{v}\in V_{\lambda }$: infatti $\mathbf{w}\in
H\Longleftrightarrow \left\langle \mathbf{v,w}\right\rangle =0$ per ogni $%
\mathbf{v}\in V_{\lambda }$; $\left\langle A\mathbf{w,v}\right\rangle
=\left\langle A\mathbf{v,w}\right\rangle $ perch\'{e} $\tciLaplace _{A}$ 
\`{e} autoaggiunta; $\left\langle A\mathbf{v,w}\right\rangle =\lambda
\left\langle \mathbf{v,w}\right\rangle =0$ per ipotesi, quindi $\left\langle
A\mathbf{w,v}\right\rangle =\left\langle A\mathbf{v,w}\right\rangle =0$ e $H$
\`{e} un sottospazio invariante rispetto all'azione di $\tciLaplace _{A}$.
Considero $\tciLaplace _{A}|_{H}:H\rightarrow H$ (perch\'{e} $H$ \`{e}
invariante per $\tciLaplace _{A}$: mi serve per avere una matrice
rappresentativa quadrata): questa nuova applicazione lineare $\tciLaplace
_{A^{\prime }}:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{k}\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{k}$ ha matrice rappresentativa $A^{\prime }\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( k,k\right) $, con $\dim H=k$ ($A^{\prime }=A|_{H}$: \`{e} la matrice
rappresentativa dell'applicazione lineare descritta da $A$ ristretta a $H$). 
$\tciLaplace _{A^{\prime }}$ \`{e} autoaggiunta: la definizione si applica
considerando che il prodotto scalare tra $\mathbf{u}_{1}\mathbf{,u}_{2}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{k}$ sia $\left\langle P\left( \mathbf{u}_{1}\right) ,P\left( \mathbf{u}%
_{2}\right) \right\rangle $ con $P:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{k}\rightarrow H$ mappa di parametrizzazione. Infatti per ogni $\mathbf{u}%
_{1}\mathbf{,u}_{2}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{k}$ $\left\langle A^{\prime }\mathbf{u}_{1}\mathbf{,u}_{2}\right\rangle
=\left\langle P\left( A^{\prime }\mathbf{u}_{1}\right) ,P\left( \mathbf{u}%
_{2}\right) \right\rangle =$ $\left\langle AP\left( \mathbf{u}_{1}\right)
,P\left( \mathbf{u}_{2}\right) \right\rangle =\left\langle P\left( \mathbf{u}%
_{1}\right) ,AP\left( \mathbf{u}_{2}\right) \right\rangle =\left\langle 
\mathbf{u}_{1},A^{\prime }\mathbf{u}_{2}\right\rangle $. Poich\'{e} $%
\tciLaplace _{A^{\prime }}$ \`{e} autoaggiunta, $A^{\prime }$ \`{e}
simmetrica e $A^{\prime }=\left( A^{\prime }\right) ^{T}$: dunque, per
ipotesi induttiva, $A^{\prime }$ \`{e} ortogonalmente diagonalizzabile ed
esiste una base ortonormale (rispetto al prodotto scalare $\left\langle
P\left( \mathbf{u}_{1}\right) ,P\left( \mathbf{u}_{2}\right) \right\rangle $%
) di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{k}$ formata da autovettori di $A^{\prime }$: $\left\{ \mathbf{u}_{1}%
\mathbf{,...,u}_{k}\right\} $. La base ortonormale corrispondente di $%
H\subseteq 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ \`{e} $B_{H}=\left\{ P\left( \mathbf{u}_{1}\right) \mathbf{,...,}%
P\left( \mathbf{u}_{k}\right) \right\} $, formata da autovettori di $%
A^{\prime }$. Data una base qualsiasi di $V_{\lambda }$, la ortonormalizzo
usando l'algoritmo di Gram-Schmidt e ottengo una nuova base $B_{V_{\lambda
}} $. Allora $B=B_{H}\cup B_{V_{\lambda }}$ \`{e} una base ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, perch\'{e} $B_{H}$ e $B_{V_{\lambda }}$ sono ciascuna una base
ortonormale e ogni vettore di $H$ \`{e} ortogonale a ogni vettore di $%
V_{\lambda }$; \`{e} inoltre una base formata da autovettori di $A$: i
vettori di $B_{V_{\lambda }}$ lo sono per costruzione, e i vettori di $%
B_{H}=B_{V_{\lambda }^{\perp }}$ lo sono perch\'{e} autovettori di $%
A^{\prime }$. Quindi $A$ \`{e} ortogonalmente diagonalizzabile. $%
\blacksquare $

Con spettro ci si riferisce all'insieme di autovalori di un'applicazione
lineare.

\begin{enumerate}
\item Se $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( 3,3\right) $ \`{e} simmetrica e $\mathbf{u,v,w}$ sono autovettori
relativi agli autovalori - rispettivamente $0,3,-2$, allora $\ker
A=V_{0}=Span\left( \mathbf{u}\right) $: infatti, se $A$ \`{e} simmetrica, 
\`{e} diagonalizzabile e quindi $g_{0}+g_{3}+g_{-2}=3$, con $g_{0}=1$.
Inoltre $\func{col}A=\func{row}A=\left( \ker A\right) ^{\perp }=V_{0}^{\perp
}=V_{3}\oplus V_{-2}=Span\left( \mathbf{v,w}\right) $.
\end{enumerate}

\textbf{Corollario (decomposizione spettrale)}%
\begin{gather*}
\text{Hp}\text{: }A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) \text{ \`{e} simmetrica; }\lambda _{1},...,\lambda _{s}%
\text{ sono autovalori distinti di }A\text{; } \\
P_{\lambda _{i}}\text{ \`{e} la matrice che descrive la proiezione
ortogonale su }V_{\lambda _{i}}\text{, con }i=1,...,s \\
\text{Ts}\text{: (i) }Id_{n}=P_{\lambda _{1}}+...+P_{\lambda _{s}} \\
\text{(ii) }A=\lambda _{1}P_{\lambda _{1}}+...+\lambda _{s}P_{\lambda _{s}}
\\
\text{(iii) }P_{\lambda _{i}}^{2}=P_{\lambda _{i}}\text{, }\left( P_{\lambda
_{i}}\right) ^{T}=P_{\lambda _{i}}\text{; }P_{\lambda _{i}}P_{\lambda
_{j}}=0_{M}\text{ per ogni }i\neq j
\end{gather*}

$\lambda _{1},...,\lambda _{s}$ sono tutti reali per conseguenza del teorema
spettrale.

\textbf{Dim} (ii) Poich\'{e} $A=A^{T}$, $A$ \`{e} ortogonalmente
diagonalizzabile ed esiste una base ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ formata da autovettori di $A$, che \`{e} l'unione di basi ortonormali
di ciascun autospazio. Sia $Q\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) =\left[ \mathbf{q}_{1}|...|\mathbf{q}_{n}\right] $ la
matrice ortogonale che ha come colonne i vettori di tale base: $%
D=Q^{-1}AQ\Longleftrightarrow A=QDQ^{T}$. Penso a $Q$ e $Q^{T}$ come somma
di $n$ matrici: $Q=\left[ \mathbf{q}_{1}\mathbf{|0|...|0}\right] +...+\left[ 
\mathbf{0|...|0|q}_{n}\right] $, $Q^{T}=\left[ 
\begin{array}{c}
\mathbf{q}_{1}^{T} \\ 
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0}%
\end{array}%
\right] +...+\left[ 
\begin{array}{c}
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0} \\ 
\mathbf{q}_{n}^{T}%
\end{array}%
\right] $. Allora $A=QDQ^{T}=\left( \left[ \mathbf{q}_{1}\mathbf{|0|...|0}%
\right] +...+\left[ \mathbf{0|...|0|q}_{n}\right] \right) \left[ 
\begin{array}{ccc}
\lambda _{1} & ... & 0 \\ 
... & ... & ... \\ 
0 & ... & \lambda _{s}%
\end{array}%
\right] \left( \left[ 
\begin{array}{c}
\mathbf{q}_{1}^{T} \\ 
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0}%
\end{array}%
\right] +...+\left[ 
\begin{array}{c}
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0} \\ 
\mathbf{q}_{n}^{T}%
\end{array}%
\right] \right) $ $=$ 

$\left( \left[ \lambda _{1}\mathbf{q}_{1}\mathbf{%
|0|...|0}\right] +...+\left[ \mathbf{0|...|0|}\lambda _{s}\mathbf{q}_{n}%
\right] \right) \left( \left[ 
\begin{array}{c}
\mathbf{q}_{1}^{T} \\ 
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0}%
\end{array}%
\right] +...+\left[ 
\begin{array}{c}
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0} \\ 
\mathbf{q}_{n}^{T}%
\end{array}%
\right] \right) $ $=\lambda _{1}\mathbf{q}_{1}\mathbf{q}_{1}^{T}+...+\lambda
_{s}\mathbf{q}_{n}\mathbf{q}_{n}^{T}$. Ma $\mathbf{q}_{1}\mathbf{q}_{1}^{T}$ 
\`{e} la matrice che rappresenta la proiezione ortogonale su $Span\left( 
\mathbf{q}_{1}\right) $,..., $\mathbf{q}_{n}\mathbf{q}_{n}^{T}$ \`{e} la
matrice che rappresenta la proiezione ortogonale su $Span\left( \mathbf{q}%
_{n}\right) $. $\lambda _{1}\mathbf{q}_{1}\mathbf{q}_{1}^{T}+...+\lambda _{s}%
\mathbf{q}_{n}\mathbf{q}_{n}^{T}=\lambda _{1}\left( \mathbf{q}_{1}\mathbf{q}%
_{1}^{T}\mathbf{+...+q}_{g\left( \lambda _{1}\right) }\mathbf{q}_{g\left(
\lambda _{1}\right) }^{T}\right) +...+\lambda _{s}\left( \mathbf{...+q}_{n}%
\mathbf{q}_{n}^{T}\right) $, dove $\mathbf{q}_{1}\mathbf{,...,q}_{g\left(
\lambda _{1}\right) }$ \`{e} una base ortonormale di $V_{\lambda _{1}}$.
Allora $\mathbf{q}_{1}\mathbf{q}_{1}^{T}\mathbf{+...+q}_{g\left( \lambda
_{1}\right) }\mathbf{q}_{g\left( \lambda _{1}\right) }^{T}$ \`{e} la matrice
che rappresenta la proiezione ortogonale su $V_{\lambda _{1}}$, e $\mathbf{%
...+q}_{n}\mathbf{q}_{n}^{T}$ \`{e} la matrice che rappresenta la proiezione
ortogonale su $V_{\lambda _{s}}$. Quindi $A=\lambda _{1}P_{\lambda
_{1}}+...+\lambda _{s}P_{\lambda _{s}}$.

(i) Se $Q\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $, $Q=\left[ \mathbf{q}_{1}|...|\mathbf{q}_{n}\right] $
ha come colonne i vettori di una base ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, $Q$ \`{e} ortogonale, quindi $QQ^{T}=Id_{n}$, perci\`{o} ripeto il
calcolo precedente scrivendo sia $Q$ sia $Q^{T}$ come somma di $n$ matrici.

$Q=\left[ \mathbf{q}_{1}\mathbf{|0|...|0}\right] +...+\left[ \mathbf{%
0|...|0|q}_{n}\right] $, $Q^{T}=\left[ 
\begin{array}{c}
\mathbf{q}_{1}^{T} \\ 
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0}%
\end{array}%
\right] +...+\left[ 
\begin{array}{c}
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0} \\ 
\mathbf{q}_{n}^{T}%
\end{array}%
\right] $. Allora $Id=QQ^{T}=

\left( \left[ \mathbf{q}_{1}\mathbf{|0|...|0}%
\right] +...+\left[ \mathbf{0|...|0|q}_{n}\right] \right) \left( \left[ 
\begin{array}{c}
\mathbf{q}_{1}^{T} \\ 
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0}%
\end{array}%
\right] +...+\left[ 
\begin{array}{c}
\mathbf{0} \\ 
\mathbf{...} \\ 
\mathbf{0} \\ 
\mathbf{q}_{n}^{T}%
\end{array}%
\right] \right) $ $=\mathbf{q}_{1}\mathbf{q}_{1}^{T}+...+\mathbf{q}_{n}%
\mathbf{q}_{n}^{T}$. Ma $\mathbf{q}_{1}\mathbf{q}_{1}^{T}$ \`{e} la matrice
che rappresenta la proiezione ortogonale su $Span\left( \mathbf{q}%
_{1}\right) $,..., $\mathbf{q}_{n}\mathbf{q}_{n}^{T}$ \`{e} la matrice che
rappresenta la proiezione ortogonale su $Span\left( \mathbf{q}_{n}\right) $. 
$\mathbf{q}_{1}\mathbf{q}_{1}^{T}+...+\mathbf{q}_{n}\mathbf{q}%
_{n}^{T}=\left( \mathbf{q}_{1}\mathbf{q}_{1}^{T}\mathbf{+...+q}_{g\left(
\lambda _{1}\right) }\mathbf{q}_{g\left( \lambda _{1}\right) }^{T}\right)
+...+\left( \mathbf{...+q}_{n}\mathbf{q}_{n}^{T}\right) $, dove $\mathbf{q}%
_{1}\mathbf{,...,q}_{g\left( \lambda _{1}\right) }$ \`{e} una base
ortonormale di $V_{\lambda _{1}}$ (raggruppo gli autovettori che
costituiscono una base dello stesso autospazio). Allora $\mathbf{q}_{1}%
\mathbf{q}_{1}^{T}\mathbf{+...+q}_{g\left( \lambda _{1}\right) }\mathbf{q}%
_{g\left( \lambda _{1}\right) }^{T}$ \`{e} la matrice che rappresenta la
proiezione ortogonale su $V_{\lambda _{1}}$, e $\mathbf{...+q}_{n}\mathbf{q}%
_{n}^{T}$ \`{e} la matrice che rappresenta la proiezione ortogonale su $%
V_{\lambda _{s}}$. Quindi $Id_{n}=P_{\lambda _{1}}+...+P_{\lambda _{s}}$.

(iii) Si \`{e} gi\`{a} mostrato che ogni matrice di proiezione ortogonale 
\`{e} idempotente e simmetrica. Si dimostra che $P_{\lambda _{i}}P_{\lambda
_{j}}=0_{M}$ per ogni $i\neq j$. Per ogni $\mathbf{v}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ calcolo $P_{\lambda _{j}}\mathbf{v\in }V_{\lambda _{j}}$. $P_{\lambda
_{i}}\left( P_{\lambda _{j}}\mathbf{v}\right) =\mathbf{0}\Longleftrightarrow
P_{\lambda _{j}}\mathbf{v\in }\ker P_{\lambda _{i}}$, ma $\ker P_{\lambda
_{i}}=V_{\lambda _{i}}^{\perp }$, e $V_{\lambda _{j}}\subseteq V_{\lambda
_{i}}^{\perp }$ per il lemma 2, quindi $P_{\lambda _{j}}\mathbf{v\in }%
V_{\lambda _{j}}\subseteq V_{\lambda _{i}}^{\perp }$ e $P_{\lambda
_{i}}\left( P_{\lambda _{j}}\mathbf{v}\right) =\mathbf{0}$. Poich\'{e}
questo vale per ogni $\mathbf{v}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, $P_{\lambda _{i}}P_{\lambda _{j}}$\textbf{\ }dev'essere la matrice
nulla. $\blacksquare $

Usare la decomposizione spettrale per interpretare una matrice simmetrica
permette di calcolare facilmente potenze: $A^{2}=\left( \lambda
_{1}P_{\lambda _{1}}+...+\lambda _{s}P_{\lambda _{s}}\right) \left( \lambda
_{1}P_{\lambda _{1}}+...+\lambda _{s}P_{\lambda _{s}}\right) =\lambda
_{1}^{2}P_{\lambda _{1}}^{2}+...+\lambda _{1}\lambda _{s}P_{\lambda
_{1}}P_{\lambda _{s}}+...+\lambda _{s}^{2}P_{\lambda _{s}}^{2}=\lambda
_{1}^{2}P_{\lambda _{1}}^{2}+...+\lambda _{s}^{2}P_{\lambda
_{s}}^{2}=\lambda _{1}^{2}P_{\lambda _{1}}+...+\lambda _{s}^{2}P_{\lambda
_{s}}$ perch\'{e} tutti i prodotti misti si annullano e ogni matrice di
proiezione \`{e} idempotente. In generale,%
\begin{equation*}
A^{k}=\lambda _{1}^{k}P_{\lambda _{1}}+...+\lambda _{s}^{k}P_{\lambda _{s}}
\end{equation*}

Questo si poteva gi\`{a} fare con $A$ semplicemente diagonalizzabile, ma ora
si pu\`{o} calcolare e. g. $\sqrt{A}$, cio\`{e} l'unica matrice $B$ tale che 
$B^{2}=A$: $\sqrt{A}=\sqrt{\lambda _{1}}P_{\lambda _{1}}+...+\sqrt{\lambda
_{s}}P_{\lambda _{s}}$, infatti $B^{2}=\lambda _{1}P_{\lambda
_{1}}+...+\lambda _{s}P_{\lambda _{s}}$. Quindi si possono calcolare potenze
con esponente non intero.

\begin{enumerate}
\item Si pu\`{o} definire, data $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $, $e^{A}$ usando lo sviluppo di Taylor: $e^{x}=1+x+\frac{%
1}{2}x^{2}+\frac{1}{3!}x^{3}+...$, e $e^{A}=Id+A+\frac{1}{2}A^{2}+\frac{1}{3!%
}A^{3}+...$. Se $A=A^{T}$, si usa la decomposizione spettrale e si ha%
\begin{eqnarray*}
e^{A} &=&\left( \lambda _{1}P_{\lambda _{1}}+...+\lambda _{s}P_{\lambda
_{s}}\right) ^{0}+\lambda _{1}P_{\lambda _{1}}+...+\lambda _{s}P_{\lambda
_{s}}+\frac{1}{2}\left( \lambda _{1}P_{\lambda _{1}}+...+\lambda
_{s}P_{\lambda _{s}}\right) ^{2}+\frac{1}{3!}\left( \lambda _{1}P_{\lambda
_{1}}+...+\lambda _{s}P_{\lambda _{s}}\right) ^{3}...= \\
&&P_{\lambda _{1}}+...+P_{\lambda _{s}}+\lambda _{1}P_{\lambda
_{1}}+...+\lambda _{s}P_{\lambda _{s}}+\frac{1}{2}\left( \lambda
_{1}^{2}P_{\lambda _{1}}+...+\lambda _{s}^{2}P_{\lambda _{s}}\right) +\frac{1%
}{3!}\left( \lambda _{1}^{3}P_{\lambda _{1}}+...+\lambda _{s}^{3}P_{\lambda
_{s}}\right) ... \\
&=&\left( 1+\lambda _{1}+\frac{1}{2}\lambda _{1}^{2}+\frac{1}{3!}\lambda
_{1}^{3}+...\right) P_{\lambda _{1}}+...+\left( 1+\lambda _{s}+\frac{1}{2}%
\lambda _{s}^{2}+\frac{1}{3!}\lambda _{s}^{3}+...\right) P_{\lambda _{s}}= \\
&&e^{\lambda _{1}}P_{\lambda _{1}}+...+e^{\lambda _{s}}P_{\lambda _{s}}
\end{eqnarray*}
\end{enumerate}

\section{Forme quadratiche}

Le forme quadratiche reali sono funzioni $q:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, polinomi omogenei in cui ogni monomio ha grado due, che sono combinazione
lineare dei quadratii delle variabili e dei prodotti di due variabili
diverse.

(Una forma lineare \`{e} $\left[ a_{1}|...|a_{n}\right] \mathbf{x}%
=a_{1}x_{1}+...+a_{n}x_{n}$)

\begin{enumerate}
\item $q\left( x_{1},...,x_{n}\right) =x_{1}^{2}+...+x_{n}^{2}$ \`{e} una
forma quadratica che definisce la norma al quadrato del vettore $\mathbf{x}%
=\left( 
\begin{array}{c}
x_{1} \\ 
... \\ 
x_{n}%
\end{array}%
\right) $, secondo il prodotto scalare standard di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$.

\item $q\left( x_{1},x_{2}\right) =x_{1}^{2}+2x_{1}x_{2}+3x_{2}^{2}$ \`{e}
una forma quadratica
\end{enumerate}

Questo tipo di funzioni pu\`{o} essere studiato usando le matrici.

\begin{enumerate}
\item $q\left( x_{1},x_{2}\right)
=x_{1}^{2}+2x_{1}x_{2}+3x_{2}^{2}=x_{1}\left( x_{1}+2x_{2}\right)
+x_{2}\left( 3x_{2}\right) =\left[ x_{1}|x_{2}\right] \left[ 
\begin{array}{c}
x_{1}+2x_{2} \\ 
3x_{2}%
\end{array}%
\right] =\left[ x_{1}|x_{2}\right] \left[ 
\begin{array}{cc}
1 & 2 \\ 
0 & 3%
\end{array}%
\right] \left[ 
\begin{array}{c}
x_{1} \\ 
x_{2}%
\end{array}%
\right] $. Quindi $q$ pu\`{o} essere rappresentata con una matrice $2\times
2 $. Questa matrice per\`{o} non \`{e} l'unica possibile: infatti $q\left(
x_{1},x_{2}\right) =x_{1}\left( x_{1}+2x_{2}\right) +x_{2}\left(
3x_{2}\right) =\left[ x_{1}+2x_{2}|3x_{2}\right] \left[ 
\begin{array}{c}
x_{1} \\ 
x_{2}%
\end{array}%
\right] =\left[ x_{1}|x_{2}\right] \left[ 
\begin{array}{cc}
1 & 0 \\ 
2 & 3%
\end{array}%
\right] \left[ 
\begin{array}{c}
x_{1} \\ 
x_{2}%
\end{array}%
\right] $ La matrice ottenuta cos\`{\i} \`{e} la trasposta della precedente.
Ci sono infinite matrici che sono associabili a questa forma quadratica.

Vale anche $q\left( x_{1},x_{2}\right)
=x_{1}^{2}+x_{1}x_{2}+x_{1}x_{2}+3x_{2}^{2}=x_{1}\left( x_{1}+x_{2}\right)
+x_{2}\left( x_{1}+3x_{2}\right) =\left[ x_{1}+x_{2}|x_{1}+3x_{2}\right] %
\left[ 
\begin{array}{c}
x_{1} \\ 
x_{2}%
\end{array}%
\right] =\left[ x_{1}|x_{2}\right] \left[ 
\begin{array}{cc}
1 & 1 \\ 
1 & 3%
\end{array}%
\right] \left[ 
\begin{array}{c}
x_{1} \\ 
x_{2}%
\end{array}%
\right] $. La matrice cos\`{\i} ottenuta \`{e} simmetrica, ed \`{e} l'unica
matrice simmetrica associabile a $q$.
\end{enumerate}

Data una matrice simmetrica $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $, si definisce la forma quadratica $q_{A}:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, $q_{A}\left( \mathbf{x}\right) =\mathbf{x}^{T}A\mathbf{x}$, con $A=\left[ 
\begin{array}{cccc}
a_{11} & a_{12} & ... & a_{1n} \\ 
a_{12} & a_{22} & ... & a_{2n} \\ 
... & ... & ... & ... \\ 
a_{1n} & a_{2n} & ... & a_{nn}%
\end{array}%
\right] $; l'espressione analitica della forma quadratica \`{e} $q_{A}\left(
x_{1},...,x_{n}\right) =\sum_{i=1}^{n}a_{ii}x_{i}^{2}+2\sum_{1\leq i\leq
j\leq n}a_{ij}x_{i}x_{j}$. Quindi sulla diagonale principale vanno i
coefficienti dei quadrati, in $a_{ij}$ va il coefficiente di $x_{i}x_{j}$
diviso per due.

\textbf{Propriet\`{a}}

\begin{description}
\item[-] $q_{A}\left( \mathbf{0}\right) =0$

\item[-] $q_{A}\left( t\mathbf{x}\right) =\left( t\mathbf{x}\right)
^{T}A\left( t\mathbf{x}\right) =t^{2}\left( \mathbf{x}^{T}A\mathbf{x}\right)
=t^{2}q_{A}\left( \mathbf{x}\right) $

\item[-] posto $\mathbf{x\neq 0}$, $q_{A}\left( t\mathbf{x}\right)
q_{A}\left( \mathbf{x}\right) \geq 0$, cio\`{e} $q_{A}\left( t\mathbf{x}%
\right) $ e $q_{A}\left( \mathbf{x}\right) $ sono concordi
\end{description}

\textbf{Def} Posto $q_{A}\left( \mathbf{x}\right) =\mathbf{x}^{T}A\mathbf{x}$%
, $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ simmetrica, la forma quadratica $q_{A}$ (o la matrice
simmetrica $A$) si dice definita positiva se $q_{A}\left( \mathbf{x}\right)
\geq 0$ $\forall $ $\mathbf{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ e $q_{A}\left( \mathbf{x}\right) =0\Longleftrightarrow \mathbf{x=0}$;
definita negativa se $q_{A}\left( \mathbf{x}\right) \leq 0$ $\forall $ $%
\mathbf{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ e $q_{A}\left( \mathbf{x}\right) =0\Longleftrightarrow \mathbf{x=0}$;
semidefinita positiva se $q_{A}\left( \mathbf{x}\right) \geq 0$ $\forall $ $%
\mathbf{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ e $\exists $ $\mathbf{v\neq 0}:q_{A}\left( \mathbf{v}\right) =0$;
semidefinita negativa se $q_{A}\left( \mathbf{x}\right) \leq 0$ $\forall $ $%
\mathbf{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ e $\exists $ $\mathbf{v\neq 0}:q_{A}\left( \mathbf{v}\right) =0$;
indefinita $\exists $ $\mathbf{v,w\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}:q_{A}\left( \mathbf{v}\right) >0$ e $q_{A}\left( \mathbf{w}\right) <0$.

Poich\'{e}, data $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $, gli elementi sulla sua diagonale appartengono sempre
all'immagine di $q_{A}\left( \mathbf{x}\right) $ (per ottenere $a_{ii}$ \`{e}
sufficiente prendere $\mathbf{x=e}_{i}$), se e. g. sulla diagonale ci sono
elementi discordi si pu\`{o} gi\`{a} concludere che $A$ \`{e} indefinita.
Dunque affinch\'{e} $A$ sia definita positiva \`{e} necessario che tutti gli
elementi sulla diagonale siano positivi.

Il segno delle forme quadratiche \`{e} interessante per vari motivi, ad
esempio:

\begin{description}
\item[-] studio di massimi e minimi di funzioni di pi\`{u} variabili reali:
se $n=1$ e $x_{0}$ \`{e} un punto stazionario di una funzione derivabile due
volte, con $f^{\prime \prime }\left( x_{0}\right) \neq 0$, $f\left( x\right)
=f\left( x_{0}\right) +\frac{1}{2}f^{\prime \prime }\left( x_{0}\right)
\left( x-x_{0}\right) ^{2}+o\left( x-x_{0}\right) ^{2}$. Per conoscere la
natura del punto studio il segno di $f^{\prime \prime }\left( x_{0}\right) $%
: se $f^{\prime \prime }\left( x_{0}\right) >0$ $x_{0}$ \`{e} un punto di
minimo locale, se $f^{\prime \prime }\left( x_{0}\right) <0$ $x_{0}$ \`{e}
un punto di massimo locale. Lo stesso si pu\`{o} fare per $n\geq 2$: se $%
\mathbf{x}_{0}$ \`{e} un punto stazionario di una funzione, $f\left( \mathbf{%
x}\right) =f\left( \mathbf{x}_{0}\right) +\frac{1}{2}\left( \mathbf{x-x}%
_{0}\right) ^{T}Hf\left( \mathbf{x}_{0}\right) \left( \mathbf{x-x}%
_{0}\right) +...$. $Hf\left( \mathbf{x}_{0}\right) $ \`{e} detta matrice
hessiana ed \`{e} simmetrica sotto opportune ipotesi: in tal caso $\left( 
\mathbf{x-x}_{0}\right) ^{T}Hf\left( \mathbf{x}_{0}\right) \left( \mathbf{x-x%
}_{0}\right) $ \`{e} una forma quadratica e per capire la natura di $\mathbf{%
x}_{0}$ bisogna studiare il segno della forma quadratica $\left( \mathbf{x-x}%
_{0}\right) ^{T}Hf\left( \mathbf{x}_{0}\right) \left( \mathbf{x-x}%
_{0}\right) $.

\item[-] La funzione bilineare che definisce il prodotto scalare non
standard in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ \`{e} $\left\langle \_,\_\right\rangle _{B}:%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}\times 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, con $B$ simmetrica: $\left\langle \mathbf{x,y}\right\rangle _{B}=\mathbf{x%
}^{T}B\mathbf{y}$. Questo \`{e} un prodotto scalare se vale anche la propriet%
\`{a} di positivit\`{a}: $\left\langle \mathbf{x,x}\right\rangle _{B}\geq 0$ 
$\forall $ $\mathbf{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ e $\left\langle \mathbf{x,x}\right\rangle _{B}=0\Longleftrightarrow 
\mathbf{x=0}$. Richiedere che valga la positivit\`{a} \`{e} equivalente a
chiedere che $B$ sia definita positiva: quindi per definire tale prodotto
scalare non standard di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ occorre che $B$ sia una matrice simmetrica definita positiva. Quindi i
prodotti scalari in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ sono in corrispondenza biunivoca con le matrici simmetriche definita
positive.
\end{description}

Come cambia la matrice di una forma quadratica al variare delle coordinate?
Sia $q_{A}\left( \mathbf{x}\right) =\mathbf{x}^{T}A\mathbf{x}$, con $\mathbf{%
x}$ vettore delle coordinate rispetto alla base canonica; fissata una nuova
base di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, sia $P$ la matrice che ha come colonne i vettori della nuova base;
allora $q_{A}\left( \mathbf{x}\right) =q_{A}\left( P\mathbf{y}\right)
=\left( P\mathbf{y}\right) ^{T}A\left( P\mathbf{y}\right) =\mathbf{y}%
^{T}\left( P^{T}AP\right) \mathbf{y}=q_{B}\left( \mathbf{y}\right) $ \`{e}
la forma quadratica che assegna a $\mathbf{y}=X_{P}\left( \mathbf{x}\right) $
lo stesso numero di $q_{A}\left( \mathbf{x}\right) $, che opera rispetto
alla base canonica. La forma $P^{T}AP$ ricorda la diagonalizzazione
ortogonale ($D=P^{T}AP$): voglio reinterpretare $q_{A}$ con le coordinate
rispetto a una base ortonormale di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ formata da autovettori di $A$, che so esistere per il teorema
spettrale perch\'{e} $A$ \`{e} simmetrica. Si pu\`{o} infatti studiare il
segno di una forma quadratica studiando la sua rappresentazione diagonale,
dato che $q_{A}$ e $q_{B}$ producono lo stesso risultato e che la
rappresentazione diagonale \`{e} particolarmente semplice. In tal caso
infatti, se $Q$ ha come colonne i vettori di una base ortonormale formata da
autovettori di $A$, $q_{D}\left( \mathbf{y}\right) =\mathbf{y}^{T}D\mathbf{y=%
}\left[ y_{1}|...|y_{n}\right] \left[ 
\begin{array}{ccc}
\lambda _{1} & ... & 0 \\ 
... & ... & ... \\ 
0 & ... & \lambda _{n}%
\end{array}%
\right] \left[ 
\begin{array}{c}
y_{1} \\ 
... \\ 
y_{n}%
\end{array}%
\right] =\left[ \lambda _{1}y_{1}|...|\lambda _{n}y_{n}\right] \left[ 
\begin{array}{c}
y_{1} \\ 
... \\ 
y_{n}%
\end{array}%
\right] =\lambda _{1}y_{1}^{2}+...+\lambda _{n}y_{n}^{2}$, con $\mathbf{y}$
vettore delle coordinate rispetto alla base ortonormale.

\textbf{Proposizione}%
\begin{gather*}
\text{Hp}\text{: }A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) \text{ simmetrica, }q_{A}\left( \mathbf{x}\right) =%
\mathbf{x}^{T}A\mathbf{x} \\
\text{Ts}\text{: (i) se }\lambda \text{ \`{e} un autovalore di }A\text{ e }%
\mathbf{v}\in V_{\lambda }\text{, }q_{A}\left( \mathbf{v}\right) =\lambda
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2} \\
\text{(ii) }\forall \text{ }\mathbf{x}\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}\text{ }\lambda _{\min }\left\vert \left\vert \mathbf{x}\right\vert
\right\vert ^{2}\leq q_{A}\left( \mathbf{x}\right) \leq \lambda _{\max
}\left\vert \left\vert \mathbf{x}\right\vert \right\vert ^{2}
\end{gather*}

L'esistenza di $\lambda _{\min },\lambda _{\max }$ \`{e} garantita dal fatto
che in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ \`{e} definita una relazione d'ordine totale.

\textbf{Dim} (i) $q_{A}\left( \mathbf{v}\right) =\mathbf{v}^{T}A\mathbf{v=v}%
^{T}\left( \lambda \mathbf{v}\right) =\lambda \left( \mathbf{v}^{T}\mathbf{v}%
\right) =\lambda \left\vert \left\vert \mathbf{v}\right\vert \right\vert
^{2} $.

(ii) Sia $D$ la matrice diagonale associata ad $A$, con cambio di base
(dalla base di autovettori alla base canonica) descritto dalla matrice
ortogonale $Q$. Se $\mathbf{x}$ \`{e} il vettore delle coordinate rispetto
alla base canonica e $\mathbf{y}$ \`{e} il vettore delle coordinate rispetto
alla base ortonormale, $\mathbf{x}=Q\mathbf{y}$. $q_{A}\left( \mathbf{x}%
\right) =q_{A}\left( Q\mathbf{y}\right) =\left( Q\mathbf{y}\right)
^{T}A\left( Q\mathbf{y}\right) =\mathbf{y}^{T}\left( Q^{T}AQ\right) \mathbf{%
y=y}^{T}D\mathbf{y}=\lambda _{1}y_{1}^{2}+...+\lambda _{n}y_{n}^{2}$ (i $%
\lambda _{i}$ non sono necessariamente distinti). Tale termine pu\`{o}
essere maggiorato prendendo come coefficiente di ogni termine $\lambda
_{\max }=\max \left\{ \lambda _{1},...,\lambda _{n}\right\} $, e
analogamente minorato con $\lambda _{\min }=\min \left\{ \lambda
_{1},...,\lambda _{n}\right\} $:%
\begin{equation*}
\lambda _{\min }\left( y_{1}^{2}+...+y_{n}^{2}\right) =\lambda _{\min
}y_{1}^{2}+...+\lambda _{\min }y_{n}^{2}\leq \lambda
_{1}y_{1}^{2}+...+\lambda _{n}y_{n}^{2}\leq \lambda _{\max
}y_{1}^{2}+...+\lambda _{\max }y_{n}^{2}=\lambda _{\max }\left(
y_{1}^{2}+...+y_{n}^{2}\right)
\end{equation*}

$y_{1}^{2}+...+y_{n}^{2}$ \`{e} la norma al quadrato di $\mathbf{y}$, quindi
si ha $\lambda _{\min }\left\vert \left\vert \mathbf{y}\right\vert
\right\vert ^{2}\leq q_{A}\left( \mathbf{x}\right) \leq \lambda _{\max
}\left\vert \left\vert \mathbf{y}\right\vert \right\vert ^{2}$. Inoltre $%
\left\vert \left\vert \mathbf{x}\right\vert \right\vert =\left\vert
\left\vert \mathbf{y}\right\vert \right\vert $ perch\'{e} $\mathbf{x}=Q%
\mathbf{y}$ e le matrici ortogonali come $Q$ sono isometrie: $\left\vert
\left\vert \mathbf{x}\right\vert \right\vert ^{2}=\left\vert \left\vert Q%
\mathbf{y}\right\vert \right\vert ^{2}=\left\langle Q\mathbf{y},Q\mathbf{y}%
\right\rangle =\left( Q\mathbf{y}\right) ^{T}\left( Q\mathbf{y}\right) =%
\mathbf{y}^{T}\left( Q^{T}Q\right) \mathbf{y=y}^{T}\mathbf{y}=\left\vert
\left\vert \mathbf{y}\right\vert \right\vert ^{2}$. Dunque $\lambda _{\min
}\left\vert \left\vert \mathbf{x}\right\vert \right\vert ^{2}\leq
q_{A}\left( \mathbf{x}\right) \leq \lambda _{\max }\left\vert \left\vert 
\mathbf{x}\right\vert \right\vert ^{2}$. $\blacksquare $

Per (ii), affinch\'{e} $q_{A}\left( \mathbf{x}\right) $ sia definita
positiva \`{e} necessario e sufficiente che $\lambda _{\min }>0$ (se $%
\mathbf{y\neq 0}$, $0<\lambda _{\min }\left\vert \left\vert \mathbf{y}%
\right\vert \right\vert ^{2}\leq q_{A}\left( \mathbf{y}\right) $; se $%
\lambda _{\min }=0$, $\ker A$ non contiene solo il vettore nullo e $%
q_{A}\left( \mathbf{x}\right) =0$ per ogni vettore in $\ker A$); affinch\'{e}
sia definita negativa che $\lambda _{\max }<0$; affinch\'{e} sia
semidefinita positiva che $\lambda _{\min }=0$; affinch\'{e} sia
semidefinita negativa che $\lambda _{\max }=0$; affinch\'{e} sia indefinita
che $\lambda _{\min }<0<\lambda _{\max }$.

\begin{enumerate}
\item Le matrici che rappresentano la proiezione ortogonale su un
sottospazio sono semidefinite positive.
\end{enumerate}

\textbf{Def} Data $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ simmetrica, si dice segnatura di $A$ la coppia di
numeri interi non negativi $\left( p,q\right) $, dove $p$ \`{e} il numero di
autovalori di $A$ positivi e $q$ il numero di autovalori di $A$ negativi.

Se $A$ \`{e} definita positiva, $\left( p,q\right) =\left( n,0\right) $; se 
\`{e} definita negativa, $\left( p,q\right) =\left( 0,n\right) $; se \`{e}
semidefinita positiva, $\left( p,q\right) =\left( p,0\right) $ con $0<p<n$;
se \`{e} semidefinita negativa, $\left( p,q\right) =\left( 0,q\right) $ con $%
0<q<n$; se \`{e} indefinita, $\left( p,q\right) $ con $0<p<n$ e $0<q<n$.

\begin{enumerate}
\item Data $A\in M_{%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}\left( n,n\right) $ simmetrica, condizioni necessarie ma non sufficienti
affinch\'{e} i suoi autovalori siano tutti positivi (e quindi $A$ sia
definita positiva) \`{e} $tr\left( A\right) =\lambda _{1}+...+\lambda _{n}>0$%
, $\det A=\lambda _{1}...\lambda _{n}>0$. Tali condizioni diventano
sufficienti se $n=2$: infatti, se $\lambda _{1}\lambda _{2}>0$, $\lambda
_{1},\lambda _{2}$ potrebbero essere entrambi negativi, ma la positivit\`{a}
di entrambi \`{e} garantita da $\lambda _{1}+\lambda _{2}>0$.
\end{enumerate}

\subsubsection{Quoziente di Rayleigh}

Posto $\mathbf{x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}\backslash \left\{ \mathbf{0}\right\} $, $\lambda _{\min }\left\vert
\left\vert \mathbf{x}\right\vert \right\vert ^{2}\leq q_{A}\left( \mathbf{x}%
\right) \leq \lambda _{\max }\left\vert \left\vert \mathbf{x}\right\vert
\right\vert ^{2}$ \`{e} equivalente a $\lambda _{\min }\leq \frac{%
q_{A}\left( \mathbf{x}\right) }{\left\vert \left\vert \mathbf{x}\right\vert
\right\vert ^{2}}\leq \lambda _{\max }$. La funzione da $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ 
\begin{equation*}
\frac{q_{A}\left( \mathbf{x}\right) }{\left\vert \left\vert \mathbf{x}%
\right\vert \right\vert ^{2}}
\end{equation*}

si dice quoziente di Rayleigh, e confronta la forma quadratica $q_{A}\left( 
\mathbf{x}\right) $ con la forma quadratica $\left\vert \left\vert \mathbf{x}%
\right\vert \right\vert ^{2}$. La diseguaglianza mostra che $\lambda _{\min
} $ e $\lambda _{\max }$ sono rispettivamente minorante e maggiorante per il
quoziente di Rayleigh; sono anche minimo e massimo, perch\'{e}, se $\mathbf{v%
}\in V_{\lambda _{\min }}$, $\frac{q_{A}\left( \mathbf{v}\right) }{%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}}=\frac{\mathbf{v%
}^{T}A\mathbf{v}}{\left\vert \left\vert \mathbf{v}\right\vert \right\vert
^{2}}=\frac{\lambda _{\min }\left\langle \mathbf{v,v}\right\rangle }{%
\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}}=\lambda _{\min
}$, e se $\mathbf{v}\in V_{\lambda _{\max }}$, $\frac{q_{A}\left( \mathbf{v}%
\right) }{\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}}=%
\frac{\mathbf{v}^{T}A\mathbf{v}}{\left\vert \left\vert \mathbf{v}\right\vert
\right\vert ^{2}}=\frac{\lambda _{\max }\left\langle \mathbf{v,v}%
\right\rangle }{\left\vert \left\vert \mathbf{v}\right\vert \right\vert ^{2}}%
=\lambda _{\max }$. Quindi c'\`{e} un nuovo criterio per la ricerca degli
autovalori di $A$; tale ricerca pu\`{o} essere notevolmente semplificata se
si considerano solo $\mathbf{x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}\backslash \left\{ \mathbf{0}\right\} :\left\vert \left\vert \mathbf{x}%
\right\vert \right\vert =1$, perch\'{e} $\frac{q_{A}\left( t\mathbf{x}%
\right) }{\left\vert \left\vert t\mathbf{x}\right\vert \right\vert ^{2}}=%
\frac{q_{A}\left( \mathbf{x}\right) }{\left\vert \left\vert \mathbf{x}%
\right\vert \right\vert ^{2}}$, infatti $q_{A}\left( t\mathbf{x}\right)
=t^{2}q_{A}\left( \mathbf{x}\right) $ (quindi normalizzando un vettore il
segno della forma quadratica, che \`{e} determinato dagli autovalori, non
cambia e il quoziente di Rayleigh non cambia, dunque neanche il suo minimo e
massimo). Dunque $\lambda _{\min }=\min \left\{ \frac{\mathbf{x}^{T}A\mathbf{%
x}}{\left\vert \left\vert \mathbf{x}\right\vert \right\vert ^{2}}:\mathbf{%
x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}\backslash \left\{ \mathbf{0}\right\} \right\} =\min \left\{ \mathbf{x}%
^{T}A\mathbf{x}:\left\vert \left\vert \mathbf{x}\right\vert \right\vert
=1\right\} $ e $\lambda _{\max }=\max \left\{ \frac{\mathbf{x}^{T}A\mathbf{x}%
}{\left\vert \left\vert \mathbf{x}\right\vert \right\vert ^{2}}:\mathbf{x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}\backslash \left\{ \mathbf{0}\right\} \right\} =\max \left\{ \mathbf{x}%
^{T}A\mathbf{x}:\left\vert \left\vert \mathbf{x}\right\vert \right\vert
=1\right\} $, e la ricerca degli autovalori \`{e} di molto semplificata. Gli 
$\mathbf{x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}\backslash \left\{ \mathbf{0}\right\} :\left\vert \left\vert \mathbf{x}%
\right\vert \right\vert =1$ sono, se $n=2$, i punti di una circonferenza di
raggio $1$, se $n=3$ sono i punti sulla superficie di una sfera di raggio $1$%
; se $n>3$ sono i punti sulla superficie di un'ipersfera di raggio $1$. $%
\left\{ \mathbf{x\in 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
}^{n}\backslash \left\{ \mathbf{0}\right\} :\left\vert \left\vert \mathbf{x}%
\right\vert \right\vert =1\right\} $ \`{e} un sottinsieme compatto di $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$, cio\`{e} chiuso e limitato (in $%
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$, un insieme compatto \`{e} unione di intervalli del tipo $\left[ a,b\right]
$). Allora si pu\`{o} applicare il teorema di Weierstrass: data $f:\Omega
\rightarrow 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
$ continua, con $\Omega \subseteq 
%TCIMACRO{\U{211d} }%
%BeginExpansion
\mathbb{R}
%EndExpansion
^{n}$ compatto, $f$ ammette massimo e minimo assoluti. Perci\`{o}, se si
vuole mostrare l'esistenza di un autovalore reale di una matrice simmetrica $%
A$ (lemma 3), si considera la forma quadratica associata e usando il teorema
di Weierstrass si pu\`{o} dire che essa assume massimo e minimo assoluti,
che sono $\lambda _{\max }$ e $\lambda _{\min }$, quindi esistono due
autovalori reali.

\end{document}
